\documentclass[10pt,conference,compsocconf]{IEEEtran}

%\usepackage{times}
%\usepackage{balance}
\usepackage{url}
\usepackage{graphicx}	% For figure environment
\usepackage{amssymb}   % For blackboard bold symbols
\usepackage{amsmath}   % For mathematical environments like bmatrix


\begin{document}
\title{LightGCNPlus \\ A Graph Convolutional Network for Matrix Completion}

\author{
  Alain Joss \\
  Solomon Thiessen \\
  Antoine Suter \\
  Department of Computer Science, ETH Zurich, Switzerland
}

\maketitle

\begin{abstract}
  Personalized recommendation has become essential for businesses to enhance user experience and satisfaction. 
  In this context, we explore the task of collaborative filtering within the framework of matrix completion. 
  While the primary goal of collaborative filtering is to rank preferences for unobserved user-item interactions, 
  predicting ratings can provide deeper insights into user preferences, 
  thus improving decision-making across various use cases.
  
  In this work, we propose LightGCNPlus, a graph convolutional encoder-decoder network designed to predict ratings for unobserved user-item interactions, 
  extending the original LightGCN model. 
  LightGCNPlus incorporates the message passing logic of LightGCN in its hidden layer, adapting it to handle a set of real valued user-item ratings, rather than binary user-item interactions.
  The decoder then concatenates these representations and passes them through a multi-layer perceptron (MLP) to predict ratings, rather than just ranking items. 
  This approach enhances the model's ability to deliver accurate and personalized recommendations.
\end{abstract}

\section{Introduction}

\section{Model and Methods}
\subsection{Problem Formulation}
Given a partially observed (sparse) user-item rating matrix $R \in \mathbb{R}^{m \times n}$, the task of matrix completion consists of predicting the missing ratings in $R$.
where $m$ is the number of users and $n$ is the number of items, we construct a bipartite graph $G = (U, I, E)$, where $U$ and $I$ are the sets of users and items, respectively, and $E$ is the set of edges connecting users to items.
The adjacency matrix $A \in \mathbb{R}^{(m+n) \times (m+n)}$ of the graph $G$ is defined as:
\begin{equation}
    A = \begin{bmatrix}
    0 & R \\
    R^T & 0
    \end{bmatrix}
\end{equation}
\subsection{Preprocessing}
As a preprocessing step, we standardize the rating matrix $R$ using $z$-scores, applied separately over columns and rows to obtain $Z_{\text{col}}$ and $Z_{\text{row}}$, 
having zero mean and unit variance along their respective dimensions.
This ensures that ratings are comparable across items in $Z_{\text{col}}$ and users in $Z_{\text{row}}$.
Subsequently, we define the standardized adjacency matrix $\hat{A}$ as:
\[
\hat{A} = \begin{bmatrix}
0 & Z_{\text{col}} \\
Z_{\text{row}}^T & 0
\end{bmatrix}
\]
(To write in results the interpretations and that it radically improves the performance, also that it is different to traditional standardization over just $R_u$).

\subsection{Encoder}
\subsubsection{Graph Convolution}
The encoder of LightGCNPlus inherits the graph convolution (message passing logic) from the LightGCN model.
The graph convolution operation propagates information across the graph, updating the hidden representation of each node based on its neighbors.
Using multiple layers of graph convolution allows the model to capture higher-order relationships between users and items.
We adapt the graph convolution operation to handle real valued user-item ratings, such that
for a given node $v$, the hidden representation $h_v^{(l+1)}$ at layer $l+1$ is computed as:
\begin{align}
    h_u^{(l+1)} &= \sum_{i \in N(u)} \frac{z_{\text{col}_{u,i}}}{\sqrt{|N(u)||N(i)|}} h_i^{(l)} \quad \forall u \in U \\
    h_i^{(l+1)} &= \sum_{u \in N(i)} \frac{z_{\text{row}_{u,i}}}{\sqrt{|N(u)||N(i)|}} h_u^{(l)}  \quad \forall i \in I
\end{align}
where $N(v)$ is the set of neighbors of node $v$, $z_{u,i}$ is the edge weight (standardized rating) between user $u$ and item $i$, and $h_v^{(l)}$ is the hidden representation of node $v$ at layer $l$.
The hidden representations $h_v^{(0)}$ at the input layer represent the learnable embeddings of node $v$. Finally, the symmetrical normalization term $\sqrt{|N(u)||N(i)|}$ ensures that the hidden representations are scaled appropriately. Other choices of normalization are possible.

The $L$-layer graph convolution operation can be expressed more compactly in terms of matrix operations as:
\begin{equation}
    H^{(l+1)} = \tilde{A} H^{(l)}
\end{equation}
where $\tilde{A} = D^{-\frac{1}{2}} \hat{A} D^{-\frac{1}{2}}$ is the normalized standardized-adjacency matrix, $D$ is the diagonal degree matrix of $\hat{A}$, and $H^{(l)}$ is the matrix of hidden representations at layer $l$.
\subsubsection{Aggregator}
After $L$ layers of graph convolution, the hidden representations $h_v^{(l)}$ of node $v$ are aggregagated to obtain the final representations.
To maximize the expressiveness of the model, we aggregate the hidden representations by concatenating them along the feature dimension, 
resulting in the final representation $h_v$:
\begin{equation}
    h_v = \text{concat}(h_v^{(0)}, h^{(1)}, \ldots, h_v^{(L)})
\end{equation}
This choice is justified by the use of a multi-layer perceptron (MLP) in the decoder, which can take advantage of the concatenation aggregation, in contrast to other aggregation methods such as summation or averaging.

\subsection{Decoder}
The decoder of LightGCNPlus is a multi-layer perceptron (MLP) that takes the concatenated hidden representations $h_u$ and $h_i$ of user $u$ and item $i$ couples as input, and outputs the predicted rating $\hat{r}_{u,i}$:
\begin{equation}
    \hat{r}_{u,i} = \text{MLP}(\text{concat}(h_u, h_i))
\end{equation}
The specific choice of the MLP's architecture is a hyperparameter of the overall model.

\subsection{Training}
The training objective of LightGCNPlus is to predict the ratings of the observed user-item interactions in the training set $\Omega$.
The learnable parameters of the model $W$ comprise the embeddings of users and items and the weights of the MLP, and are trained end-to-end using the Adam optimizer.
We initialize the embeddings using a normal distribution, with tunable standard deviation $\sigma_{\text{init}}$, and the weights of the MLP using the Xavier initialization.
As loss function we use the mean squared error (MSE) between the predicted ratings $\hat{R}$ and the true ratings $R$:
\begin{equation}
    \mathcal{L} = \frac{1}{|\Omega|} \sum_{(u,i) \in \Omega} (\hat{r}_{u,i} - r_{u,i})^2 + \lambda ||W||_2^2
\end{equation}
where $\lambda$ is the weight decay regularization strength.
In addition to weight decay, to regularize the model we apply dropout to the MLP.
Finally, we avoid mini-batching to ensure that the model can learn the global structure of the graph, concurrently speeding up convergence.

\subsection{Tuning}
The hyperparameters of LightGCNPlus include the number of layers $L$, the dimensionality of the hidden representations $K$, the learning rate $\eta$, the weight decay regularization strength $\lambda$, the dropout rate $p$, and the hyperparameters of the MLP.
The MLP's hyperparameters consists of $\sigma_{\text{init}}$, the number of hidden layers, the number of units per hidden layer, and the activation function $f$.

After performing an initial grid search, we narrow the search-space by fixing following hyperparameters: $K=28$, $\eta=0.01$, $p=0.5$, and $\lambda=0.00005$, $\sigma_{\text{init}}=0.075$, and $f=\text{GELU}$.

We tune the remaining hyperparameters, the number of layers in the graph convolution $L$, the dimension of the hidden representations $K$ and the MLP's architecture, by performing a simple grid search.

\subsection{Inference}
\subsubsection{Ensembling}
In the tuning phase, we find that multiple models with different hyperparameters can achieve similar performance.
We take advantage of this insight by ensembling the top $M$ models, decreasing the variance of the predictions.
We find that this additional step makes predictions more robust and improves the overall performance of the model.

The ensembled prediction matrix is obtained by computing a weighted average of the predictions of the individual models.
The weights are determined by splitting the validation set into a set for fitting the ensemble weights and a set for evaluating the ensemble's performance.
We cap the number of models in the ensemble at $M=10$ to prevent overfitting.
\subsubsection{Postprocessing}
To ensure that the predictions are within the valid rating range, we clip the predictions to the interval $[1, 5]$.

\section{Experiments}


\subsection{Models and Methods}
The models and methods
section should describe what was
done to answer the research question, describe how it was done,
justify the experimental design, and
explain how the results were analyzed.

The model refers to the underlying mathematical model or structure which 
you use to describe your problem, or that your solution is based on. 
The methods on the other hand, are the algorithms used to solve the problem. 
In some cases, the suggested method directly solves the problem, without having it 
stated in terms of an underlying model. Generally though it is a better practice to have 
the model figured out and stated clearly, rather than presenting a method without specifying 
the model. In this case, the method can be more easily evaluated in the task of fitting 
the given data to the underlying model.

The methods part of this section, is not a step-by-step, directive,
protocol as you might see in your lab manual, but detailed enough such
that an interested reader can reproduce your
work~\cite{anderson04,wavelab}.

The methods section of a research paper provides the information by
which a study's validity is judged.
Therefore, it requires a clear and precise description of how an
experiment was done, and the rationale
for why specific experimental procedures were chosen.
It is usually helpful to
structure the methods section by~\cite{kallet04methods}:
\begin{enumerate}
\item Layout the model you used to describe the problem or the solution.
\item Describing the algorithms used in the study, briefly including
  details such as hyperparameter values (e.g. thresholds), and
  preprocessing steps (e.g. normalizing the data to have mean value of
  zero).
\item Explaining how the materials were prepared, for example the
  images used and their resolution.
\item Describing the research protocol, for example which examples
  were used for estimating the parameters (training) and which were
  used for computing performance.
\item Explaining how measurements were made and what
  calculations were performed. Do not reproduce the full source code in
  the paper, but explain the key steps.
\end{enumerate}

\subsection{Results}

Organize the results section based on the sequence of table and
figures you include. Prepare the tables and figures as soon as all
the data are analyzed and arrange them in the sequence that best
presents your findings in a logical way. A good strategy is to note,
on a draft of each table or figure, the one or two key results you
want to address in the text portion of the results.
The information from the figures is
summarized in Table~\ref{tab:fourier-wavelet}.

\begin{table*}[htbp]
  \centering
  \begin{tabular}[c]{|l||l|l|l|}
    \hline
    Basis&Support&Suitable signals&Unsuitable signals\\
    \hline
    Fourier&global&sine like&localized\\
    wavelet&local&localized&sine like\\
    \hline
  \end{tabular}
  \caption{Characteristics of Fourier and wavelet basis.}
  \label{tab:fourier-wavelet}
\end{table*}

When reporting computational or measurement results, always
report the mean (average value) along with a measure of variability
(standard deviation(s) or standard error of the mean).


\section{Tips for Good Software}
\label{sec:tips-software}

There is a lot of literature (for example~\cite{hunt99pragmatic} and
\cite{spolsky04software}) on how to write software. It is not the
intention of this section to replace software engineering
courses. However, in the interests of reproducible
research~\cite{schwab00}, there are a few guidelines to make your
reader happy:
\begin{itemize}
\item Have a \texttt{README} file that (at least) describes what your
  software does, and which commands to run to obtain results. Also
  mention anything special that needs to be set up, such as
  toolboxes\footnote{For those who are
  particularly interested, other common structures can be found at
  \url{http://en.wikipedia.org/wiki/README} and
  \url{http://www.gnu.org/software/womb/gnits/}.}.
\item A list of authors and contributors can be included in a file
  called \texttt{AUTHORS}, acknowledging any help that you may have
  obtained. For small projects, this information is often also
  included in the \texttt{README}.
\item Use meaningful filenames, and not \texttt{temp1.m},
  \texttt{temp2.m}. The code should also unzip into a subdirectory.
\item Document your code. Each file should at least have a short
  description about its reason for existence. Non obvious steps in the
  code should be commented.
\item Describe how the results presented in your paper can potentially
  be reproduced.
\end{itemize}


\section{Computational Intelligence Laboratory Requirements}
\label{sec:cil}

Your semester project is a group effort. It consists of four parts:
\begin{enumerate}
\item The programming assignments you solve during the semester.
\item Developing a novel solution for one of the assignments, e.g. by
  combining methods from previous programming assignments into a novel
  solution.
\item Comparing your novel solution to previous assignments.
\item Writing up your findings in a short scientific paper.
\end{enumerate}

\subsection{Developing a Novel Solution}

As your final programming assignment, you develop a novel solution to
one of the four application problems. You are free to exploit any idea
you have, provided it is not identical to any other group submission
or existing Matlab implementation of an algorithm on the
internet\footnote{\url{http://www.ethz.ch/students/semester/plagiarism_s_en.pdf}}.

Two examples for developing a novel solution:
\begin{itemize}
\item You implemented a collaborative filtering algorithm based on
  dimension reduction as part of an assignment. Now you apply
  dimension reduction to inpainting.
\item You implemented both a clustering and a sparse coding algorithm
  for image compression. Now you combine both techniques into a novel
  compression method.
\end{itemize}

\subsection{Comparison to Baselines}

You compare your novel algorithm to \emph{at least two baseline
  algorithms}. For the baselines, you can use the implementations you
developed as part of the programming assignments.


\subsection{Write Up}

The submission must be in PDF form, using the \LaTeX{} template
corresponding to the IEEE style of publication. Refer to
Section~\ref{sec:latex-primer} for more information about preparing
your document. The document should be a maximum of {\bf 4 pages}.

\subsection{\LaTeX{} Primer}
\label{sec:latex-primer}

\LaTeX{} is one of the most commonly used document preparation systems
for scientific journals and conferences. It is based on the idea
that authors should be able to focus on the content of what they are
writing without being distracted by its visual presentation.
The source of this file can be used as a starting point for how to use
the different commands in \LaTeX{}. We are using an IEEE style for
this course.

\subsubsection{Installation}

There are various different packages available for processing \LaTeX{}
documents.
On Windows, use the Mik\TeX{} package (\url{http://miktex.org/}), and
on OSX use MacTeX
(\url{http://www.tug.org/mactex/2009/}). Alternatively, on OSX, you
can install the \texttt{tetex} package via
Fink\footnote{\url{http://www.finkproject.org/}} or 
Macports\footnote{\url{http://www.macports.org/}}.

\subsubsection{Compiling \LaTeX{}}
Your directory should contain at least 4 files, in addition to image
files. Images should be in \texttt{.png}, \texttt{.jpg} or
\texttt{.pdf} format.
\begin{itemize}
\item IEEEtran.cls
\item IEEEtran.bst
\item groupXX-submission.tex
\item groupXX-literature.bib
\end{itemize}
Note that you should replace groupXX with your chosen group name.
Then, from the command line, type:
\begin{verbatim}
$ pdflatex groupXX-submission
$ bibtex groupXX-literature
$ pdflatex groupXX-submission
$ pdflatex groupXX-submission
\end{verbatim}
This should give you a PDF document \texttt{groupXX-submission.pdf}.

\subsubsection{Equations}

There are three types of equations available: inline equations, for
example $y=mx + c$, which appear in the text, unnumbered equations
$$y=mx + c,$$
which are presented on a line on its own, and numbered equations
\begin{equation}
  \label{eq:linear}
  y = mx + c
\end{equation}
which you can refer to at a later point (Equation~(\ref{eq:linear})).

\subsubsection{Tables and Figures}

Tables and figures are ``floating'' objects, which means that the text
can flow around it.
Note
that \texttt{figure*} and \texttt{table*} cause the corresponding
figure or table to span both columns.


\subsection{Grading}

There are two different types of grading criteria applied to your
project, with the corresponding weights shown in brackets.
\begin{description}
\item[Competitive] \ \\
  The following criteria is scored based on your rank
  in comparison with the rest of the class.
  \begin{itemize}
  \item time taken for computation (10\%)
  \item average rank for all other criteria relevant to the task, for
    example reconstruction error and sparsity (20\%)
  \end{itemize}
  The ranks will then be converted on a linear scale into a grade
  between 4 and 6.
\item[Non-competitive] \ \\
  The following criteria is scored based on an
  evaluation by the teaching assistants.
  \begin{itemize}
  \item quality of paper (30\%)
  \item quality of implementation (20\%)
  \item creativity of solution (20\%)
  \end{itemize}
\end{description}

\subsection{Submission System}

The deadline for submitting your project report is Friday, 22 June
2012.
You need to submit:
\begin{itemize}
\item PDF of paper.
\item Archive (\texttt{.tar.gz} or \texttt{.zip}) of software. Please
  do not forget to include author information in the source archive.
\end{itemize}

\textbf{Important:} Please check the submission instructions on the webpage 
as it is the most updated instructions. 

\section{Summary}

The aim of a scientific paper is to convey the idea or discovery of
the researcher to the minds of the readers. The associated software
package provides the relevant details, which are often only briefly
explained in the paper, such that the research can be reproduced.
To write good papers, identify your key idea, make your contributions
explicit, and use examples and illustrations to describe the problems
and solutions.

\section*{Acknowledgements}
The author thanks Christian Sigg for his careful reading and helpful
suggestions.

\bibliographystyle{IEEEtran}
\bibliography{howto-paper}
\end{document}
