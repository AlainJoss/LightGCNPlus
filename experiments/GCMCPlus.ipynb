{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Graph Convolutional Matrix Completion (GC-MC)\n",
    "\n",
    "In this notebook we implement the GCMC model proposed in the [paper](https://arxiv.org/abs/1706.02263).\n",
    "\n",
    "#### Architecture\n",
    "1. Encoder:\n",
    "    - create 5 bipartite graphs (one for each rating level)\n",
    "    - for each node, message pass the embeddings of its neighbors\n",
    "    - concatenate the embeddings of the different rating levels\n",
    "    - pass the concatenated embeddings through a fully connected layer (decide whether to use act-MLP or just MLP)\n",
    "2. Decoder:\n",
    "    - compute probability of each rating level\n",
    "    - compute final rating as expectation of the rating levels\n",
    "\n",
    "#### Possible loss functions\n",
    "- cross entropy: $-\\sum_{(i,j) \\in \\Omega} \\sum_{r \\in \\mathcal{R}} \\text{I}\\{M_{(i,j)} == r\\} \\log P(M_{(i,j)} == r)$\n",
    "- RMSE: $\\sqrt{\\frac{1}{|\\Omega|} \\sum_{(i,j) \\in \\Omega} (M_{(i,j)} - \\mathbb{E}[M_{(i,j)}])^2}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch\n",
    "from torch import nn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load and Preprocess Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1176952\n",
      "1176952\n",
      "tensor(1176952., device='mps:0')\n",
      "[  0   1   2   3   4   5   6   7   8   9  10  11  12  13  14  15  16  17\n",
      "  18  19  20  21  22  23  24  25  26  27  28  29  30  31  32  33  34  35\n",
      "  36  37  38  39  40  41  42  43  44  45  46  47  48  49  50  51  52  53\n",
      "  54  55  56  57  58  59  60  61  62  63  64  65  66  67  68  69  70  71\n",
      "  72  73  74  75  77  78  79  80  81  82  83  84  85  86  87  88  89  90\n",
      "  91  92  93  94  95  96  97  98  99 100 101 102 103 104 105 106 107 108\n",
      " 109 111 112 113 116 119 121 123 124 125 126 127 128 131 133 134 135 138\n",
      " 142 143 144 146 151 152 166 167 172 174 184 189 192 193 197 199 210 218\n",
      " 245 253 267 289 291 311 319 359]\n",
      "[0.         0.05277798 0.05598925 0.0567048  0.05862104 0.05882353\n",
      " 0.061199   0.06286946 0.06388766 0.06772855 0.06900655 0.07088812\n",
      " 0.07124705 0.07198158 0.07216878 0.0727393  0.07372098 0.07580981\n",
      " 0.07624929 0.07738233 0.07761505 0.08111071 0.08137884 0.08276059\n",
      " 0.08333334 0.0836242  0.08391814 0.08512565 0.0860663  0.08638684\n",
      " 0.086711   0.0873704  0.08838835 0.08873565 0.08908708 0.08944272\n",
      " 0.08980265 0.09016696 0.09090909 0.09166985 0.09284767 0.09407209\n",
      " 0.09449112 0.0949158  0.09578263 0.09622505 0.09667365 0.09712858\n",
      " 0.09759001 0.09805807 0.09853293 0.09901476 0.09950373 0.1\n",
      " 0.10050379 0.10101525 0.10153461 0.10206207 0.10259783 0.10314212\n",
      " 0.10369517 0.1042572  0.10482848 0.10540926 0.10599979 0.10660036\n",
      " 0.10721125 0.10783277 0.10846523 0.10910894 0.10976426 0.11043152\n",
      " 0.11111111 0.1118034  0.1125088  0.11322771 0.11396058 0.11547004\n",
      " 0.11624764 0.11704115 0.11785114 0.11867817 0.11952286 0.12038586\n",
      " 0.12126782 0.12216945 0.12309149 0.12403473 0.125      0.12598816\n",
      " 0.12700012 0.12803687 0.12909944 0.13018891 0.13130642 0.13245323\n",
      " 0.13363062 0.13483998 0.13608277 0.13736056 0.13867505 0.14002801\n",
      " 0.14142136 0.14285715 0.14433756 0.145865   0.14744195 0.1490712\n",
      " 0.15075567 0.15249857 0.15430336 0.15617377 0.15811388 0.16012816\n",
      " 0.16222142 0.16439898 0.16666667 0.16903085 0.1714986  0.17407766\n",
      " 0.17677669 0.1796053  0.18257418 0.18569534 0.18898225 0.19245009\n",
      " 0.19611613 0.2        0.20412414 0.2085144  0.21320072 0.21821788\n",
      " 0.2236068  0.22941573 0.23570228 0.24253564 0.25       0.2581989\n",
      " 0.26726124 0.2773501  0.28867513 0.30151135 0.31622776 0.33333334\n",
      " 0.35355338 0.3779645  0.40824828 0.4472136  0.5        0.57735026\n",
      " 0.70710677 1.                inf]\n",
      "[0.         0.05277798 0.05598925 0.0567048  0.05862104 0.05882353\n",
      " 0.061199   0.06286946 0.06388766 0.06772855 0.06900655 0.07088812\n",
      " 0.07124705 0.07198158 0.07216878 0.0727393  0.07372098 0.07580981\n",
      " 0.07624929 0.07738233 0.07761505 0.08111071 0.08137884 0.08276059\n",
      " 0.08333334 0.0836242  0.08391814 0.08512565 0.0860663  0.08638684\n",
      " 0.086711   0.0873704  0.08838835 0.08873565 0.08908708 0.08944272\n",
      " 0.08980265 0.09016696 0.09090909 0.09166985 0.09284767 0.09407209\n",
      " 0.09449112 0.0949158  0.09578263 0.09622505 0.09667365 0.09712858\n",
      " 0.09759001 0.09805807 0.09853293 0.09901476 0.09950373 0.1\n",
      " 0.10050379 0.10101525 0.10153461 0.10206207 0.10259783 0.10314212\n",
      " 0.10369517 0.1042572  0.10482848 0.10540926 0.10599979 0.10660036\n",
      " 0.10721125 0.10783277 0.10846523 0.10910894 0.10976426 0.11043152\n",
      " 0.11111111 0.1118034  0.1125088  0.11322771 0.11396058 0.11547004\n",
      " 0.11624764 0.11704115 0.11785114 0.11867817 0.11952286 0.12038586\n",
      " 0.12126782 0.12216945 0.12309149 0.12403473 0.125      0.12598816\n",
      " 0.12700012 0.12803687 0.12909944 0.13018891 0.13130642 0.13245323\n",
      " 0.13363062 0.13483998 0.13608277 0.13736056 0.13867505 0.14002801\n",
      " 0.14142136 0.14285715 0.14433756 0.145865   0.14744195 0.1490712\n",
      " 0.15075567 0.15249857 0.15430336 0.15617377 0.15811388 0.16012816\n",
      " 0.16222142 0.16439898 0.16666667 0.16903085 0.1714986  0.17407766\n",
      " 0.17677669 0.1796053  0.18257418 0.18569534 0.18898225 0.19245009\n",
      " 0.19611613 0.2        0.20412414 0.2085144  0.21320072 0.21821788\n",
      " 0.2236068  0.22941573 0.23570228 0.24253564 0.25       0.2581989\n",
      " 0.26726124 0.2773501  0.28867513 0.30151135 0.31622776 0.33333334\n",
      " 0.35355338 0.3779645  0.40824828 0.4472136  0.5        0.57735026\n",
      " 0.70710677 1.        ]\n",
      "tensor(True, device='mps:0')\n",
      "tensor(True, device='mps:0')\n",
      "[0.         0.00428086 0.00509939 ... 0.49999997 0.5        0.57735026]\n"
     ]
    }
   ],
   "source": [
    "from config import N_u, N_v, VAL_SIZE, DEVICE\n",
    "from load import load_train_data\n",
    "from preprocess import extract_users_items_ratings, create_bipartite_graph, create_degree_matrix, create_inverse_sqrt_degree_matrix\n",
    "\n",
    "# Load data\n",
    "train_df = load_train_data()\n",
    "\n",
    "# Extract adjacency lists: observed values edge index (src, tgt) and ratings (values)\n",
    "all_users, all_items, all_ratings = extract_users_items_ratings(train_df)\n",
    "\n",
    "# Create rating matrix from the triplets\n",
    "all_ratings_matrix = np.zeros((N_u, N_v))\n",
    "all_ratings_matrix[all_users, all_items] = all_ratings\n",
    "\n",
    "# Split the data into trai and val sets\n",
    "train_users, val_users, train_items, val_items, train_ratings, val_ratings = \\\n",
    "    train_test_split(all_users, all_items, all_ratings, test_size=VAL_SIZE)\n",
    "\n",
    "# convert lists to torch tensors\n",
    "train_users = torch.tensor(train_users, dtype=torch.long).to(DEVICE)\n",
    "val_users = torch.tensor(val_users, dtype=torch.long).to(DEVICE)\n",
    "train_items = torch.tensor(train_items, dtype=torch.long).to(DEVICE)\n",
    "val_items = torch.tensor(val_items, dtype=torch.long).to(DEVICE)\n",
    "train_ratings = torch.tensor(train_ratings, dtype=torch.float).to(DEVICE)\n",
    "val_ratings = torch.tensor(val_ratings, dtype=torch.float).to(DEVICE)\n",
    "\n",
    "print(len(train_users) + len(val_users))\n",
    "\n",
    "# Create adjacency lists for rating r \n",
    "u_v_r_train = []\n",
    "u_v_r_val = []\n",
    "total = 0\n",
    "for r in range(1, 6):\n",
    "    # assign value of 1 to each triplet with rating r instead of rating r\n",
    "    u_v_r_train.append((train_users[train_ratings == r], train_items[train_ratings == r], torch.ones_like(train_ratings[train_ratings == r])))\n",
    "    u_v_r_val.append((val_users[val_ratings == r], val_items[val_ratings == r], torch.ones_like(val_ratings[val_ratings == r])))\n",
    "    total += len(train_users[train_ratings == r]) + len(val_users[val_ratings == r])\n",
    "print(total)\n",
    "\n",
    "# Create bipartite graphs\n",
    "graphs_r_train = [create_bipartite_graph(u, v, r) for u, v, r in u_v_r_train]\n",
    "graphs_r_val = [create_bipartite_graph(u, v, r) for u, v, r in u_v_r_val]\n",
    "\n",
    "# check that graphs_r_train[0] doesn't contain only zero entries\n",
    "mask1 = graphs_r_train[0] == 1\n",
    "mask2 = graphs_r_train[1] == 1\n",
    "mask3 = graphs_r_train[2] == 1\n",
    "mask4 = graphs_r_train[3] == 1\n",
    "mask5 = graphs_r_train[4] == 1\n",
    "\n",
    "mask21 = graphs_r_val[0] == 1\n",
    "mask22 = graphs_r_val[1] == 1\n",
    "mask23 = graphs_r_val[2] == 1\n",
    "mask24 = graphs_r_val[3] == 1\n",
    "mask25 = graphs_r_val[4] == 1\n",
    "\n",
    "total_entries = mask1.sum() + mask2.sum() + mask3.sum() + mask4.sum() + mask5.sum() + mask21.sum() + mask22.sum() + mask23.sum() + mask24.sum() + mask25.sum()\n",
    "print(total_entries / 2)\n",
    "\n",
    "# create degree matrix for each rating\n",
    "degree_matrices_r = [create_degree_matrix(graph) for graph in graphs_r_train]\n",
    "print(np.unique(degree_matrices_r[0].cpu().numpy()))\n",
    "degree_norms_r = [create_inverse_sqrt_degree_matrix(degree_matrix) for degree_matrix in degree_matrices_r]\n",
    "print(np.unique(degree_norms_r[0].cpu().numpy()))\n",
    "# for each degree matrix, replace inf with 0\n",
    "for i in range(5):\n",
    "    degree_norms_r[i][degree_norms_r[i] == float('inf')] = 0\n",
    "print(np.unique(degree_norms_r[0].cpu().numpy()))\n",
    "\n",
    "# check that degree matrices are symmetric\n",
    "print((degree_matrices_r[0] == degree_matrices_r[0].T).all())\n",
    "\n",
    "# check that degree_norms_r[0] is symmetric\n",
    "print((degree_norms_r[0] == degree_norms_r[0].T).all())\n",
    "\n",
    "# create normalized adjacency matrices\n",
    "norm_adj_r = [degree_norm @ graph @ degree_norm for degree_norm, graph in zip(degree_norms_r, graphs_r_train)]\n",
    "print(np.unique(norm_adj_r[0].cpu().numpy()))\n",
    "\n",
    "# send adj_matrices_r to device\n",
    "norm_adj_r = [adj_matrix.to(DEVICE) for adj_matrix in norm_adj_r]\n",
    "\n",
    "# Nice, everything works"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from models import save_model_inputs\n",
    "\n",
    "class BaseLightGCN(nn.Module):\n",
    "    def __init__(self, norm_adj_r, act_fn, embedding_dim, n_layers, init_emb_std, dropout_rate):\n",
    "        super(BaseLightGCN, self).__init__()\n",
    "\n",
    "        self.norm_adj_r = norm_adj_r  # bipartite graphs (one for each rating r)\n",
    "        self.K = embedding_dim\n",
    "        self.L = n_layers \n",
    "        self.act_fn = act_fn\n",
    "\n",
    "        # Initialize embeddings\n",
    "        self.E_u = nn.Embedding(num_embeddings=N_u, embedding_dim=self.K)\n",
    "        self.E_v = nn.Embedding(num_embeddings=N_v, embedding_dim=self.K)\n",
    "        nn.init.normal_(self.E_u.weight, std=init_emb_std)\n",
    "        nn.init.normal_(self.E_v.weight, std=init_emb_std)\n",
    "\n",
    "        # Projection to output space after message passing, aggregation, and selection\n",
    "        self.mlp = self.create_mlp(dropout_rate)\n",
    "\n",
    "        # crate learnable parameter list of Q_r matrices of shape K x K\n",
    "        self.Q_r = nn.ParameterList([nn.Parameter(torch.randn(self.K, self.K)) for _ in range(5)])\n",
    "\n",
    "    def create_mlp(self, dropout_rate):\n",
    "        raise NotImplementedError(\"Derived classes must implement this method\")\n",
    "    \n",
    "    def message_passing_r(self, r) -> list[torch.Tensor]:\n",
    "        E_0 = torch.cat([self.E_u.weight, self.E_v.weight], dim=0)  # size (N_u + N_v) x K\n",
    "        E_layers = [E_0]\n",
    "        E_l = E_0\n",
    "\n",
    "        for l in range(self.L):\n",
    "            E_l = torch.mm(self.norm_adj_r[r], E_l)  # shape (N_u + N_v) x K\n",
    "            E_layers.append(E_l) \n",
    "        return E_layers\n",
    "    \n",
    "    def message_pass_r(self, r) -> list[torch.Tensor]:\n",
    "        E_u_v = torch.cat([self.E_u.weight, self.E_v.weight], dim=0)  \n",
    "        E_r = torch.mm(self.norm_adj_r[r], E_u_v) \n",
    "        return E_r\n",
    "    \n",
    "    def aggregate(self, embs: list) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Aggregate the embeddings from the message passing layers.\n",
    "        \"\"\"\n",
    "        E_agg = torch.cat(embs, dim=1)\n",
    "        return E_agg\n",
    "    \n",
    "    def select_embeddings(self, users, items, E_agg):\n",
    "        E_u, E_v = torch.split(E_agg, [N_u, N_v], dim=0)\n",
    "        # Select embeddings of users and items from the adjacency lists\n",
    "        E_u = E_u[users]\n",
    "        E_v = E_v[items]  # shape (N_train, K * (L + 1))\n",
    "        return E_u, E_v\n",
    "    \n",
    "    def forward1(self, users, items):\n",
    "\n",
    "        # TODO: try average aggregation, or project to each embedding to dim K from 5K, and then use bilinear decoder\n",
    "\n",
    "        E_r = [self.message_pass_r(r) for r in range(5)]\n",
    "        E_agg = self.aggregate(E_r)\n",
    "        # input shape for attention: (N_u + N_v) x (K * 2)\n",
    "        E_u_sel, E_v_sel = self.select_embeddings(users, items, E_agg)\n",
    "\n",
    "        # Project to output space\n",
    "        concat_users_items = torch.cat([E_u_sel, E_v_sel], dim=1)  # shape (N_train, 2K * (L + 1))\n",
    "        out = self.mlp(concat_users_items).squeeze()  \n",
    "        return out \n",
    "\n",
    "    def forward(self, users, items):\n",
    "        # TODO: use lightGCN message passing to aggregate information over hops \n",
    "        E_r = [self.message_pass_r(r) for r in range(5)]\n",
    "        # TODO: project down to K using MLP instead of aggregation\n",
    "        E_agg = torch.mean(torch.stack(E_r), dim=0)\n",
    "        E_u_sel, E_v_sel = self.select_embeddings(users, items, E_agg)\n",
    "\n",
    "        # Compute logits for each rating level\n",
    "        logits = []\n",
    "        for r in range(5):\n",
    "            logit = torch.einsum('ij,jk,ik->i', E_u_sel, self.Q_r[r], E_v_sel)\n",
    "            logits.append(logit)\n",
    "\n",
    "        logits = torch.stack(logits, dim=1)  # Shape: (batch_size, 5)\n",
    "\n",
    "        # Compute the softmax probabilities\n",
    "        softmax_probs = torch.softmax(logits, dim=1)  # Shape: (batch_size, 5)\n",
    "\n",
    "        # Compute the final rating prediction as the expected value of the softmax probabilities\n",
    "        ratings = torch.arange(1, 6).float().to(DEVICE)  # Shape: (5,)\n",
    "        preds = torch.sum(softmax_probs * ratings, dim=1)  # Shape: (batch_size,)\n",
    "        \n",
    "        # assert all preds are in [1, 5]\n",
    "        #assert (preds >= 1).all() and (preds <= 5).all()\n",
    "\n",
    "        return preds\n",
    "\n",
    "        \n",
    "\n",
    "    def get_ratings(self, users, items):\n",
    "        return self.forward(users, items)\n",
    "\n",
    "class LightGCN(BaseLightGCN):\n",
    "    def __init__(self, norm_adj_r, act_fn, embedding_dim, n_layers, init_emb_std, dropout_rate, projections):\n",
    "        self.projections = projections\n",
    "        super().__init__(norm_adj_r, act_fn, embedding_dim, n_layers, init_emb_std, dropout_rate)\n",
    "\n",
    "        # For reproducibility after training\n",
    "        # save_model_inputs(norm_adj_r, act_fn, embedding_dim, n_layers, init_emb_std, dropout_rate, projections)\n",
    "\n",
    "    def create_mlp(self, dropout_rate):\n",
    "        layers = []\n",
    "        input_dim = self.K * 5 * 2\n",
    "        for proj in self.projections:\n",
    "            output_dim = self.K * proj\n",
    "            layers.append(nn.Linear(input_dim, output_dim))\n",
    "            layers.append(self.act_fn)\n",
    "            layers.append(nn.Dropout(dropout_rate))\n",
    "            input_dim = output_dim\n",
    "        layers.append(nn.Linear(input_dim, 1))\n",
    "        return nn.Sequential(*layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training LightGCN with K=28, L=3, C=(4,)\n",
      "Epoch 0 - Train loss: 1.9872 - Val loss: 1.8727 - Val loss original: 1.8727\n",
      "Epoch 1 - Train loss: 1.8735 - Val loss: 1.3769 - Val loss original: 1.3769\n",
      "Epoch 2 - Train loss: 1.3639 - Val loss: 1.4100 - Val loss original: 1.4100\n",
      "Epoch 3 - Train loss: 1.4051 - Val loss: 1.2589 - Val loss original: 1.2589\n",
      "Epoch 4 - Train loss: 1.2393 - Val loss: 1.3009 - Val loss original: 1.3009\n",
      "Epoch 5 - Train loss: 1.2759 - Val loss: 1.3018 - Val loss original: 1.3018\n",
      "Epoch 6 - Train loss: 1.2762 - Val loss: 1.2976 - Val loss original: 1.2976\n",
      "Epoch 7 - Train loss: 1.2723 - Val loss: 1.2938 - Val loss original: 1.2938\n",
      "Epoch 8 - Train loss: 1.2689 - Val loss: 1.2896 - Val loss original: 1.2896\n",
      "Epoch 9 - Train loss: 1.2651 - Val loss: 1.2832 - Val loss original: 1.2832\n",
      "Epoch 10 - Train loss: 1.2594 - Val loss: 1.2707 - Val loss original: 1.2707\n",
      "Epoch 11 - Train loss: 1.2482 - Val loss: 1.2427 - Val loss original: 1.2427\n",
      "Epoch 12 - Train loss: 1.2230 - Val loss: 1.1835 - Val loss original: 1.1835\n",
      "Epoch 13 - Train loss: 1.1716 - Val loss: 1.2496 - Val loss original: 1.2496\n",
      "Epoch 14 - Train loss: 1.2556 - Val loss: 1.1648 - Val loss original: 1.1648\n",
      "Epoch 15 - Train loss: 1.1631 - Val loss: 1.1704 - Val loss original: 1.1704\n",
      "Epoch 16 - Train loss: 1.1604 - Val loss: 1.1883 - Val loss original: 1.1883\n",
      "Epoch 17 - Train loss: 1.1751 - Val loss: 1.1819 - Val loss original: 1.1819\n",
      "Epoch 18 - Train loss: 1.1695 - Val loss: 1.1505 - Val loss original: 1.1505\n",
      "Epoch 19 - Train loss: 1.1432 - Val loss: 1.1362 - Val loss original: 1.1362\n",
      "Epoch 20 - Train loss: 1.1382 - Val loss: 1.1577 - Val loss original: 1.1577\n",
      "Epoch 21 - Train loss: 1.1639 - Val loss: 1.1059 - Val loss original: 1.1059\n",
      "Epoch 22 - Train loss: 1.1070 - Val loss: 1.1410 - Val loss original: 1.1410\n",
      "Epoch 23 - Train loss: 1.1386 - Val loss: 1.1027 - Val loss original: 1.1027\n",
      "Epoch 24 - Train loss: 1.1043 - Val loss: 1.1044 - Val loss original: 1.1044\n",
      "Epoch 25 - Train loss: 1.1118 - Val loss: 1.0932 - Val loss original: 1.0932\n",
      "Epoch 26 - Train loss: 1.0994 - Val loss: 1.0883 - Val loss original: 1.0883\n",
      "Epoch 27 - Train loss: 1.0882 - Val loss: 1.1041 - Val loss original: 1.1041\n",
      "Epoch 28 - Train loss: 1.1011 - Val loss: 1.0796 - Val loss original: 1.0796\n",
      "Epoch 29 - Train loss: 1.0796 - Val loss: 1.0901 - Val loss original: 1.0901\n",
      "Epoch 30 - Train loss: 1.0949 - Val loss: 1.0787 - Val loss original: 1.0787\n",
      "Epoch 31 - Train loss: 1.0820 - Val loss: 1.0825 - Val loss original: 1.0825\n",
      "Epoch 32 - Train loss: 1.0810 - Val loss: 1.0883 - Val loss original: 1.0883\n",
      "Epoch 33 - Train loss: 1.0860 - Val loss: 1.0717 - Val loss original: 1.0717\n",
      "Epoch 34 - Train loss: 1.0733 - Val loss: 1.0874 - Val loss original: 1.0874\n",
      "Epoch 35 - Train loss: 1.0920 - Val loss: 1.0724 - Val loss original: 1.0724\n",
      "Epoch 36 - Train loss: 1.0731 - Val loss: 1.0859 - Val loss original: 1.0859\n",
      "Epoch 37 - Train loss: 1.0841 - Val loss: 1.0714 - Val loss original: 1.0714\n",
      "Epoch 38 - Train loss: 1.0724 - Val loss: 1.0845 - Val loss original: 1.0845\n",
      "Epoch 39 - Train loss: 1.0887 - Val loss: 1.0729 - Val loss original: 1.0729\n",
      "Epoch 40 - Train loss: 1.0735 - Val loss: 1.0823 - Val loss original: 1.0823\n",
      "Epoch 41 - Train loss: 1.0818 - Val loss: 1.0712 - Val loss original: 1.0712\n",
      "Epoch 42 - Train loss: 1.0743 - Val loss: 1.0752 - Val loss original: 1.0752\n",
      "Epoch 43 - Train loss: 1.0796 - Val loss: 1.0817 - Val loss original: 1.0817\n",
      "Epoch 44 - Train loss: 1.0829 - Val loss: 1.0690 - Val loss original: 1.0690\n",
      "Epoch 45 - Train loss: 1.0720 - Val loss: 1.0835 - Val loss original: 1.0835\n",
      "Epoch 46 - Train loss: 1.0886 - Val loss: 1.0891 - Val loss original: 1.0891\n",
      "Epoch 47 - Train loss: 1.0892 - Val loss: 1.0719 - Val loss original: 1.0719\n",
      "Epoch 48 - Train loss: 1.0732 - Val loss: 1.0930 - Val loss original: 1.0930\n",
      "Epoch 49 - Train loss: 1.0975 - Val loss: 1.0850 - Val loss original: 1.0850\n",
      "Epoch 50 - Train loss: 1.0839 - Val loss: 1.0815 - Val loss original: 1.0815\n",
      "Epoch 51 - Train loss: 1.0805 - Val loss: 1.0829 - Val loss original: 1.0829\n",
      "Epoch 52 - Train loss: 1.0865 - Val loss: 1.0687 - Val loss original: 1.0687\n",
      "Epoch 53 - Train loss: 1.0706 - Val loss: 1.0803 - Val loss original: 1.0803\n",
      "Epoch 54 - Train loss: 1.0799 - Val loss: 1.0675 - Val loss original: 1.0675\n",
      "Epoch 55 - Train loss: 1.0691 - Val loss: 1.0784 - Val loss original: 1.0784\n",
      "Epoch 56 - Train loss: 1.0824 - Val loss: 1.0695 - Val loss original: 1.0695\n",
      "Epoch 57 - Train loss: 1.0705 - Val loss: 1.0756 - Val loss original: 1.0756\n",
      "Epoch 58 - Train loss: 1.0757 - Val loss: 1.0711 - Val loss original: 1.0711\n",
      "Epoch 59 - Train loss: 1.0737 - Val loss: 1.0712 - Val loss original: 1.0712\n",
      "Epoch 60 - Train loss: 1.0739 - Val loss: 1.0755 - Val loss original: 1.0755\n",
      "Epoch 61 - Train loss: 1.0762 - Val loss: 1.0687 - Val loss original: 1.0687\n",
      "Epoch 62 - Train loss: 1.0709 - Val loss: 1.0760 - Val loss original: 1.0760\n",
      "Epoch 63 - Train loss: 1.0801 - Val loss: 1.0739 - Val loss original: 1.0739\n",
      "Epoch 64 - Train loss: 1.0758 - Val loss: 1.0690 - Val loss original: 1.0690\n",
      "Epoch 65 - Train loss: 1.0716 - Val loss: 1.0770 - Val loss original: 1.0770\n",
      "Epoch 66 - Train loss: 1.0812 - Val loss: 1.0764 - Val loss original: 1.0764\n",
      "Epoch 67 - Train loss: 1.0778 - Val loss: 1.0692 - Val loss original: 1.0692\n",
      "Epoch 68 - Train loss: 1.0713 - Val loss: 1.0774 - Val loss original: 1.0774\n",
      "Epoch 69 - Train loss: 1.0810 - Val loss: 1.0775 - Val loss original: 1.0775\n",
      "Epoch 70 - Train loss: 1.0782 - Val loss: 1.0684 - Val loss original: 1.0684\n",
      "Epoch 71 - Train loss: 1.0703 - Val loss: 1.0773 - Val loss original: 1.0773\n",
      "Epoch 72 - Train loss: 1.0811 - Val loss: 1.0784 - Val loss original: 1.0784\n",
      "Epoch 73 - Train loss: 1.0791 - Val loss: 1.0673 - Val loss original: 1.0673\n",
      "Epoch 74 - Train loss: 1.0693 - Val loss: 1.0784 - Val loss original: 1.0784\n",
      "Epoch 75 - Train loss: 1.0823 - Val loss: 1.0800 - Val loss original: 1.0800\n",
      "Epoch 76 - Train loss: 1.0801 - Val loss: 1.0685 - Val loss original: 1.0685\n",
      "Epoch 77 - Train loss: 1.0698 - Val loss: 1.0823 - Val loss original: 1.0823\n",
      "Epoch 78 - Train loss: 1.0861 - Val loss: 1.0789 - Val loss original: 1.0789\n",
      "Epoch 79 - Train loss: 1.0788 - Val loss: 1.0709 - Val loss original: 1.0709\n",
      "Epoch 80 - Train loss: 1.0717 - Val loss: 1.0839 - Val loss original: 1.0839\n",
      "Epoch 81 - Train loss: 1.0879 - Val loss: 1.0738 - Val loss original: 1.0738\n",
      "Epoch 82 - Train loss: 1.0745 - Val loss: 1.0729 - Val loss original: 1.0729\n",
      "Epoch 83 - Train loss: 1.0738 - Val loss: 1.0794 - Val loss original: 1.0794\n",
      "Epoch 84 - Train loss: 1.0833 - Val loss: 1.0687 - Val loss original: 1.0687\n",
      "Epoch 85 - Train loss: 1.0704 - Val loss: 1.0737 - Val loss original: 1.0737\n",
      "Epoch 86 - Train loss: 1.0748 - Val loss: 1.0729 - Val loss original: 1.0729\n",
      "Epoch 87 - Train loss: 1.0763 - Val loss: 1.0675 - Val loss original: 1.0675\n",
      "Epoch 88 - Train loss: 1.0700 - Val loss: 1.0728 - Val loss original: 1.0728\n",
      "Epoch 89 - Train loss: 1.0743 - Val loss: 1.0698 - Val loss original: 1.0698\n",
      "Epoch 90 - Train loss: 1.0732 - Val loss: 1.0676 - Val loss original: 1.0676\n",
      "Epoch 91 - Train loss: 1.0705 - Val loss: 1.0713 - Val loss original: 1.0713\n",
      "Epoch 92 - Train loss: 1.0733 - Val loss: 1.0700 - Val loss original: 1.0700\n",
      "Epoch 93 - Train loss: 1.0735 - Val loss: 1.0679 - Val loss original: 1.0679\n",
      "Epoch 94 - Train loss: 1.0705 - Val loss: 1.0686 - Val loss original: 1.0686\n",
      "Epoch 95 - Train loss: 1.0711 - Val loss: 1.0702 - Val loss original: 1.0702\n",
      "Epoch 96 - Train loss: 1.0738 - Val loss: 1.0713 - Val loss original: 1.0713\n",
      "Epoch 97 - Train loss: 1.0734 - Val loss: 1.0688 - Val loss original: 1.0688\n",
      "Epoch 98 - Train loss: 1.0721 - Val loss: 1.0676 - Val loss original: 1.0676\n",
      "Epoch 99 - Train loss: 1.0703 - Val loss: 1.0678 - Val loss original: 1.0678\n",
      "Epoch 100 - Train loss: 1.0704 - Val loss: 1.0687 - Val loss original: 1.0687\n",
      "Epoch 101 - Train loss: 1.0720 - Val loss: 1.0711 - Val loss original: 1.0711\n",
      "Epoch 102 - Train loss: 1.0731 - Val loss: 1.0716 - Val loss original: 1.0716\n",
      "Epoch 103 - Train loss: 1.0753 - Val loss: 1.0752 - Val loss original: 1.0752\n",
      "Epoch 104 - Train loss: 1.0766 - Val loss: 1.0738 - Val loss original: 1.0738\n",
      "Epoch 105 - Train loss: 1.0774 - Val loss: 1.0753 - Val loss original: 1.0753\n",
      "Epoch 106 - Train loss: 1.0765 - Val loss: 1.0704 - Val loss original: 1.0704\n",
      "Epoch 107 - Train loss: 1.0735 - Val loss: 1.0679 - Val loss original: 1.0679\n",
      "Epoch 108 - Train loss: 1.0700 - Val loss: 1.0673 - Val loss original: 1.0673\n",
      "Epoch 109 - Train loss: 1.0696 - Val loss: 1.0684 - Val loss original: 1.0684\n",
      "Epoch 110 - Train loss: 1.0718 - Val loss: 1.0714 - Val loss original: 1.0714\n",
      "Epoch 111 - Train loss: 1.0733 - Val loss: 1.0706 - Val loss original: 1.0706\n",
      "Epoch 112 - Train loss: 1.0743 - Val loss: 1.0714 - Val loss original: 1.0714\n",
      "Epoch 113 - Train loss: 1.0732 - Val loss: 1.0692 - Val loss original: 1.0692\n",
      "Epoch 114 - Train loss: 1.0723 - Val loss: 1.0684 - Val loss original: 1.0684\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[15], line 35\u001b[0m\n\u001b[1;32m     33\u001b[0m optimizer \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39moptim\u001b[38;5;241m.\u001b[39mAdam(model\u001b[38;5;241m.\u001b[39mparameters(), lr\u001b[38;5;241m=\u001b[39mLR, weight_decay\u001b[38;5;241m=\u001b[39mWEIGHT_DECAY)\n\u001b[1;32m     34\u001b[0m loss_fn \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mMSELoss()\n\u001b[0;32m---> 35\u001b[0m train_rmse, val_rmse_std, val_rmse_orig \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mloss_fn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_users\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_items\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_ratings\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_users\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_items\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_ratings\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_ratings\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmeans\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mEPOCHS\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mSTOP_THRESHOLD\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbosity\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     36\u001b[0m report_training_results(train_rmse, val_rmse_std, val_rmse_orig)\n",
      "File \u001b[0;32m~/Desktop/CIL-project/movie_rating_prediction/src/train.py:106\u001b[0m, in \u001b[0;36mtrain_model\u001b[0;34m(model, optimizer, loss_fn, train_users, train_items, standardized_train_ratings, val_users, val_items, orig_val_ratings, standardized_val_ratings, means, stds, n_epochs, stop_threshold, save_best_model, verbosity)\u001b[0m\n\u001b[1;32m    104\u001b[0m val_losses_orig \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m    105\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(n_epochs):\n\u001b[0;32m--> 106\u001b[0m     train_loss \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_one_epoch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mloss_fn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_users\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_items\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstandardized_train_ratings\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    107\u001b[0m     val_loss_standardized \u001b[38;5;241m=\u001b[39m evaluate_one_epoch(model, loss_fn, val_users, val_items, standardized_val_ratings)\n\u001b[1;32m    108\u001b[0m     val_loss_original \u001b[38;5;241m=\u001b[39m evaluate_one_epoch_original(model, loss_fn, val_users, val_items, orig_val_ratings, means, stds)\n",
      "File \u001b[0;32m~/Desktop/CIL-project/movie_rating_prediction/src/train.py:30\u001b[0m, in \u001b[0;36mtrain_one_epoch\u001b[0;34m(model, optimizer, loss_fn, users, items, ratings)\u001b[0m\n\u001b[1;32m     27\u001b[0m J\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[1;32m     28\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[0;32m---> 30\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mJ\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from train import train_model\n",
    "from config import DEVICE\n",
    "from train import train_model\n",
    "from postprocess import report_training_results\n",
    "\n",
    "# Model and optimizer hyperparameters\n",
    "L=4\n",
    "K=28\n",
    "INIT_EMBS_STD=0.075\n",
    "LR=0.1\n",
    "WEIGHT_DECAY=0.00005\n",
    "DROPOUT=0.5\n",
    "PROJECTIONS = (5,)\n",
    "ACT_FN = nn.GELU()\n",
    "\n",
    "# Train loop hyperparameters\n",
    "EPOCHS=2000\n",
    "STOP_THRESHOLD=1e-06\n",
    "\n",
    "ks = [28, 30, 32]\n",
    "layers = [3, 4]\n",
    "projections = [(4,), (2,), (1,)]  # (8,), \n",
    "\n",
    "# to not change train loop (should actually separate concerns better and work with the reversing in the postprocessing)\n",
    "means = np.zeros(N_v)\n",
    "stds = np.ones(N_v)\n",
    "\n",
    "for K in ks:\n",
    "    for L in layers:\n",
    "        for C in projections:\n",
    "            print(f\"Training LightGCN with K={K}, L={L}, C={C}\")\n",
    "            model = LightGCN(norm_adj_r, ACT_FN, K, L, INIT_EMBS_STD, DROPOUT, C).to(DEVICE)\n",
    "            optimizer = torch.optim.Adam(model.parameters(), lr=LR, weight_decay=WEIGHT_DECAY)\n",
    "            loss_fn = nn.MSELoss()\n",
    "            train_rmse, val_rmse_std, val_rmse_orig = train_model(model, optimizer, loss_fn, train_users, train_items, train_ratings, val_users, val_items, val_ratings, val_ratings, means, stds, EPOCHS, STOP_THRESHOLD, False, verbosity=1)\n",
    "            report_training_results(train_rmse, val_rmse_std, val_rmse_orig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
