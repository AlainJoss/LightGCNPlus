{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LightGCNPlus\n",
    "\n",
    "### TODO: \n",
    "- try to learn the weighting of the different layers in the message passing mechanism.\n",
    "\n",
    "### Scope\n",
    "In this notebook we train different variants of our LightGCNPlus model on different hyperparameters. We then re-train the best performing model and evaluate it on Kaggle's public test set.\n",
    "\n",
    "### About the model\n",
    "LightGCNPlus is a graph-based collaborative filtering model that extends the [original LightGCN model](https://arxiv.org/pdf/2002.02126.pdf). The original LightGCN is only able to rank items. We thus refine the model to be able to predict ratings. Our LightGCNPlus model is composed of the following components:\n",
    "- message passing: $e_u^{(l)} = \\sum_{v \\in N(u)} \\frac{sr_{u,v}}{\\sqrt{|N(u)||N(v)|}} e_v^{(l-1)}$, where $sr_{u,v}$ is the standardized rating of user $u$ for item $v$.\n",
    "- aggregation mechanism: $h_u = \\text{concat}(e_u^{(0)}, e_u^{(1)}, ..., e_u^{(L)})$, where $L$ is the number of layers in the message passing mechanism.\n",
    "- output layer: $\\hat{R}_{(i,j)} = \\text{MLP}(\\text{concat}(h_i, h_j))$, where $\\text{MLP}$ is a multi-layer perceptron that projects the embedding couples of observed ratings to the output space.\n",
    "\n",
    "How our model differs from the original GCMC model:\n",
    "- In the message passing layer we use the layered message passing (multi-hop) mechanism of LightGCN instead of just a one-hop pass.\n",
    "- To project to the output space we use a multi-layer perceptron instead of the softmax approach to compute probabilities over ratings.\n",
    "\n",
    "### Hyperparameter tuning\n",
    "The hyperparameters comprise both choices about the models' architecture and the training procedure.\n",
    "We perform a grid search over the following hyperparameters:\n",
    "- K: Embedding size.\n",
    "- L: Number of layers in the message passing mechanism.\n",
    "- PROJECTIONS: Architecture of the MLP that projects the embedding couples of observed ratings to the output space.\n",
    "\n",
    "Other hyperparameters such as the learning rate, the batch size, the number of epochs, the optimizer, etc. have been selected based on the results on previous experiments. Thus, to reduce the computational cost of the grid search, the following hyperparameters are kept constant:\n",
    "- LR: learning rate.\n",
    "- INIT_EMBS_STD: standard deviation of the normal distribution used to initialize the embeddings.\n",
    "- WEIGHT_DECAY: weight decay coefficient for the Adam optimizer.\n",
    "- DROPOUT: dropout rate used in the MLP's hidden layers.\n",
    "- ACT_FN: activation function used in the MLP's hidden layers.\n",
    "\n",
    "We also define hyperparameters for the training procedure:\n",
    "- EPOCHS: number of backpropagation steps.\n",
    "- STOP_THRESHOLD: minimum improvement in the validation loss to continue training.\n",
    "\n",
    "### Training\n",
    "- loss functions:\n",
    "    - train: $\\text{MSE} = \\frac{1}{|\\Omega|} \\sum_{(i,j) \\in \\Omega} (M_{(i,j)} - \\hat{M}_{(i,j)})^2$\n",
    "    - eval: $\\text{RMSE} = \\sqrt{\\text{MSE}}$\n",
    "- optimizer: Adam\n",
    "- Batching: no mini-batching of the training data, doesn't improve performance and slows convergence down. \n",
    "- Regularization: dropout and L2 regularization (weight decay in the Adam optimizer)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Graph Convolutional Matrix Completion Plus (GCMCPlus)\n",
    "\n",
    "### Scope\n",
    "In this notebook we train different variants of our GCMCPlus model on different hyperparameters. We then re-train the best performing model and evaluate it on Kaggle's public test set.\n",
    "\n",
    "### About the model\n",
    "GCMCPlus is a graph-based collaborative filtering model that extends the [original GCMC model](https://arxiv.org/abs/1706.02263). We refine the model by taking ideas from the [neural collaborative filtering paper](https://arxiv.org/pdf/1708.05031.pdf) and the [LightGCN model](https://arxiv.org/pdf/2002.02126.pdf). Our GCMCPlus model is composed of the following components:\n",
    "- preprocessing: for each rating level $r$, create $\\tilde{A}_r = D_r^{-\\frac{1}{2}} A_r D_r^{-\\frac{1}{2}}$, where $A_r$ is the bipartite graph of the interactions of rating level $r$, and $D_r$ is the diagonal degree matrix of $A_r$.\n",
    "- message passing: $e_{u_{r}}^{(l)} = \\tilde{A}_{(u,v)_{r}} e_{v_{r}}^{(l-1)}$, where $e_{u_{r}}^{(0)}$ is the learnable embedding of user $u$ for rating level $r$.\n",
    "- aggregation mechanism of layered message passing at rating level $r$: $h_{u_{r}} = \\frac{1}{L} \\sum_{l=1}^{L} e_{u_{r}}^{(l)}$, where $L$ is the number of layers in the message passing mechanism.\n",
    "- aggregation mechanism of different rating levels: $h_u = \\text{concat}(h_{u_1}, h_{u_2}, ..., h_{u_{|\\mathcal{R}|}})$, where $|\\mathcal{R}|$ is the number of rating levels.\n",
    "- output layer: $\\hat{M}_{(i,j)} = \\text{MLP}(\\text{concat}(h_i, h_j))$, where $\\text{MLP}$ is a multi-layer perceptron that projects the embedding couples of observed ratings to the output space.\n",
    "\n",
    "#### Architecture\n",
    "1. Encoder:\n",
    "    - create 5 bipartite graphs (one for each rating level)\n",
    "    - for each node, message pass the embeddings of its neighbors\n",
    "    - concatenate the embeddings of the different rating levels\n",
    "    - pass the concatenated embeddings through a fully connected layer (decide whether to use act-MLP or just MLP)\n",
    "2. Decoder:\n",
    "    - compute probability of each rating level\n",
    "    - compute final rating as expectation of the rating levels\n",
    "\n",
    "#### Possible loss functions\n",
    "- cross entropy: $-\\sum_{(i,j) \\in \\Omega} \\sum_{r \\in \\mathcal{R}} \\text{I}\\{M_{(i,j)} == r\\} \\log P(M_{(i,j)} == r)$\n",
    "- RMSE: $\\sqrt{\\frac{1}{|\\Omega|} \\sum_{(i,j) \\in \\Omega} (M_{(i,j)} - \\mathbb{E}[M_{(i,j)}])^2}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch\n",
    "from torch import nn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load and Preprocess Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1176952\n",
      "1176952\n",
      "tensor(1176952., device='mps:0')\n",
      "[  0   1   2   3   4   5   6   7   8   9  10  11  12  13  14  15  16  17\n",
      "  18  19  20  21  22  23  24  25  26  27  28  29  30  31  32  33  34  35\n",
      "  36  37  38  39  40  41  42  43  44  45  46  47  48  49  50  51  52  53\n",
      "  54  55  56  57  58  59  60  61  62  63  64  65  66  67  68  69  70  71\n",
      "  72  73  74  75  76  77  78  79  80  81  82  83  84  85  86  87  88  89\n",
      "  90  91  92  93  94  96  97  99 100 101 102 103 104 105 106 107 108 109\n",
      " 111 112 114 117 118 119 120 122 124 125 126 127 131 133 134 135 137 138\n",
      " 139 142 143 148 152 155 167 168 170 171 175 183 188 192 193 194 198 211\n",
      " 214 244 253 267 291 293 309 320 360]\n",
      "[0.         0.05270463 0.0559017  0.05688801 0.05842062 0.05862104\n",
      " 0.061199   0.06286946 0.06401844 0.06835859 0.06884284 0.07106691\n",
      " 0.07179581 0.07198158 0.07216878 0.0729325  0.07392213 0.07559289\n",
      " 0.07647192 0.0766965  0.07715168 0.07738233 0.08032193 0.08111071\n",
      " 0.08219949 0.0836242  0.08391814 0.08481889 0.08512565 0.08543577\n",
      " 0.0860663  0.08638684 0.086711   0.0873704  0.08873565 0.08908708\n",
      " 0.08944272 0.08980265 0.09053575 0.09128709 0.09166985 0.09205746\n",
      " 0.09245003 0.09365858 0.09449112 0.0949158  0.09578263 0.09622505\n",
      " 0.09667365 0.09712858 0.09759001 0.09805807 0.09853293 0.09901476\n",
      " 0.09950373 0.1        0.10050379 0.10153461 0.10206207 0.10314212\n",
      " 0.10369517 0.1042572  0.10482848 0.10540926 0.10599979 0.10660036\n",
      " 0.10721125 0.10783277 0.10846523 0.10910894 0.10976426 0.11043152\n",
      " 0.11111111 0.1118034  0.1125088  0.11322771 0.11396058 0.11470786\n",
      " 0.11547004 0.11624764 0.11704115 0.11785114 0.11867817 0.11952286\n",
      " 0.12038586 0.12126782 0.12216945 0.12309149 0.12403473 0.125\n",
      " 0.12598816 0.12700012 0.12803687 0.12909944 0.13018891 0.13130642\n",
      " 0.13245323 0.13363062 0.13483998 0.13608277 0.13736056 0.13867505\n",
      " 0.14002801 0.14142136 0.14285715 0.14433756 0.145865   0.14744195\n",
      " 0.1490712  0.15075567 0.15249857 0.15430336 0.15617377 0.15811388\n",
      " 0.16012816 0.16222142 0.16439898 0.16666667 0.16903085 0.1714986\n",
      " 0.17407766 0.17677669 0.1796053  0.18257418 0.18569534 0.18898225\n",
      " 0.19245009 0.19611613 0.2        0.20412414 0.2085144  0.21320072\n",
      " 0.21821788 0.2236068  0.22941573 0.23570228 0.24253564 0.25\n",
      " 0.2581989  0.26726124 0.2773501  0.28867513 0.30151135 0.31622776\n",
      " 0.33333334 0.35355338 0.3779645  0.40824828 0.4472136  0.5\n",
      " 0.57735026 0.70710677 1.                inf]\n",
      "[0.         0.05270463 0.0559017  0.05688801 0.05842062 0.05862104\n",
      " 0.061199   0.06286946 0.06401844 0.06835859 0.06884284 0.07106691\n",
      " 0.07179581 0.07198158 0.07216878 0.0729325  0.07392213 0.07559289\n",
      " 0.07647192 0.0766965  0.07715168 0.07738233 0.08032193 0.08111071\n",
      " 0.08219949 0.0836242  0.08391814 0.08481889 0.08512565 0.08543577\n",
      " 0.0860663  0.08638684 0.086711   0.0873704  0.08873565 0.08908708\n",
      " 0.08944272 0.08980265 0.09053575 0.09128709 0.09166985 0.09205746\n",
      " 0.09245003 0.09365858 0.09449112 0.0949158  0.09578263 0.09622505\n",
      " 0.09667365 0.09712858 0.09759001 0.09805807 0.09853293 0.09901476\n",
      " 0.09950373 0.1        0.10050379 0.10153461 0.10206207 0.10314212\n",
      " 0.10369517 0.1042572  0.10482848 0.10540926 0.10599979 0.10660036\n",
      " 0.10721125 0.10783277 0.10846523 0.10910894 0.10976426 0.11043152\n",
      " 0.11111111 0.1118034  0.1125088  0.11322771 0.11396058 0.11470786\n",
      " 0.11547004 0.11624764 0.11704115 0.11785114 0.11867817 0.11952286\n",
      " 0.12038586 0.12126782 0.12216945 0.12309149 0.12403473 0.125\n",
      " 0.12598816 0.12700012 0.12803687 0.12909944 0.13018891 0.13130642\n",
      " 0.13245323 0.13363062 0.13483998 0.13608277 0.13736056 0.13867505\n",
      " 0.14002801 0.14142136 0.14285715 0.14433756 0.145865   0.14744195\n",
      " 0.1490712  0.15075567 0.15249857 0.15430336 0.15617377 0.15811388\n",
      " 0.16012816 0.16222142 0.16439898 0.16666667 0.16903085 0.1714986\n",
      " 0.17407766 0.17677669 0.1796053  0.18257418 0.18569534 0.18898225\n",
      " 0.19245009 0.19611613 0.2        0.20412414 0.2085144  0.21320072\n",
      " 0.21821788 0.2236068  0.22941573 0.23570228 0.24253564 0.25\n",
      " 0.2581989  0.26726124 0.2773501  0.28867513 0.30151135 0.31622776\n",
      " 0.33333334 0.35355338 0.3779645  0.40824828 0.4472136  0.5\n",
      " 0.57735026 0.70710677 1.        ]\n",
      "tensor(True, device='mps:0')\n",
      "tensor(True, device='mps:0')\n",
      "[0.         0.00427491 0.00509939 ... 0.49999997 0.5        0.57735026]\n"
     ]
    }
   ],
   "source": [
    "from config import N_u, N_v, VAL_SIZE, DEVICE\n",
    "from load import load_train_data\n",
    "from preprocess import extract_users_items_ratings, create_bipartite_graph, create_degree_matrix, create_inverse_sqrt_degree_matrix\n",
    "\n",
    "# Load data\n",
    "train_df = load_train_data()\n",
    "\n",
    "# Extract adjacency lists: observed values edge index (src, tgt) and ratings (values)\n",
    "all_users, all_items, all_ratings = extract_users_items_ratings(train_df)\n",
    "\n",
    "# Create rating matrix from the triplets\n",
    "all_ratings_matrix = np.zeros((N_u, N_v))\n",
    "all_ratings_matrix[all_users, all_items] = all_ratings\n",
    "\n",
    "# Split the data into trai and val sets\n",
    "train_users, val_users, train_items, val_items, train_ratings, val_ratings = \\\n",
    "    train_test_split(all_users, all_items, all_ratings, test_size=VAL_SIZE)\n",
    "\n",
    "# convert lists to torch tensors\n",
    "train_users = torch.tensor(train_users, dtype=torch.long).to(DEVICE)\n",
    "val_users = torch.tensor(val_users, dtype=torch.long).to(DEVICE)\n",
    "train_items = torch.tensor(train_items, dtype=torch.long).to(DEVICE)\n",
    "val_items = torch.tensor(val_items, dtype=torch.long).to(DEVICE)\n",
    "train_ratings = torch.tensor(train_ratings, dtype=torch.float).to(DEVICE)\n",
    "val_ratings = torch.tensor(val_ratings, dtype=torch.float).to(DEVICE)\n",
    "\n",
    "print(len(train_users) + len(val_users))\n",
    "\n",
    "# Create adjacency lists for rating r \n",
    "u_v_r_train = []\n",
    "u_v_r_val = []\n",
    "total = 0\n",
    "for r in range(1, 6):\n",
    "    # assign value of 1 to each triplet with rating r instead of rating r\n",
    "    u_v_r_train.append((train_users[train_ratings == r], train_items[train_ratings == r], torch.ones_like(train_ratings[train_ratings == r])))\n",
    "    u_v_r_val.append((val_users[val_ratings == r], val_items[val_ratings == r], torch.ones_like(val_ratings[val_ratings == r])))\n",
    "    total += len(train_users[train_ratings == r]) + len(val_users[val_ratings == r])\n",
    "print(total)\n",
    "\n",
    "# Create bipartite graphs\n",
    "graphs_r_train = [create_bipartite_graph(u, v, r) for u, v, r in u_v_r_train]\n",
    "graphs_r_val = [create_bipartite_graph(u, v, r) for u, v, r in u_v_r_val]\n",
    "\n",
    "# check that graphs_r_train[0] doesn't contain only zero entries\n",
    "mask1 = graphs_r_train[0] == 1\n",
    "mask2 = graphs_r_train[1] == 1\n",
    "mask3 = graphs_r_train[2] == 1\n",
    "mask4 = graphs_r_train[3] == 1\n",
    "mask5 = graphs_r_train[4] == 1\n",
    "\n",
    "mask21 = graphs_r_val[0] == 1\n",
    "mask22 = graphs_r_val[1] == 1\n",
    "mask23 = graphs_r_val[2] == 1\n",
    "mask24 = graphs_r_val[3] == 1\n",
    "mask25 = graphs_r_val[4] == 1\n",
    "\n",
    "total_entries = mask1.sum() + mask2.sum() + mask3.sum() + mask4.sum() + mask5.sum() + mask21.sum() + mask22.sum() + mask23.sum() + mask24.sum() + mask25.sum()\n",
    "print(total_entries / 2)\n",
    "\n",
    "# create degree matrix for each rating\n",
    "degree_matrices_r = [create_degree_matrix(graph) for graph in graphs_r_train]\n",
    "print(np.unique(degree_matrices_r[0].cpu().numpy()))\n",
    "degree_norms_r = [create_inverse_sqrt_degree_matrix(degree_matrix) for degree_matrix in degree_matrices_r]\n",
    "print(np.unique(degree_norms_r[0].cpu().numpy()))\n",
    "# for each degree matrix, replace inf with 0\n",
    "for i in range(5):\n",
    "    degree_norms_r[i][degree_norms_r[i] == float('inf')] = 0\n",
    "print(np.unique(degree_norms_r[0].cpu().numpy()))\n",
    "\n",
    "# check that degree matrices are symmetric\n",
    "print((degree_matrices_r[0] == degree_matrices_r[0].T).all())\n",
    "\n",
    "# check that degree_norms_r[0] is symmetric\n",
    "print((degree_norms_r[0] == degree_norms_r[0].T).all())\n",
    "\n",
    "# create normalized adjacency matrices\n",
    "norm_adj_r = [degree_norm @ graph @ degree_norm for degree_norm, graph in zip(degree_norms_r, graphs_r_train)]\n",
    "print(np.unique(norm_adj_r[0].cpu().numpy()))\n",
    "\n",
    "# send adj_matrices_r to device\n",
    "norm_adj_r = [adj_matrix.to(DEVICE) for adj_matrix in norm_adj_r]\n",
    "\n",
    "# Nice, everything works"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BaseLightGCN(nn.Module):\n",
    "    def __init__(self, norm_adj_r, act_fn, embedding_dim, n_layers, init_emb_std, dropout_rate):\n",
    "        super(BaseLightGCN, self).__init__()\n",
    "\n",
    "        self.norm_adj_r = norm_adj_r  # bipartite graphs (one for each rating r)\n",
    "        self.K = embedding_dim\n",
    "        self.L = n_layers \n",
    "        self.act_fn = act_fn\n",
    "\n",
    "        # Initialize embeddings\n",
    "        self.E_u = nn.Embedding(num_embeddings=N_u, embedding_dim=self.K)\n",
    "        self.E_v = nn.Embedding(num_embeddings=N_v, embedding_dim=self.K)\n",
    "        nn.init.normal_(self.E_u.weight, std=init_emb_std)\n",
    "        nn.init.normal_(self.E_v.weight, std=init_emb_std)\n",
    "\n",
    "        # Projection to output space after message passing, aggregation, and selection\n",
    "        self.mlp = self.create_mlp(dropout_rate)\n",
    "\n",
    "        # crate learnable parameter list of Q_r matrices of shape K x K\n",
    "        self.Q_r = nn.ParameterList([nn.Parameter(torch.randn(self.K, self.K)) for _ in range(5)])\n",
    "\n",
    "    def create_mlp(self, dropout_rate):\n",
    "        raise NotImplementedError(\"Derived classes must implement this method\")\n",
    "    \n",
    "    def message_pass_r(self, r) -> list[torch.Tensor]:\n",
    "        E_0 = torch.cat([self.E_u.weight, self.E_v.weight], dim=0)  # size (N_u + N_v) x K\n",
    "        E_layers = [E_0]\n",
    "        E_l = E_0\n",
    "\n",
    "        for l in range(self.L):\n",
    "            E_l = torch.mm(self.norm_adj_r[r], E_l)  # shape (N_u + N_v) x K\n",
    "            E_layers.append(E_l) \n",
    "\n",
    "        # print(\"E_layers[0].shape: \", E_layers[0].shape)\n",
    "        # print(\"len(E_layers): \", len(E_layers))\n",
    "        return E_layers\n",
    "    \n",
    "    def aggregate_message_passing_r(self, E_r: list) -> torch.Tensor:\n",
    "        E_agg_r = torch.stack(E_r, dim=0)  # shape (L + 1, N_u + N_v, K)\n",
    "        E_agg_r_mean = torch.mean(E_agg_r, dim=0)  # shape (N_u + N_v, K)\n",
    "        # print(\"E_agg_r.shape: \", E_agg_r.shape)\n",
    "        # print(\"E_agg_r_mean.shape: \", E_agg_r_mean.shape)\n",
    "        return E_agg_r_mean\n",
    "    \n",
    "    def aggregate(self, embs: list) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Aggregate the embeddings from the message passing layers.\n",
    "        \"\"\"\n",
    "        E_agg = torch.cat(embs, dim=1)  # shape (N_u + N_v, K * (L + 1))\n",
    "        # print(\"E_agg.shape: \", E_agg.shape)\n",
    "        return E_agg\n",
    "    \n",
    "    def select_embeddings(self, users, items, E_agg):\n",
    "        E_u, E_v = torch.split(E_agg, [N_u, N_v], dim=0)\n",
    "        # Select embeddings of users and items from the adjacency lists\n",
    "        E_u = E_u[users]\n",
    "        E_v = E_v[items]  # shape (N_train, K * (L + 1))\n",
    "        return E_u, E_v\n",
    "    \n",
    "    def forward(self, users, items):\n",
    "        E_r = [self.message_pass_r(r) for r in range(5)]\n",
    "        E_r_agg = [self.aggregate_message_passing_r(E) for E in E_r]\n",
    "        E_agg = self.aggregate(E_r_agg)\n",
    "        E_u_sel, E_v_sel = self.select_embeddings(users, items, E_agg)\n",
    "\n",
    "        # Project to output space\n",
    "        concat_users_items = torch.cat([E_u_sel, E_v_sel], dim=1)  # shape (N_train, (L + 1) * 5 * K * 2)\n",
    "        out = self.mlp(concat_users_items).squeeze()  \n",
    "        return out \n",
    "\n",
    "    def forward_1(self, users, items):\n",
    "        # TODO: use lightGCN message passing to aggregate information over hops \n",
    "        E_r = [self.message_pass_r(r) for r in range(5)]\n",
    "        # TODO: project down to K using MLP instead of aggregation\n",
    "        E_agg = torch.mean(torch.stack(E_r), dim=0)\n",
    "        E_u_sel, E_v_sel = self.select_embeddings(users, items, E_agg)\n",
    "\n",
    "        # Compute logits for each rating level\n",
    "        logits = []\n",
    "        for r in range(5):\n",
    "            logit = torch.einsum('ij,jk,ik->i', E_u_sel, self.Q_r[r], E_v_sel)\n",
    "            logits.append(logit)\n",
    "\n",
    "        logits = torch.stack(logits, dim=1)  # Shape: (batch_size, 5)\n",
    "\n",
    "        # Compute the softmax probabilities\n",
    "        softmax_probs = torch.softmax(logits, dim=1)  # Shape: (batch_size, 5)\n",
    "\n",
    "        # Compute the final rating prediction as the expected value of the softmax probabilities\n",
    "        ratings = torch.arange(1, 6).float().to(DEVICE)  # Shape: (5,)\n",
    "        preds = torch.sum(softmax_probs * ratings, dim=1)  # Shape: (batch_size,)\n",
    "        \n",
    "        # assert all preds are in [1, 5]\n",
    "        #assert (preds >= 1).all() and (preds <= 5).all()\n",
    "\n",
    "        return preds\n",
    "\n",
    "        \n",
    "\n",
    "    def get_ratings(self, users, items):\n",
    "        return self.forward(users, items)\n",
    "\n",
    "class LightGCN(BaseLightGCN):\n",
    "    def __init__(self, norm_adj_r, act_fn, embedding_dim, n_layers, init_emb_std, dropout_rate, projections):\n",
    "        self.projections = projections\n",
    "        super().__init__(norm_adj_r, act_fn, embedding_dim, n_layers, init_emb_std, dropout_rate)\n",
    "\n",
    "        # For reproducibility after training\n",
    "        # save_model_inputs(norm_adj_r, act_fn, embedding_dim, n_layers, init_emb_std, dropout_rate, projections)\n",
    "\n",
    "    def create_mlp(self, dropout_rate):\n",
    "        layers = []\n",
    "        input_dim = self.K * 5 * 2\n",
    "        for proj in self.projections:\n",
    "            output_dim = self.K * proj\n",
    "            layers.append(nn.Linear(input_dim, output_dim))\n",
    "            layers.append(self.act_fn)\n",
    "            layers.append(nn.Dropout(dropout_rate))\n",
    "            input_dim = output_dim\n",
    "        layers.append(nn.Linear(input_dim, 1))\n",
    "        return nn.Sequential(*layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from train import train_model\n",
    "from config import DEVICE\n",
    "from train import train_model\n",
    "from postprocess import report_training_results\n",
    "\n",
    "# Model and optimizer hyperparameters\n",
    "L=2\n",
    "K=28\n",
    "INIT_EMBS_STD=0.075\n",
    "LR=0.1\n",
    "WEIGHT_DECAY=0.00005\n",
    "DROPOUT=0.5\n",
    "PROJECTIONS = (5,)\n",
    "ACT_FN = nn.GELU()\n",
    "C=(5,)\n",
    "\n",
    "# Train loop hyperparameters\n",
    "EPOCHS=2000\n",
    "STOP_THRESHOLD=1e-06\n",
    "\n",
    "# to not change train loop (should actually separate concerns better and work with the reversing in the postprocessing)\n",
    "means = np.zeros(N_v)\n",
    "stds = np.ones(N_v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "K=28, L=2, C=(5,)\n",
      "Epoch 1 - Avg loss in last 1 epochs: - Train: 1.6453 - Val std: 44.5435 - Val orig: 44.5435\n",
      "Epoch 2 - Avg loss in last 1 epochs: - Train: 45.3476 - Val std: 4.2660 - Val orig: 4.2660\n",
      "Epoch 3 - Avg loss in last 1 epochs: - Train: 4.5000 - Val std: 5.8757 - Val orig: 5.8757\n",
      "Epoch 4 - Avg loss in last 1 epochs: - Train: 5.8993 - Val std: 5.1578 - Val orig: 5.1578\n",
      "Epoch 5 - Avg loss in last 1 epochs: - Train: 5.7780 - Val std: 90.2074 - Val orig: 90.2074\n",
      "Epoch 6 - Avg loss in last 1 epochs: - Train: 91.6336 - Val std: 9.7058 - Val orig: 9.7058\n",
      "Epoch 7 - Avg loss in last 1 epochs: - Train: 10.4967 - Val std: 11.5870 - Val orig: 11.5870\n",
      "Epoch 8 - Avg loss in last 1 epochs: - Train: 12.4538 - Val std: 6.1482 - Val orig: 6.1482\n",
      "Epoch 9 - Avg loss in last 1 epochs: - Train: 7.0680 - Val std: 1.6010 - Val orig: 1.6010\n",
      "Epoch 10 - Avg loss in last 1 epochs: - Train: 2.3869 - Val std: 7.2354 - Val orig: 7.2354\n",
      "Epoch 11 - Avg loss in last 1 epochs: - Train: 7.3218 - Val std: 4.4869 - Val orig: 4.4869\n",
      "Epoch 12 - Avg loss in last 1 epochs: - Train: 4.5160 - Val std: 2.6959 - Val orig: 2.6959\n",
      "Epoch 13 - Avg loss in last 1 epochs: - Train: 2.9588 - Val std: 2.3550 - Val orig: 2.3550\n",
      "Epoch 14 - Avg loss in last 1 epochs: - Train: 3.7992 - Val std: 1.9745 - Val orig: 1.9745\n",
      "Epoch 15 - Avg loss in last 1 epochs: - Train: 3.3834 - Val std: 1.4427 - Val orig: 1.4427\n",
      "Epoch 16 - Avg loss in last 1 epochs: - Train: 2.1041 - Val std: 2.7679 - Val orig: 2.7679\n",
      "Epoch 17 - Avg loss in last 1 epochs: - Train: 2.8373 - Val std: 3.3441 - Val orig: 3.3441\n",
      "Epoch 18 - Avg loss in last 1 epochs: - Train: 3.3547 - Val std: 3.2537 - Val orig: 3.2537\n",
      "Epoch 19 - Avg loss in last 1 epochs: - Train: 3.2715 - Val std: 2.7179 - Val orig: 2.7179\n",
      "Epoch 20 - Avg loss in last 1 epochs: - Train: 2.8007 - Val std: 1.7336 - Val orig: 1.7336\n",
      "Epoch 21 - Avg loss in last 1 epochs: - Train: 2.1883 - Val std: 1.1827 - Val orig: 1.1827\n",
      "Epoch 22 - Avg loss in last 1 epochs: - Train: 2.5116 - Val std: 1.2318 - Val orig: 1.2318\n",
      "Epoch 23 - Avg loss in last 1 epochs: - Train: 2.6395 - Val std: 1.5865 - Val orig: 1.5865\n",
      "Epoch 24 - Avg loss in last 1 epochs: - Train: 2.3178 - Val std: 2.4346 - Val orig: 2.4346\n",
      "Epoch 25 - Avg loss in last 1 epochs: - Train: 2.7394 - Val std: 1.8313 - Val orig: 1.8313\n",
      "Epoch 26 - Avg loss in last 1 epochs: - Train: 2.4197 - Val std: 1.3089 - Val orig: 1.3089\n",
      "Epoch 27 - Avg loss in last 1 epochs: - Train: 2.6440 - Val std: 1.4181 - Val orig: 1.4181\n",
      "Epoch 28 - Avg loss in last 1 epochs: - Train: 2.3810 - Val std: 1.3177 - Val orig: 1.3177\n",
      "Epoch 29 - Avg loss in last 1 epochs: - Train: 1.7576 - Val std: 1.5141 - Val orig: 1.5141\n",
      "Epoch 30 - Avg loss in last 1 epochs: - Train: 1.8486 - Val std: 1.3665 - Val orig: 1.3665\n",
      "Epoch 31 - Avg loss in last 1 epochs: - Train: 1.8693 - Val std: 1.6843 - Val orig: 1.6843\n",
      "Epoch 32 - Avg loss in last 1 epochs: - Train: 2.2515 - Val std: 1.5657 - Val orig: 1.5657\n",
      "Epoch 33 - Avg loss in last 1 epochs: - Train: 2.3133 - Val std: 1.3872 - Val orig: 1.3872\n",
      "Epoch 34 - Avg loss in last 1 epochs: - Train: 2.3458 - Val std: 1.4186 - Val orig: 1.4186\n",
      "Epoch 35 - Avg loss in last 1 epochs: - Train: 2.3273 - Val std: 1.3021 - Val orig: 1.3021\n",
      "Epoch 36 - Avg loss in last 1 epochs: - Train: 2.0342 - Val std: 1.3451 - Val orig: 1.3451\n",
      "Epoch 37 - Avg loss in last 1 epochs: - Train: 1.8680 - Val std: 1.2601 - Val orig: 1.2601\n",
      "Epoch 38 - Avg loss in last 1 epochs: - Train: 1.6614 - Val std: 1.1842 - Val orig: 1.1842\n",
      "Epoch 39 - Avg loss in last 1 epochs: - Train: 1.4878 - Val std: 1.2388 - Val orig: 1.2388\n",
      "Epoch 40 - Avg loss in last 1 epochs: - Train: 1.4540 - Val std: 1.3124 - Val orig: 1.3124\n",
      "Epoch 41 - Avg loss in last 1 epochs: - Train: 1.4759 - Val std: 1.3125 - Val orig: 1.3125\n",
      "Epoch 42 - Avg loss in last 1 epochs: - Train: 1.4636 - Val std: 1.2357 - Val orig: 1.2357\n",
      "Epoch 43 - Avg loss in last 1 epochs: - Train: 1.4073 - Val std: 1.1430 - Val orig: 1.1430\n",
      "Epoch 44 - Avg loss in last 1 epochs: - Train: 1.3576 - Val std: 1.1084 - Val orig: 1.1084\n",
      "Epoch 45 - Avg loss in last 1 epochs: - Train: 1.3651 - Val std: 1.1164 - Val orig: 1.1164\n",
      "Epoch 46 - Avg loss in last 1 epochs: - Train: 1.3920 - Val std: 1.1064 - Val orig: 1.1064\n",
      "Epoch 47 - Avg loss in last 1 epochs: - Train: 1.3721 - Val std: 1.1007 - Val orig: 1.1007\n",
      "Epoch 48 - Avg loss in last 1 epochs: - Train: 1.3348 - Val std: 1.1359 - Val orig: 1.1359\n",
      "Epoch 49 - Avg loss in last 1 epochs: - Train: 1.3359 - Val std: 1.1668 - Val orig: 1.1668\n",
      "Epoch 50 - Avg loss in last 1 epochs: - Train: 1.3514 - Val std: 1.1483 - Val orig: 1.1483\n",
      "Epoch 51 - Avg loss in last 1 epochs: - Train: 1.3406 - Val std: 1.1047 - Val orig: 1.1047\n",
      "Epoch 52 - Avg loss in last 1 epochs: - Train: 1.3195 - Val std: 1.0764 - Val orig: 1.0764\n",
      "Epoch 53 - Avg loss in last 1 epochs: - Train: 1.3140 - Val std: 1.0727 - Val orig: 1.0727\n",
      "Epoch 54 - Avg loss in last 1 epochs: - Train: 1.3257 - Val std: 1.0713 - Val orig: 1.0713\n",
      "Epoch 55 - Avg loss in last 1 epochs: - Train: 1.3261 - Val std: 1.0633 - Val orig: 1.0633\n",
      "Epoch 56 - Avg loss in last 1 epochs: - Train: 1.3101 - Val std: 1.0660 - Val orig: 1.0660\n",
      "Epoch 57 - Avg loss in last 1 epochs: - Train: 1.2961 - Val std: 1.0859 - Val orig: 1.0859\n",
      "Epoch 58 - Avg loss in last 1 epochs: - Train: 1.2953 - Val std: 1.1025 - Val orig: 1.1025\n",
      "Epoch 59 - Avg loss in last 1 epochs: - Train: 1.2999 - Val std: 1.0964 - Val orig: 1.0964\n",
      "Epoch 60 - Avg loss in last 1 epochs: - Train: 1.2945 - Val std: 1.0719 - Val orig: 1.0719\n",
      "Epoch 61 - Avg loss in last 1 epochs: - Train: 1.2811 - Val std: 1.0499 - Val orig: 1.0499\n",
      "Epoch 62 - Avg loss in last 1 epochs: - Train: 1.2746 - Val std: 1.0418 - Val orig: 1.0418\n",
      "Epoch 63 - Avg loss in last 1 epochs: - Train: 1.2775 - Val std: 1.0396 - Val orig: 1.0396\n",
      "Epoch 64 - Avg loss in last 1 epochs: - Train: 1.2774 - Val std: 1.0378 - Val orig: 1.0378\n",
      "Epoch 65 - Avg loss in last 1 epochs: - Train: 1.2682 - Val std: 1.0431 - Val orig: 1.0431\n",
      "Epoch 66 - Avg loss in last 1 epochs: - Train: 1.2595 - Val std: 1.0566 - Val orig: 1.0566\n",
      "Epoch 67 - Avg loss in last 1 epochs: - Train: 1.2588 - Val std: 1.0652 - Val orig: 1.0652\n",
      "Epoch 68 - Avg loss in last 1 epochs: - Train: 1.2599 - Val std: 1.0587 - Val orig: 1.0587\n",
      "Epoch 69 - Avg loss in last 1 epochs: - Train: 1.2551 - Val std: 1.0433 - Val orig: 1.0433\n",
      "Epoch 70 - Avg loss in last 1 epochs: - Train: 1.2490 - Val std: 1.0319 - Val orig: 1.0319\n",
      "Epoch 71 - Avg loss in last 1 epochs: - Train: 1.2470 - Val std: 1.0277 - Val orig: 1.0277\n",
      "Epoch 72 - Avg loss in last 1 epochs: - Train: 1.2490 - Val std: 1.0269 - Val orig: 1.0269\n",
      "Epoch 73 - Avg loss in last 1 epochs: - Train: 1.2442 - Val std: 1.0299 - Val orig: 1.0299\n",
      "Epoch 74 - Avg loss in last 1 epochs: - Train: 1.2395 - Val std: 1.0380 - Val orig: 1.0380\n",
      "Epoch 75 - Avg loss in last 1 epochs: - Train: 1.2386 - Val std: 1.0452 - Val orig: 1.0452\n",
      "Epoch 76 - Avg loss in last 1 epochs: - Train: 1.2385 - Val std: 1.0436 - Val orig: 1.0436\n",
      "Epoch 77 - Avg loss in last 1 epochs: - Train: 1.2348 - Val std: 1.0346 - Val orig: 1.0346\n",
      "Epoch 78 - Avg loss in last 1 epochs: - Train: 1.2311 - Val std: 1.0257 - Val orig: 1.0257\n",
      "Epoch 79 - Avg loss in last 1 epochs: - Train: 1.2314 - Val std: 1.0213 - Val orig: 1.0213\n"
     ]
    }
   ],
   "source": [
    "model = LightGCN(norm_adj_r, ACT_FN, K, L, INIT_EMBS_STD, DROPOUT, C).to(DEVICE)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=LR, weight_decay=WEIGHT_DECAY)\n",
    "loss_fn = nn.MSELoss()\n",
    "print(f\"K={K}, L={L}, C={C}\")\n",
    "train_rmse, val_rmse_std, val_rmse_orig = train_model(model, optimizer, loss_fn, train_users, train_items, train_ratings, val_users, val_items, val_ratings, val_ratings, means, stds, EPOCHS, STOP_THRESHOLD, False, verbosity=1)\n",
    "report_training_results(train_rmse, val_rmse_std, val_rmse_orig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
