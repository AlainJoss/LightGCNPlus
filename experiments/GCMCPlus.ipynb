{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LightGCNPlus\n",
    "\n",
    "### TODO: \n",
    "- try to learn the weighting of the different layers in the message passing mechanism.\n",
    "\n",
    "### Scope\n",
    "In this notebook we train different variants of our LightGCNPlus model on different hyperparameters. We then re-train the best performing model and evaluate it on Kaggle's public test set.\n",
    "\n",
    "### About the model\n",
    "LightGCNPlus is a graph-based collaborative filtering model that extends the [original LightGCN model](https://arxiv.org/pdf/2002.02126.pdf). The original LightGCN is only able to rank items. We thus refine the model to be able to predict ratings. Our LightGCNPlus model is composed of the following components:\n",
    "- message passing: $e_u^{(l)} = \\sum_{v \\in N(u)} \\frac{sr_{u,v}}{\\sqrt{|N(u)||N(v)|}} e_v^{(l-1)}$, where $sr_{u,v}$ is the standardized rating of user $u$ for item $v$.\n",
    "- aggregation mechanism: $h_u = \\text{concat}(e_u^{(0)}, e_u^{(1)}, ..., e_u^{(L)})$, where $L$ is the number of layers in the message passing mechanism.\n",
    "- output layer: $\\hat{R}_{(i,j)} = \\text{MLP}(\\text{concat}(h_i, h_j))$, where $\\text{MLP}$ is a multi-layer perceptron that projects the embedding couples of observed ratings to the output space.\n",
    "\n",
    "How our model differs from the original GCMC model:\n",
    "- In the message passing layer we use the layered message passing (multi-hop) mechanism of LightGCN instead of just a one-hop pass.\n",
    "- To project to the output space we use a multi-layer perceptron instead of the softmax approach to compute probabilities over ratings.\n",
    "\n",
    "### Hyperparameter tuning\n",
    "The hyperparameters comprise both choices about the models' architecture and the training procedure.\n",
    "We perform a grid search over the following hyperparameters:\n",
    "...\n",
    "\n",
    "Other hyperparameters such as the learning rate, the batch size, the number of epochs, the optimizer, etc. have been selected based on the results on previous experiments. Thus, to reduce the computational cost of the grid search, the following hyperparameters are kept constant:\n",
    "- LR: learning rate.\n",
    "- INIT_EMBS_STD: standard deviation of the normal distribution used to initialize the embeddings.\n",
    "- WEIGHT_DECAY: weight decay coefficient for the Adam optimizer.\n",
    "- DROPOUT: dropout rate used in the MLP's hidden layers.\n",
    "- ACT_FN: activation function used in the MLP's hidden layers.\n",
    "\n",
    "We also define hyperparameters for the training procedure:\n",
    "- EPOCHS: number of backpropagation steps.\n",
    "- STOP_THRESHOLD: minimum improvement in the validation loss to continue training.\n",
    "\n",
    "### Training\n",
    "- loss functions:\n",
    "    - train: $\\text{MSE} = \\frac{1}{|\\Omega|} \\sum_{(i,j) \\in \\Omega} (M_{(i,j)} - \\hat{M}_{(i,j)})^2$\n",
    "    - eval: $\\text{RMSE} = \\sqrt{\\text{MSE}}$\n",
    "- optimizer: Adam\n",
    "- Batching: no mini-batching of the training data, doesn't improve performance and slows convergence down. \n",
    "- Regularization: dropout and L2 regularization (weight decay in the Adam optimizer)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Graph Convolutional Matrix Completion Plus (GCMCPlus)\n",
    "\n",
    "### Scope\n",
    "In this notebook we train different variants of our GCMCPlus model on different hyperparameters. We then re-train the best performing model and evaluate it on Kaggle's public test set.\n",
    "\n",
    "### About the model\n",
    "GCMCPlus is a graph-based collaborative filtering model that extends the [original GCMC model](https://arxiv.org/abs/1706.02263). We refine the model by taking ideas from the [neural collaborative filtering paper](https://arxiv.org/pdf/1708.05031.pdf) and the [LightGCN model](https://arxiv.org/pdf/2002.02126.pdf). Our GCMCPlus model is composed of the following components:\n",
    "- preprocessing: for each rating level $r$, create $\\tilde{A}_r = D_r^{-\\frac{1}{2}} A_r D_r^{-\\frac{1}{2}}$, where $A_r$ is the bipartite graph of the interactions of rating level $r$, and $D_r$ is the diagonal degree matrix of $A_r$.\n",
    "- message passing: $e_{u_{r}}^{(l)} = \\tilde{A}_{(u,v)_{r}} e_{v_{r}}^{(l-1)}$, where $e_{u_{r}}^{(0)}$ is the learnable embedding of user $u$ for rating level $r$.\n",
    "- aggregation mechanism of layered message passing at rating level $r$: $h_{u_{r}} = \\frac{1}{L} \\sum_{l=1}^{L} e_{u_{r}}^{(l)}$, where $L$ is the number of layers in the message passing mechanism.\n",
    "- aggregation mechanism of different rating levels: $h_u = \\text{concat}(h_{u_1}, h_{u_2}, ..., h_{u_{|\\mathcal{R}|}})$, where $|\\mathcal{R}|$ is the number of rating levels.\n",
    "- output layer: $\\hat{M}_{(i,j)} = \\text{MLP}(\\text{concat}(h_i, h_j))$, where $\\text{MLP}$ is a multi-layer perceptron that projects the embedding couples of observed ratings to the output space.\n",
    "\n",
    "#### Architecture\n",
    "1. Encoder:\n",
    "    - create 5 bipartite graphs (one for each rating level)\n",
    "    - for each node, message pass the embeddings of its neighbors\n",
    "    - concatenate the embeddings of the different rating levels\n",
    "    - pass the concatenated embeddings through a fully connected layer (decide whether to use act-MLP or just MLP)\n",
    "2. Decoder:\n",
    "    - compute probability of each rating level\n",
    "    - compute final rating as expectation of the rating levels\n",
    "\n",
    "#### Possible loss functions\n",
    "- cross entropy: $-\\sum_{(i,j) \\in \\Omega} \\sum_{r \\in \\mathcal{R}} \\text{I}\\{M_{(i,j)} == r\\} \\log P(M_{(i,j)} == r)$\n",
    "- RMSE: $\\sqrt{\\frac{1}{|\\Omega|} \\sum_{(i,j) \\in \\Omega} (M_{(i,j)} - \\mathbb{E}[M_{(i,j)}])^2}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch\n",
    "from torch import nn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load and Preprocess Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1176952\n",
      "1176952\n",
      "tensor(1176952., device='mps:0')\n",
      "[  0   1   2   3   4   5   6   7   8   9  10  11  12  13  14  15  16  17\n",
      "  18  19  20  21  22  23  24  25  26  27  28  29  30  31  32  33  34  35\n",
      "  36  37  38  39  40  41  42  43  44  45  46  47  48  49  50  51  52  53\n",
      "  54  55  56  57  58  59  60  61  62  63  64  65  66  67  68  69  70  71\n",
      "  72  73  74  75  76  77  78  79  80  81  82  83  84  85  86  87  88  89\n",
      "  90  91  92  93  94  95  97  98  99 100 101 102 103 104 106 107 108 109\n",
      " 110 112 113 116 117 120 123 124 125 126 127 128 131 132 133 134 136 138\n",
      " 142 144 146 149 155 166 167 168 173 174 184 188 191 197 198 209 217 246\n",
      " 253 268 289 292 311 318 356]\n",
      "[0.         0.0529999  0.05607722 0.0567048  0.05852057 0.05882353\n",
      " 0.06108472 0.06286946 0.06375767 0.06788442 0.06917145 0.07106691\n",
      " 0.07124705 0.07235746 0.0729325  0.07372098 0.07580981 0.07602859\n",
      " 0.07715168 0.07738233 0.07761505 0.08032193 0.08192319 0.08276059\n",
      " 0.08333334 0.08391814 0.08512565 0.0857493  0.08638684 0.086711\n",
      " 0.08703883 0.0873704  0.08838835 0.08873565 0.08908708 0.08944272\n",
      " 0.08980265 0.09016696 0.09128709 0.09245003 0.09284767 0.09407209\n",
      " 0.09449112 0.09534626 0.09578263 0.09622505 0.09667365 0.09712858\n",
      " 0.09805807 0.09853293 0.09901476 0.09950373 0.1        0.10050379\n",
      " 0.10101525 0.10153461 0.10259783 0.10314212 0.10369517 0.1042572\n",
      " 0.10482848 0.10540926 0.10599979 0.10660036 0.10721125 0.10783277\n",
      " 0.10846523 0.10910894 0.10976426 0.11043152 0.11111111 0.1118034\n",
      " 0.1125088  0.11322771 0.11396058 0.11470786 0.11547004 0.11624764\n",
      " 0.11704115 0.11785114 0.11867817 0.11952286 0.12038586 0.12126782\n",
      " 0.12216945 0.12309149 0.12403473 0.125      0.12598816 0.12700012\n",
      " 0.12803687 0.12909944 0.13018891 0.13130642 0.13245323 0.13363062\n",
      " 0.13483998 0.13608277 0.13736056 0.13867505 0.14002801 0.14142136\n",
      " 0.14285715 0.14433756 0.145865   0.14744195 0.1490712  0.15075567\n",
      " 0.15249857 0.15430336 0.15617377 0.15811388 0.16012816 0.16222142\n",
      " 0.16439898 0.16666667 0.16903085 0.1714986  0.17407766 0.17677669\n",
      " 0.1796053  0.18257418 0.18569534 0.18898225 0.19245009 0.19611613\n",
      " 0.2        0.20412414 0.2085144  0.21320072 0.21821788 0.2236068\n",
      " 0.22941573 0.23570228 0.24253564 0.25       0.2581989  0.26726124\n",
      " 0.2773501  0.28867513 0.30151135 0.31622776 0.33333334 0.35355338\n",
      " 0.3779645  0.40824828 0.4472136  0.5        0.57735026 0.70710677\n",
      " 1.                inf]\n",
      "[0.         0.0529999  0.05607722 0.0567048  0.05852057 0.05882353\n",
      " 0.06108472 0.06286946 0.06375767 0.06788442 0.06917145 0.07106691\n",
      " 0.07124705 0.07235746 0.0729325  0.07372098 0.07580981 0.07602859\n",
      " 0.07715168 0.07738233 0.07761505 0.08032193 0.08192319 0.08276059\n",
      " 0.08333334 0.08391814 0.08512565 0.0857493  0.08638684 0.086711\n",
      " 0.08703883 0.0873704  0.08838835 0.08873565 0.08908708 0.08944272\n",
      " 0.08980265 0.09016696 0.09128709 0.09245003 0.09284767 0.09407209\n",
      " 0.09449112 0.09534626 0.09578263 0.09622505 0.09667365 0.09712858\n",
      " 0.09805807 0.09853293 0.09901476 0.09950373 0.1        0.10050379\n",
      " 0.10101525 0.10153461 0.10259783 0.10314212 0.10369517 0.1042572\n",
      " 0.10482848 0.10540926 0.10599979 0.10660036 0.10721125 0.10783277\n",
      " 0.10846523 0.10910894 0.10976426 0.11043152 0.11111111 0.1118034\n",
      " 0.1125088  0.11322771 0.11396058 0.11470786 0.11547004 0.11624764\n",
      " 0.11704115 0.11785114 0.11867817 0.11952286 0.12038586 0.12126782\n",
      " 0.12216945 0.12309149 0.12403473 0.125      0.12598816 0.12700012\n",
      " 0.12803687 0.12909944 0.13018891 0.13130642 0.13245323 0.13363062\n",
      " 0.13483998 0.13608277 0.13736056 0.13867505 0.14002801 0.14142136\n",
      " 0.14285715 0.14433756 0.145865   0.14744195 0.1490712  0.15075567\n",
      " 0.15249857 0.15430336 0.15617377 0.15811388 0.16012816 0.16222142\n",
      " 0.16439898 0.16666667 0.16903085 0.1714986  0.17407766 0.17677669\n",
      " 0.1796053  0.18257418 0.18569534 0.18898225 0.19245009 0.19611613\n",
      " 0.2        0.20412414 0.2085144  0.21320072 0.21821788 0.2236068\n",
      " 0.22941573 0.23570228 0.24253564 0.25       0.2581989  0.26726124\n",
      " 0.2773501  0.28867513 0.30151135 0.31622776 0.33333334 0.35355338\n",
      " 0.3779645  0.40824828 0.4472136  0.5        0.57735026 0.70710677\n",
      " 1.        ]\n",
      "tensor(True, device='mps:0')\n",
      "tensor(True, device='mps:0')\n",
      "[0.         0.00434192 0.00515047 ... 0.49999997 0.5        0.57735026]\n"
     ]
    }
   ],
   "source": [
    "from config import N_u, N_v, VAL_SIZE, DEVICE\n",
    "from load import load_train_data\n",
    "from preprocess import extract_users_items_ratings, create_bipartite_graph, create_degree_matrix, create_inverse_sqrt_degree_matrix\n",
    "\n",
    "# Load data\n",
    "train_df = load_train_data()\n",
    "\n",
    "# Extract adjacency lists: observed values edge index (src, tgt) and ratings (values)\n",
    "all_users, all_items, all_ratings = extract_users_items_ratings(train_df)\n",
    "\n",
    "# Create rating matrix from the triplets\n",
    "all_ratings_matrix = np.zeros((N_u, N_v))\n",
    "all_ratings_matrix[all_users, all_items] = all_ratings\n",
    "\n",
    "# Split the data into trai and val sets\n",
    "train_users, val_users, train_items, val_items, train_ratings, val_ratings = \\\n",
    "    train_test_split(all_users, all_items, all_ratings, test_size=VAL_SIZE)\n",
    "\n",
    "# convert lists to torch tensors\n",
    "train_users = torch.tensor(train_users, dtype=torch.long).to(DEVICE)\n",
    "val_users = torch.tensor(val_users, dtype=torch.long).to(DEVICE)\n",
    "train_items = torch.tensor(train_items, dtype=torch.long).to(DEVICE)\n",
    "val_items = torch.tensor(val_items, dtype=torch.long).to(DEVICE)\n",
    "train_ratings = torch.tensor(train_ratings, dtype=torch.float).to(DEVICE)\n",
    "val_ratings = torch.tensor(val_ratings, dtype=torch.float).to(DEVICE)\n",
    "\n",
    "print(len(train_users) + len(val_users))\n",
    "\n",
    "# Create adjacency lists for rating r \n",
    "u_v_r_train = []\n",
    "u_v_r_val = []\n",
    "total = 0\n",
    "for r in range(1, 6):\n",
    "    # assign value of 1 to each triplet with rating r instead of rating r\n",
    "    u_v_r_train.append((train_users[train_ratings == r], train_items[train_ratings == r], torch.ones_like(train_ratings[train_ratings == r])))\n",
    "    u_v_r_val.append((val_users[val_ratings == r], val_items[val_ratings == r], torch.ones_like(val_ratings[val_ratings == r])))\n",
    "    total += len(train_users[train_ratings == r]) + len(val_users[val_ratings == r])\n",
    "print(total)\n",
    "\n",
    "# Create bipartite graphs\n",
    "graphs_r_train = [create_bipartite_graph(u, v, r) for u, v, r in u_v_r_train]\n",
    "graphs_r_val = [create_bipartite_graph(u, v, r) for u, v, r in u_v_r_val]\n",
    "\n",
    "# check that graphs_r_train[0] doesn't contain only zero entries\n",
    "mask1 = graphs_r_train[0] == 1\n",
    "mask2 = graphs_r_train[1] == 1\n",
    "mask3 = graphs_r_train[2] == 1\n",
    "mask4 = graphs_r_train[3] == 1\n",
    "mask5 = graphs_r_train[4] == 1\n",
    "\n",
    "mask21 = graphs_r_val[0] == 1\n",
    "mask22 = graphs_r_val[1] == 1\n",
    "mask23 = graphs_r_val[2] == 1\n",
    "mask24 = graphs_r_val[3] == 1\n",
    "mask25 = graphs_r_val[4] == 1\n",
    "\n",
    "total_entries = mask1.sum() + mask2.sum() + mask3.sum() + mask4.sum() + mask5.sum() + mask21.sum() + mask22.sum() + mask23.sum() + mask24.sum() + mask25.sum()\n",
    "print(total_entries / 2)\n",
    "\n",
    "# create degree matrix for each rating\n",
    "degree_matrices_r = [create_degree_matrix(graph) for graph in graphs_r_train]\n",
    "print(np.unique(degree_matrices_r[0].cpu().numpy()))\n",
    "degree_norms_r = [create_inverse_sqrt_degree_matrix(degree_matrix) for degree_matrix in degree_matrices_r]\n",
    "print(np.unique(degree_norms_r[0].cpu().numpy()))\n",
    "# for each degree matrix, replace inf with 0\n",
    "for i in range(5):\n",
    "    degree_norms_r[i][degree_norms_r[i] == float('inf')] = 0\n",
    "print(np.unique(degree_norms_r[0].cpu().numpy()))\n",
    "\n",
    "# check that degree matrices are symmetric\n",
    "print((degree_matrices_r[0] == degree_matrices_r[0].T).all())\n",
    "\n",
    "# check that degree_norms_r[0] is symmetric\n",
    "print((degree_norms_r[0] == degree_norms_r[0].T).all())\n",
    "\n",
    "# create normalized adjacency matrices\n",
    "norm_adj_r = [degree_norm @ graph @ degree_norm for degree_norm, graph in zip(degree_norms_r, graphs_r_train)]\n",
    "print(np.unique(norm_adj_r[0].cpu().numpy()))\n",
    "\n",
    "# send adj_matrices_r to device\n",
    "norm_adj_r = [adj_matrix.to(DEVICE) for adj_matrix in norm_adj_r]\n",
    "\n",
    "# Nice, everything works"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BaseLightGCN(nn.Module):\n",
    "    def __init__(self, norm_adj_r, act_fn, embedding_dim, n_layers, init_emb_std, dropout_rate):\n",
    "        super(BaseLightGCN, self).__init__()\n",
    "\n",
    "        self.norm_adj_r = norm_adj_r  # bipartite graphs (one for each rating r)\n",
    "        self.K = embedding_dim\n",
    "        self.L = n_layers \n",
    "        self.act_fn = act_fn\n",
    "\n",
    "        # Initialize embeddings\n",
    "        self.E_u = nn.Embedding(num_embeddings=N_u, embedding_dim=self.K)\n",
    "        self.E_v = nn.Embedding(num_embeddings=N_v, embedding_dim=self.K)\n",
    "        nn.init.normal_(self.E_u.weight, std=init_emb_std)\n",
    "        nn.init.normal_(self.E_v.weight, std=init_emb_std)\n",
    "\n",
    "        # Projection to output space after message passing, aggregation, and selection\n",
    "        self.mlp = self.create_mlp(dropout_rate)\n",
    "\n",
    "        # crate learnable parameter list of Q_r matrices of shape K x K\n",
    "        self.Q_r = nn.ParameterList([nn.Parameter(torch.randn(self.K, self.K)) for _ in range(5)])\n",
    "\n",
    "    def create_mlp(self, dropout_rate):\n",
    "        raise NotImplementedError(\"Derived classes must implement this method\")\n",
    "    \n",
    "    def message_pass_r(self, r) -> list[torch.Tensor]:\n",
    "        E_0 = torch.cat([self.E_u.weight, self.E_v.weight], dim=0)  # size (N_u + N_v) x K\n",
    "        E_layers = [E_0]\n",
    "        E_l = E_0\n",
    "\n",
    "        for l in range(self.L):\n",
    "            E_l = torch.mm(self.norm_adj_r[r], E_l)  # shape (N_u + N_v) x K\n",
    "            E_layers.append(E_l) \n",
    "\n",
    "        # print(\"E_layers[0].shape: \", E_layers[0].shape)\n",
    "        # print(\"len(E_layers): \", len(E_layers))\n",
    "        return E_layers\n",
    "    \n",
    "    def aggregate_message_passing_r(self, E_r: list) -> torch.Tensor:\n",
    "        E_agg_r = torch.stack(E_r, dim=0)  # shape (L + 1, N_u + N_v, K)\n",
    "        E_agg_r_mean = torch.mean(E_agg_r, dim=0)  # shape (N_u + N_v, K)\n",
    "        # print(\"E_agg_r.shape: \", E_agg_r.shape)\n",
    "        # print(\"E_agg_r_mean.shape: \", E_agg_r_mean.shape)\n",
    "        return E_agg_r_mean\n",
    "    \n",
    "    def aggregate(self, embs: list) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Aggregate the embeddings from the message passing layers.\n",
    "        \"\"\"\n",
    "        E_agg = torch.cat(embs, dim=1)  # shape (N_u + N_v, K * (L + 1))\n",
    "        # print(\"E_agg.shape: \", E_agg.shape)\n",
    "        return E_agg\n",
    "    \n",
    "    def select_embeddings(self, users, items, E_agg):\n",
    "        E_u, E_v = torch.split(E_agg, [N_u, N_v], dim=0)\n",
    "        # Select embeddings of users and items from the adjacency lists\n",
    "        E_u = E_u[users]\n",
    "        E_v = E_v[items]  # shape (N_train, K * (L + 1))\n",
    "        return E_u, E_v\n",
    "    \n",
    "    def forward(self, users, items):\n",
    "        E_r = [self.message_pass_r(r) for r in range(5)]\n",
    "        E_r_agg = [self.aggregate_message_passing_r(E) for E in E_r]\n",
    "        E_agg = self.aggregate(E_r_agg)\n",
    "        E_u_sel, E_v_sel = self.select_embeddings(users, items, E_agg)\n",
    "\n",
    "        # Project to output space\n",
    "        concat_users_items = torch.cat([E_u_sel, E_v_sel], dim=1)  # shape (N_train, (L + 1) * 5 * K * 2)\n",
    "        out = self.mlp(concat_users_items).squeeze()  \n",
    "        return out \n",
    "\n",
    "    def forward_1(self, users, items):\n",
    "        # TODO: use lightGCN message passing to aggregate information over hops \n",
    "        E_r = [self.message_pass_r(r) for r in range(5)]\n",
    "        # TODO: project down to K using MLP instead of aggregation\n",
    "        E_agg = torch.mean(torch.stack(E_r), dim=0)\n",
    "        E_u_sel, E_v_sel = self.select_embeddings(users, items, E_agg)\n",
    "\n",
    "        # Compute logits for each rating level\n",
    "        logits = []\n",
    "        for r in range(5):\n",
    "            logit = torch.einsum('ij,jk,ik->i', E_u_sel, self.Q_r[r], E_v_sel)\n",
    "            logits.append(logit)\n",
    "\n",
    "        logits = torch.stack(logits, dim=1)  # Shape: (batch_size, 5)\n",
    "\n",
    "        # Compute the softmax probabilities\n",
    "        softmax_probs = torch.softmax(logits, dim=1)  # Shape: (batch_size, 5)\n",
    "\n",
    "        # Compute the final rating prediction as the expected value of the softmax probabilities\n",
    "        ratings = torch.arange(1, 6).float().to(DEVICE)  # Shape: (5,)\n",
    "        preds = torch.sum(softmax_probs * ratings, dim=1)  # Shape: (batch_size,)\n",
    "        \n",
    "        # assert all preds are in [1, 5]\n",
    "        #assert (preds >= 1).all() and (preds <= 5).all()\n",
    "\n",
    "        return preds\n",
    "\n",
    "        \n",
    "\n",
    "    def get_ratings(self, users, items):\n",
    "        return self.forward(users, items)\n",
    "\n",
    "class LightGCN(BaseLightGCN):\n",
    "    def __init__(self, norm_adj_r, act_fn, embedding_dim, n_layers, init_emb_std, dropout_rate, projections):\n",
    "        self.projections = projections\n",
    "        super().__init__(norm_adj_r, act_fn, embedding_dim, n_layers, init_emb_std, dropout_rate)\n",
    "\n",
    "        # For reproducibility after training\n",
    "        # save_model_inputs(norm_adj_r, act_fn, embedding_dim, n_layers, init_emb_std, dropout_rate, projections)\n",
    "\n",
    "    def create_mlp(self, dropout_rate):\n",
    "        layers = []\n",
    "        input_dim = self.K * 5 * 2\n",
    "        for proj in self.projections:\n",
    "            output_dim = self.K * proj\n",
    "            layers.append(nn.Linear(input_dim, output_dim))\n",
    "            layers.append(self.act_fn)\n",
    "            layers.append(nn.Dropout(dropout_rate))\n",
    "            input_dim = output_dim\n",
    "        layers.append(nn.Linear(input_dim, 1))\n",
    "        return nn.Sequential(*layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from train import train_model\n",
    "from config import DEVICE\n",
    "from train import train_model\n",
    "from postprocess import report_training_results\n",
    "\n",
    "# Model and optimizer hyperparameters\n",
    "L=3\n",
    "K=28\n",
    "INIT_EMBS_STD=0.075\n",
    "LR=0.1\n",
    "WEIGHT_DECAY=0.00005\n",
    "DROPOUT=0.5\n",
    "PROJECTIONS = (5,)\n",
    "ACT_FN = nn.GELU()\n",
    "C=(5,)\n",
    "\n",
    "# Train loop hyperparameters\n",
    "EPOCHS=2000\n",
    "STOP_THRESHOLD=1e-06\n",
    "\n",
    "# to not change train loop (should actually separate concerns better and work with the reversing in the postprocessing)\n",
    "means = np.zeros(N_v)\n",
    "stds = np.ones(N_v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "K=28, L=3, C=(5,)\n",
      "Epoch 1 - Avg loss in last 1 epochs: - Train: 5.9346 - Val std: 6.0958 - Val orig: 6.0958\n",
      "Epoch 2 - Avg loss in last 1 epochs: - Train: 6.6244 - Val std: 59.1188 - Val orig: 59.1188\n",
      "Epoch 3 - Avg loss in last 1 epochs: - Train: 59.6555 - Val std: 1.6401 - Val orig: 1.6401\n",
      "Epoch 4 - Avg loss in last 1 epochs: - Train: 2.2601 - Val std: 5.1530 - Val orig: 5.1530\n",
      "Epoch 5 - Avg loss in last 1 epochs: - Train: 5.3094 - Val std: 6.5373 - Val orig: 6.5373\n",
      "Epoch 6 - Avg loss in last 1 epochs: - Train: 7.5576 - Val std: 8.9916 - Val orig: 8.9916\n",
      "Epoch 7 - Avg loss in last 1 epochs: - Train: 10.7836 - Val std: 4.3794 - Val orig: 4.3794\n",
      "Epoch 8 - Avg loss in last 1 epochs: - Train: 8.8006 - Val std: 10.0810 - Val orig: 10.0810\n",
      "Epoch 9 - Avg loss in last 1 epochs: - Train: 13.1388 - Val std: 16.1591 - Val orig: 16.1591\n",
      "Epoch 10 - Avg loss in last 1 epochs: - Train: 20.0191 - Val std: 38.8448 - Val orig: 38.8448\n",
      "Epoch 11 - Avg loss in last 1 epochs: - Train: 40.6243 - Val std: 11.4139 - Val orig: 11.4139\n",
      "Epoch 12 - Avg loss in last 1 epochs: - Train: 16.9091 - Val std: 28.3241 - Val orig: 28.3241\n",
      "Epoch 13 - Avg loss in last 1 epochs: - Train: 30.9464 - Val std: 5.9639 - Val orig: 5.9639\n",
      "Epoch 14 - Avg loss in last 1 epochs: - Train: 11.0758 - Val std: 23.1187 - Val orig: 23.1187\n",
      "Epoch 15 - Avg loss in last 1 epochs: - Train: 24.3424 - Val std: 13.7073 - Val orig: 13.7073\n",
      "Epoch 16 - Avg loss in last 1 epochs: - Train: 14.8370 - Val std: 1.9436 - Val orig: 1.9436\n",
      "Epoch 17 - Avg loss in last 1 epochs: - Train: 4.7118 - Val std: 8.3341 - Val orig: 8.3341\n",
      "Epoch 18 - Avg loss in last 1 epochs: - Train: 9.0917 - Val std: 7.1021 - Val orig: 7.1021\n",
      "Epoch 19 - Avg loss in last 1 epochs: - Train: 7.7156 - Val std: 2.9421 - Val orig: 2.9421\n",
      "Epoch 20 - Avg loss in last 1 epochs: - Train: 3.8084 - Val std: 1.5655 - Val orig: 1.5655\n",
      "Epoch 21 - Avg loss in last 1 epochs: - Train: 2.5380 - Val std: 1.3019 - Val orig: 1.3019\n",
      "Epoch 22 - Avg loss in last 1 epochs: - Train: 2.0833 - Val std: 1.2317 - Val orig: 1.2317\n",
      "Epoch 23 - Avg loss in last 1 epochs: - Train: 1.7850 - Val std: 1.6726 - Val orig: 1.6726\n",
      "Epoch 24 - Avg loss in last 1 epochs: - Train: 1.9497 - Val std: 2.2196 - Val orig: 2.2196\n",
      "Epoch 25 - Avg loss in last 1 epochs: - Train: 2.3506 - Val std: 2.6395 - Val orig: 2.6395\n",
      "Epoch 26 - Avg loss in last 1 epochs: - Train: 2.7110 - Val std: 2.8372 - Val orig: 2.8372\n",
      "Epoch 27 - Avg loss in last 1 epochs: - Train: 2.8841 - Val std: 2.4620 - Val orig: 2.4620\n",
      "Epoch 28 - Avg loss in last 1 epochs: - Train: 2.5084 - Val std: 3.7979 - Val orig: 3.7979\n",
      "Epoch 29 - Avg loss in last 1 epochs: - Train: 3.8599 - Val std: 2.0637 - Val orig: 2.0637\n",
      "Epoch 30 - Avg loss in last 1 epochs: - Train: 2.1293 - Val std: 2.1352 - Val orig: 2.1352\n",
      "Epoch 31 - Avg loss in last 1 epochs: - Train: 2.2061 - Val std: 2.0191 - Val orig: 2.0191\n",
      "Epoch 32 - Avg loss in last 1 epochs: - Train: 2.1342 - Val std: 1.5113 - Val orig: 1.5113\n",
      "Epoch 33 - Avg loss in last 1 epochs: - Train: 1.8421 - Val std: 1.1410 - Val orig: 1.1410\n",
      "Epoch 34 - Avg loss in last 1 epochs: - Train: 2.0958 - Val std: 1.1468 - Val orig: 1.1468\n",
      "Epoch 35 - Avg loss in last 1 epochs: - Train: 2.0386 - Val std: 1.2116 - Val orig: 1.2116\n",
      "Epoch 36 - Avg loss in last 1 epochs: - Train: 1.8248 - Val std: 1.3623 - Val orig: 1.3623\n",
      "Epoch 37 - Avg loss in last 1 epochs: - Train: 1.8978 - Val std: 1.3982 - Val orig: 1.3982\n",
      "Epoch 38 - Avg loss in last 1 epochs: - Train: 1.9770 - Val std: 1.3274 - Val orig: 1.3274\n",
      "Epoch 39 - Avg loss in last 1 epochs: - Train: 1.9798 - Val std: 1.2318 - Val orig: 1.2318\n",
      "Epoch 40 - Avg loss in last 1 epochs: - Train: 1.9511 - Val std: 1.1713 - Val orig: 1.1713\n",
      "Epoch 41 - Avg loss in last 1 epochs: - Train: 1.9313 - Val std: 1.1558 - Val orig: 1.1558\n",
      "Epoch 42 - Avg loss in last 1 epochs: - Train: 1.9231 - Val std: 1.1599 - Val orig: 1.1599\n",
      "Epoch 43 - Avg loss in last 1 epochs: - Train: 1.9032 - Val std: 1.1717 - Val orig: 1.1717\n",
      "Epoch 44 - Avg loss in last 1 epochs: - Train: 1.8710 - Val std: 1.1901 - Val orig: 1.1901\n",
      "Epoch 45 - Avg loss in last 1 epochs: - Train: 1.8349 - Val std: 1.2119 - Val orig: 1.2119\n",
      "Epoch 46 - Avg loss in last 1 epochs: - Train: 1.8007 - Val std: 1.2397 - Val orig: 1.2397\n",
      "Epoch 47 - Avg loss in last 1 epochs: - Train: 1.7756 - Val std: 1.2744 - Val orig: 1.2744\n",
      "Epoch 48 - Avg loss in last 1 epochs: - Train: 1.7635 - Val std: 1.3095 - Val orig: 1.3095\n",
      "Epoch 49 - Avg loss in last 1 epochs: - Train: 1.7602 - Val std: 1.3363 - Val orig: 1.3363\n",
      "Epoch 50 - Avg loss in last 1 epochs: - Train: 1.7613 - Val std: 1.3478 - Val orig: 1.3478\n",
      "Epoch 51 - Avg loss in last 1 epochs: - Train: 1.7581 - Val std: 1.3401 - Val orig: 1.3401\n",
      "Epoch 52 - Avg loss in last 1 epochs: - Train: 1.7475 - Val std: 1.3218 - Val orig: 1.3218\n",
      "Epoch 53 - Avg loss in last 1 epochs: - Train: 1.7353 - Val std: 1.3093 - Val orig: 1.3093\n",
      "Epoch 54 - Avg loss in last 1 epochs: - Train: 1.7339 - Val std: 1.2965 - Val orig: 1.2965\n",
      "Epoch 55 - Avg loss in last 1 epochs: - Train: 1.7334 - Val std: 1.2674 - Val orig: 1.2674\n",
      "Epoch 56 - Avg loss in last 1 epochs: - Train: 1.7208 - Val std: 1.2412 - Val orig: 1.2412\n",
      "Epoch 57 - Avg loss in last 1 epochs: - Train: 1.7141 - Val std: 1.2234 - Val orig: 1.2234\n",
      "Epoch 58 - Avg loss in last 1 epochs: - Train: 1.7121 - Val std: 1.2078 - Val orig: 1.2078\n",
      "Epoch 59 - Avg loss in last 1 epochs: - Train: 1.7102 - Val std: 1.1933 - Val orig: 1.1933\n",
      "Epoch 60 - Avg loss in last 1 epochs: - Train: 1.7082 - Val std: 1.1817 - Val orig: 1.1817\n",
      "Epoch 61 - Avg loss in last 1 epochs: - Train: 1.7012 - Val std: 1.1748 - Val orig: 1.1748\n",
      "Epoch 62 - Avg loss in last 1 epochs: - Train: 1.6973 - Val std: 1.1731 - Val orig: 1.1731\n",
      "Epoch 63 - Avg loss in last 1 epochs: - Train: 1.6898 - Val std: 1.1753 - Val orig: 1.1753\n",
      "Epoch 64 - Avg loss in last 1 epochs: - Train: 1.6817 - Val std: 1.1796 - Val orig: 1.1796\n",
      "Epoch 65 - Avg loss in last 1 epochs: - Train: 1.6761 - Val std: 1.1864 - Val orig: 1.1864\n",
      "Epoch 66 - Avg loss in last 1 epochs: - Train: 1.6633 - Val std: 1.1967 - Val orig: 1.1967\n",
      "Epoch 67 - Avg loss in last 1 epochs: - Train: 1.6589 - Val std: 1.2087 - Val orig: 1.2087\n",
      "Epoch 68 - Avg loss in last 1 epochs: - Train: 1.6525 - Val std: 1.2177 - Val orig: 1.2177\n",
      "Epoch 69 - Avg loss in last 1 epochs: - Train: 1.6482 - Val std: 1.2230 - Val orig: 1.2230\n",
      "Epoch 70 - Avg loss in last 1 epochs: - Train: 1.6437 - Val std: 1.2252 - Val orig: 1.2252\n",
      "Epoch 71 - Avg loss in last 1 epochs: - Train: 1.6380 - Val std: 1.2242 - Val orig: 1.2242\n",
      "Epoch 72 - Avg loss in last 1 epochs: - Train: 1.6339 - Val std: 1.2196 - Val orig: 1.2196\n",
      "Epoch 73 - Avg loss in last 1 epochs: - Train: 1.6309 - Val std: 1.2111 - Val orig: 1.2111\n",
      "Epoch 74 - Avg loss in last 1 epochs: - Train: 1.6268 - Val std: 1.2000 - Val orig: 1.2000\n",
      "Epoch 75 - Avg loss in last 1 epochs: - Train: 1.6209 - Val std: 1.1895 - Val orig: 1.1895\n",
      "Epoch 76 - Avg loss in last 1 epochs: - Train: 1.6183 - Val std: 1.1809 - Val orig: 1.1809\n",
      "Epoch 77 - Avg loss in last 1 epochs: - Train: 1.6160 - Val std: 1.1735 - Val orig: 1.1735\n",
      "Epoch 78 - Avg loss in last 1 epochs: - Train: 1.6150 - Val std: 1.1672 - Val orig: 1.1672\n",
      "Epoch 79 - Avg loss in last 1 epochs: - Train: 1.6138 - Val std: 1.1627 - Val orig: 1.1627\n",
      "Epoch 80 - Avg loss in last 1 epochs: - Train: 1.6108 - Val std: 1.1607 - Val orig: 1.1607\n",
      "Epoch 81 - Avg loss in last 1 epochs: - Train: 1.6081 - Val std: 1.1611 - Val orig: 1.1611\n",
      "Epoch 82 - Avg loss in last 1 epochs: - Train: 1.6066 - Val std: 1.1634 - Val orig: 1.1634\n",
      "Epoch 83 - Avg loss in last 1 epochs: - Train: 1.6030 - Val std: 1.1664 - Val orig: 1.1664\n",
      "Epoch 84 - Avg loss in last 1 epochs: - Train: 1.6018 - Val std: 1.1699 - Val orig: 1.1699\n",
      "Epoch 85 - Avg loss in last 1 epochs: - Train: 1.5976 - Val std: 1.1736 - Val orig: 1.1736\n",
      "Epoch 86 - Avg loss in last 1 epochs: - Train: 1.5967 - Val std: 1.1773 - Val orig: 1.1773\n",
      "Epoch 87 - Avg loss in last 1 epochs: - Train: 1.5932 - Val std: 1.1799 - Val orig: 1.1799\n",
      "Epoch 88 - Avg loss in last 1 epochs: - Train: 1.5918 - Val std: 1.1809 - Val orig: 1.1809\n",
      "Epoch 89 - Avg loss in last 1 epochs: - Train: 1.5884 - Val std: 1.1803 - Val orig: 1.1803\n",
      "Epoch 90 - Avg loss in last 1 epochs: - Train: 1.5863 - Val std: 1.1784 - Val orig: 1.1784\n",
      "Epoch 91 - Avg loss in last 1 epochs: - Train: 1.5844 - Val std: 1.1754 - Val orig: 1.1754\n",
      "Epoch 92 - Avg loss in last 1 epochs: - Train: 1.5806 - Val std: 1.1711 - Val orig: 1.1711\n",
      "Epoch 93 - Avg loss in last 1 epochs: - Train: 1.5786 - Val std: 1.1662 - Val orig: 1.1662\n",
      "Epoch 94 - Avg loss in last 1 epochs: - Train: 1.5753 - Val std: 1.1613 - Val orig: 1.1613\n",
      "Epoch 95 - Avg loss in last 1 epochs: - Train: 1.5739 - Val std: 1.1569 - Val orig: 1.1569\n",
      "Epoch 96 - Avg loss in last 1 epochs: - Train: 1.5715 - Val std: 1.1533 - Val orig: 1.1533\n",
      "Epoch 97 - Avg loss in last 1 epochs: - Train: 1.5692 - Val std: 1.1504 - Val orig: 1.1504\n",
      "Epoch 98 - Avg loss in last 1 epochs: - Train: 1.5654 - Val std: 1.1483 - Val orig: 1.1483\n",
      "Epoch 99 - Avg loss in last 1 epochs: - Train: 1.5625 - Val std: 1.1472 - Val orig: 1.1472\n",
      "Epoch 100 - Avg loss in last 1 epochs: - Train: 1.5605 - Val std: 1.1470 - Val orig: 1.1470\n",
      "Epoch 101 - Avg loss in last 1 epochs: - Train: 1.5586 - Val std: 1.1474 - Val orig: 1.1474\n",
      "Epoch 102 - Avg loss in last 1 epochs: - Train: 1.5554 - Val std: 1.1479 - Val orig: 1.1479\n",
      "Epoch 103 - Avg loss in last 1 epochs: - Train: 1.5549 - Val std: 1.1487 - Val orig: 1.1487\n",
      "Epoch 104 - Avg loss in last 1 epochs: - Train: 1.5513 - Val std: 1.1494 - Val orig: 1.1494\n",
      "Epoch 105 - Avg loss in last 1 epochs: - Train: 1.5497 - Val std: 1.1498 - Val orig: 1.1498\n",
      "Epoch 106 - Avg loss in last 1 epochs: - Train: 1.5476 - Val std: 1.1498 - Val orig: 1.1498\n",
      "Epoch 107 - Avg loss in last 1 epochs: - Train: 1.5458 - Val std: 1.1493 - Val orig: 1.1493\n",
      "Epoch 108 - Avg loss in last 1 epochs: - Train: 1.5435 - Val std: 1.1482 - Val orig: 1.1482\n",
      "Epoch 109 - Avg loss in last 1 epochs: - Train: 1.5411 - Val std: 1.1467 - Val orig: 1.1467\n",
      "Epoch 110 - Avg loss in last 1 epochs: - Train: 1.5398 - Val std: 1.1447 - Val orig: 1.1447\n",
      "Epoch 111 - Avg loss in last 1 epochs: - Train: 1.5372 - Val std: 1.1425 - Val orig: 1.1425\n",
      "Epoch 112 - Avg loss in last 1 epochs: - Train: 1.5337 - Val std: 1.1401 - Val orig: 1.1401\n",
      "Epoch 113 - Avg loss in last 1 epochs: - Train: 1.5332 - Val std: 1.1378 - Val orig: 1.1378\n",
      "Epoch 114 - Avg loss in last 1 epochs: - Train: 1.5296 - Val std: 1.1358 - Val orig: 1.1358\n",
      "Epoch 115 - Avg loss in last 1 epochs: - Train: 1.5261 - Val std: 1.1341 - Val orig: 1.1341\n",
      "Epoch 116 - Avg loss in last 1 epochs: - Train: 1.5258 - Val std: 1.1329 - Val orig: 1.1329\n",
      "Epoch 117 - Avg loss in last 1 epochs: - Train: 1.5223 - Val std: 1.1321 - Val orig: 1.1321\n",
      "Epoch 118 - Avg loss in last 1 epochs: - Train: 1.5207 - Val std: 1.1317 - Val orig: 1.1317\n",
      "Epoch 119 - Avg loss in last 1 epochs: - Train: 1.5183 - Val std: 1.1315 - Val orig: 1.1315\n",
      "Epoch 120 - Avg loss in last 1 epochs: - Train: 1.5175 - Val std: 1.1313 - Val orig: 1.1313\n",
      "Epoch 121 - Avg loss in last 1 epochs: - Train: 1.5143 - Val std: 1.1311 - Val orig: 1.1311\n",
      "Epoch 122 - Avg loss in last 1 epochs: - Train: 1.5127 - Val std: 1.1308 - Val orig: 1.1308\n",
      "Epoch 123 - Avg loss in last 1 epochs: - Train: 1.5112 - Val std: 1.1303 - Val orig: 1.1303\n",
      "Epoch 124 - Avg loss in last 1 epochs: - Train: 1.5091 - Val std: 1.1297 - Val orig: 1.1297\n",
      "Epoch 125 - Avg loss in last 1 epochs: - Train: 1.5061 - Val std: 1.1288 - Val orig: 1.1288\n",
      "Epoch 126 - Avg loss in last 1 epochs: - Train: 1.5043 - Val std: 1.1279 - Val orig: 1.1279\n",
      "Epoch 127 - Avg loss in last 1 epochs: - Train: 1.5015 - Val std: 1.1269 - Val orig: 1.1269\n",
      "Epoch 128 - Avg loss in last 1 epochs: - Train: 1.4984 - Val std: 1.1256 - Val orig: 1.1256\n",
      "Epoch 129 - Avg loss in last 1 epochs: - Train: 1.4985 - Val std: 1.1243 - Val orig: 1.1243\n",
      "Epoch 130 - Avg loss in last 1 epochs: - Train: 1.4956 - Val std: 1.1229 - Val orig: 1.1229\n",
      "Epoch 131 - Avg loss in last 1 epochs: - Train: 1.4942 - Val std: 1.1214 - Val orig: 1.1214\n",
      "Epoch 132 - Avg loss in last 1 epochs: - Train: 1.4916 - Val std: 1.1201 - Val orig: 1.1201\n",
      "Epoch 133 - Avg loss in last 1 epochs: - Train: 1.4880 - Val std: 1.1189 - Val orig: 1.1189\n",
      "Epoch 134 - Avg loss in last 1 epochs: - Train: 1.4868 - Val std: 1.1179 - Val orig: 1.1179\n",
      "Epoch 135 - Avg loss in last 1 epochs: - Train: 1.4843 - Val std: 1.1170 - Val orig: 1.1170\n",
      "Epoch 136 - Avg loss in last 1 epochs: - Train: 1.4833 - Val std: 1.1164 - Val orig: 1.1164\n",
      "Epoch 137 - Avg loss in last 1 epochs: - Train: 1.4816 - Val std: 1.1160 - Val orig: 1.1160\n",
      "Epoch 138 - Avg loss in last 1 epochs: - Train: 1.4786 - Val std: 1.1155 - Val orig: 1.1155\n",
      "Epoch 139 - Avg loss in last 1 epochs: - Train: 1.4771 - Val std: 1.1151 - Val orig: 1.1151\n",
      "Epoch 140 - Avg loss in last 1 epochs: - Train: 1.4758 - Val std: 1.1145 - Val orig: 1.1145\n",
      "Epoch 141 - Avg loss in last 1 epochs: - Train: 1.4734 - Val std: 1.1139 - Val orig: 1.1139\n",
      "Epoch 142 - Avg loss in last 1 epochs: - Train: 1.4710 - Val std: 1.1132 - Val orig: 1.1132\n",
      "Epoch 143 - Avg loss in last 1 epochs: - Train: 1.4679 - Val std: 1.1123 - Val orig: 1.1123\n",
      "Epoch 144 - Avg loss in last 1 epochs: - Train: 1.4664 - Val std: 1.1114 - Val orig: 1.1114\n",
      "Epoch 145 - Avg loss in last 1 epochs: - Train: 1.4657 - Val std: 1.1103 - Val orig: 1.1103\n",
      "Epoch 146 - Avg loss in last 1 epochs: - Train: 1.4628 - Val std: 1.1093 - Val orig: 1.1093\n",
      "Epoch 147 - Avg loss in last 1 epochs: - Train: 1.4593 - Val std: 1.1083 - Val orig: 1.1083\n",
      "Epoch 148 - Avg loss in last 1 epochs: - Train: 1.4587 - Val std: 1.1073 - Val orig: 1.1073\n",
      "Epoch 149 - Avg loss in last 1 epochs: - Train: 1.4567 - Val std: 1.1065 - Val orig: 1.1065\n",
      "Epoch 150 - Avg loss in last 1 epochs: - Train: 1.4551 - Val std: 1.1057 - Val orig: 1.1057\n",
      "Epoch 151 - Avg loss in last 1 epochs: - Train: 1.4525 - Val std: 1.1050 - Val orig: 1.1050\n",
      "Epoch 152 - Avg loss in last 1 epochs: - Train: 1.4524 - Val std: 1.1044 - Val orig: 1.1044\n",
      "Epoch 153 - Avg loss in last 1 epochs: - Train: 1.4494 - Val std: 1.1039 - Val orig: 1.1039\n",
      "Epoch 154 - Avg loss in last 1 epochs: - Train: 1.4475 - Val std: 1.1033 - Val orig: 1.1033\n",
      "Epoch 155 - Avg loss in last 1 epochs: - Train: 1.4440 - Val std: 1.1028 - Val orig: 1.1028\n",
      "Epoch 156 - Avg loss in last 1 epochs: - Train: 1.4429 - Val std: 1.1021 - Val orig: 1.1021\n",
      "Epoch 157 - Avg loss in last 1 epochs: - Train: 1.4405 - Val std: 1.1013 - Val orig: 1.1013\n",
      "Epoch 158 - Avg loss in last 1 epochs: - Train: 1.4396 - Val std: 1.1005 - Val orig: 1.1005\n",
      "Epoch 159 - Avg loss in last 1 epochs: - Train: 1.4380 - Val std: 1.0996 - Val orig: 1.0996\n",
      "Epoch 160 - Avg loss in last 1 epochs: - Train: 1.4350 - Val std: 1.0988 - Val orig: 1.0988\n",
      "Epoch 161 - Avg loss in last 1 epochs: - Train: 1.4333 - Val std: 1.0980 - Val orig: 1.0980\n",
      "Epoch 162 - Avg loss in last 1 epochs: - Train: 1.4313 - Val std: 1.0971 - Val orig: 1.0971\n",
      "Epoch 163 - Avg loss in last 1 epochs: - Train: 1.4294 - Val std: 1.0963 - Val orig: 1.0963\n",
      "Epoch 164 - Avg loss in last 1 epochs: - Train: 1.4267 - Val std: 1.0955 - Val orig: 1.0955\n",
      "Epoch 165 - Avg loss in last 1 epochs: - Train: 1.4267 - Val std: 1.0948 - Val orig: 1.0948\n",
      "Epoch 166 - Avg loss in last 1 epochs: - Train: 1.4235 - Val std: 1.0942 - Val orig: 1.0942\n",
      "Epoch 167 - Avg loss in last 1 epochs: - Train: 1.4208 - Val std: 1.0936 - Val orig: 1.0936\n",
      "Epoch 168 - Avg loss in last 1 epochs: - Train: 1.4197 - Val std: 1.0931 - Val orig: 1.0931\n",
      "Epoch 169 - Avg loss in last 1 epochs: - Train: 1.4172 - Val std: 1.0925 - Val orig: 1.0925\n",
      "Epoch 170 - Avg loss in last 1 epochs: - Train: 1.4162 - Val std: 1.0920 - Val orig: 1.0920\n",
      "Epoch 171 - Avg loss in last 1 epochs: - Train: 1.4146 - Val std: 1.0914 - Val orig: 1.0914\n",
      "Epoch 172 - Avg loss in last 1 epochs: - Train: 1.4131 - Val std: 1.0908 - Val orig: 1.0908\n",
      "Epoch 173 - Avg loss in last 1 epochs: - Train: 1.4115 - Val std: 1.0902 - Val orig: 1.0902\n",
      "Epoch 174 - Avg loss in last 1 epochs: - Train: 1.4067 - Val std: 1.0896 - Val orig: 1.0896\n",
      "Epoch 175 - Avg loss in last 1 epochs: - Train: 1.4067 - Val std: 1.0890 - Val orig: 1.0890\n",
      "Epoch 176 - Avg loss in last 1 epochs: - Train: 1.4046 - Val std: 1.0883 - Val orig: 1.0883\n",
      "Epoch 177 - Avg loss in last 1 epochs: - Train: 1.4024 - Val std: 1.0877 - Val orig: 1.0877\n",
      "Epoch 178 - Avg loss in last 1 epochs: - Train: 1.4018 - Val std: 1.0870 - Val orig: 1.0870\n",
      "Epoch 179 - Avg loss in last 1 epochs: - Train: 1.3987 - Val std: 1.0862 - Val orig: 1.0862\n",
      "Epoch 180 - Avg loss in last 1 epochs: - Train: 1.3963 - Val std: 1.0855 - Val orig: 1.0855\n",
      "Epoch 181 - Avg loss in last 1 epochs: - Train: 1.3961 - Val std: 1.0847 - Val orig: 1.0847\n",
      "Epoch 182 - Avg loss in last 1 epochs: - Train: 1.3938 - Val std: 1.0840 - Val orig: 1.0840\n",
      "Epoch 183 - Avg loss in last 1 epochs: - Train: 1.3916 - Val std: 1.0833 - Val orig: 1.0833\n",
      "Epoch 184 - Avg loss in last 1 epochs: - Train: 1.3911 - Val std: 1.0828 - Val orig: 1.0828\n",
      "Epoch 185 - Avg loss in last 1 epochs: - Train: 1.3888 - Val std: 1.0823 - Val orig: 1.0823\n",
      "Epoch 186 - Avg loss in last 1 epochs: - Train: 1.3867 - Val std: 1.0818 - Val orig: 1.0818\n",
      "Epoch 187 - Avg loss in last 1 epochs: - Train: 1.3848 - Val std: 1.0814 - Val orig: 1.0814\n",
      "Epoch 188 - Avg loss in last 1 epochs: - Train: 1.3827 - Val std: 1.0809 - Val orig: 1.0809\n",
      "Epoch 189 - Avg loss in last 1 epochs: - Train: 1.3804 - Val std: 1.0803 - Val orig: 1.0803\n",
      "Epoch 190 - Avg loss in last 1 epochs: - Train: 1.3799 - Val std: 1.0797 - Val orig: 1.0797\n",
      "Epoch 191 - Avg loss in last 1 epochs: - Train: 1.3783 - Val std: 1.0791 - Val orig: 1.0791\n",
      "Epoch 192 - Avg loss in last 1 epochs: - Train: 1.3752 - Val std: 1.0784 - Val orig: 1.0784\n",
      "Epoch 193 - Avg loss in last 1 epochs: - Train: 1.3723 - Val std: 1.0777 - Val orig: 1.0777\n",
      "Epoch 194 - Avg loss in last 1 epochs: - Train: 1.3728 - Val std: 1.0769 - Val orig: 1.0769\n",
      "Epoch 195 - Avg loss in last 1 epochs: - Train: 1.3694 - Val std: 1.0762 - Val orig: 1.0762\n",
      "Epoch 196 - Avg loss in last 1 epochs: - Train: 1.3692 - Val std: 1.0756 - Val orig: 1.0756\n",
      "Epoch 197 - Avg loss in last 1 epochs: - Train: 1.3665 - Val std: 1.0751 - Val orig: 1.0751\n",
      "Epoch 198 - Avg loss in last 1 epochs: - Train: 1.3643 - Val std: 1.0746 - Val orig: 1.0746\n",
      "Epoch 199 - Avg loss in last 1 epochs: - Train: 1.3637 - Val std: 1.0742 - Val orig: 1.0742\n",
      "Epoch 200 - Avg loss in last 1 epochs: - Train: 1.3622 - Val std: 1.0738 - Val orig: 1.0738\n",
      "Epoch 201 - Avg loss in last 1 epochs: - Train: 1.3601 - Val std: 1.0734 - Val orig: 1.0734\n",
      "Epoch 202 - Avg loss in last 1 epochs: - Train: 1.3584 - Val std: 1.0730 - Val orig: 1.0730\n",
      "Epoch 203 - Avg loss in last 1 epochs: - Train: 1.3562 - Val std: 1.0726 - Val orig: 1.0726\n",
      "Epoch 204 - Avg loss in last 1 epochs: - Train: 1.3554 - Val std: 1.0721 - Val orig: 1.0721\n",
      "Epoch 205 - Avg loss in last 1 epochs: - Train: 1.3527 - Val std: 1.0716 - Val orig: 1.0716\n",
      "Epoch 206 - Avg loss in last 1 epochs: - Train: 1.3509 - Val std: 1.0711 - Val orig: 1.0711\n",
      "Epoch 207 - Avg loss in last 1 epochs: - Train: 1.3497 - Val std: 1.0705 - Val orig: 1.0705\n",
      "Epoch 208 - Avg loss in last 1 epochs: - Train: 1.3475 - Val std: 1.0699 - Val orig: 1.0699\n",
      "Epoch 209 - Avg loss in last 1 epochs: - Train: 1.3467 - Val std: 1.0692 - Val orig: 1.0692\n",
      "Epoch 210 - Avg loss in last 1 epochs: - Train: 1.3455 - Val std: 1.0686 - Val orig: 1.0686\n",
      "Epoch 211 - Avg loss in last 1 epochs: - Train: 1.3423 - Val std: 1.0680 - Val orig: 1.0680\n",
      "Epoch 212 - Avg loss in last 1 epochs: - Train: 1.3428 - Val std: 1.0674 - Val orig: 1.0674\n",
      "Epoch 213 - Avg loss in last 1 epochs: - Train: 1.3396 - Val std: 1.0669 - Val orig: 1.0669\n",
      "Epoch 214 - Avg loss in last 1 epochs: - Train: 1.3376 - Val std: 1.0665 - Val orig: 1.0665\n",
      "Epoch 215 - Avg loss in last 1 epochs: - Train: 1.3363 - Val std: 1.0660 - Val orig: 1.0660\n",
      "Epoch 216 - Avg loss in last 1 epochs: - Train: 1.3344 - Val std: 1.0656 - Val orig: 1.0656\n",
      "Epoch 217 - Avg loss in last 1 epochs: - Train: 1.3321 - Val std: 1.0652 - Val orig: 1.0652\n",
      "Epoch 218 - Avg loss in last 1 epochs: - Train: 1.3307 - Val std: 1.0647 - Val orig: 1.0647\n",
      "Epoch 219 - Avg loss in last 1 epochs: - Train: 1.3290 - Val std: 1.0642 - Val orig: 1.0642\n",
      "Epoch 220 - Avg loss in last 1 epochs: - Train: 1.3284 - Val std: 1.0637 - Val orig: 1.0637\n",
      "Epoch 221 - Avg loss in last 1 epochs: - Train: 1.3275 - Val std: 1.0633 - Val orig: 1.0633\n",
      "Epoch 222 - Avg loss in last 1 epochs: - Train: 1.3241 - Val std: 1.0628 - Val orig: 1.0628\n",
      "Epoch 223 - Avg loss in last 1 epochs: - Train: 1.3231 - Val std: 1.0624 - Val orig: 1.0624\n",
      "Epoch 224 - Avg loss in last 1 epochs: - Train: 1.3217 - Val std: 1.0619 - Val orig: 1.0619\n",
      "Epoch 225 - Avg loss in last 1 epochs: - Train: 1.3196 - Val std: 1.0615 - Val orig: 1.0615\n",
      "Epoch 226 - Avg loss in last 1 epochs: - Train: 1.3195 - Val std: 1.0610 - Val orig: 1.0610\n",
      "Epoch 227 - Avg loss in last 1 epochs: - Train: 1.3178 - Val std: 1.0605 - Val orig: 1.0605\n",
      "Epoch 228 - Avg loss in last 1 epochs: - Train: 1.3153 - Val std: 1.0601 - Val orig: 1.0601\n",
      "Epoch 229 - Avg loss in last 1 epochs: - Train: 1.3132 - Val std: 1.0596 - Val orig: 1.0596\n",
      "Epoch 230 - Avg loss in last 1 epochs: - Train: 1.3120 - Val std: 1.0592 - Val orig: 1.0592\n",
      "Epoch 231 - Avg loss in last 1 epochs: - Train: 1.3102 - Val std: 1.0588 - Val orig: 1.0588\n",
      "Epoch 232 - Avg loss in last 1 epochs: - Train: 1.3091 - Val std: 1.0583 - Val orig: 1.0583\n",
      "Epoch 233 - Avg loss in last 1 epochs: - Train: 1.3079 - Val std: 1.0579 - Val orig: 1.0579\n",
      "Epoch 234 - Avg loss in last 1 epochs: - Train: 1.3065 - Val std: 1.0575 - Val orig: 1.0575\n",
      "Epoch 235 - Avg loss in last 1 epochs: - Train: 1.3046 - Val std: 1.0571 - Val orig: 1.0571\n",
      "Epoch 236 - Avg loss in last 1 epochs: - Train: 1.3014 - Val std: 1.0566 - Val orig: 1.0566\n",
      "Epoch 237 - Avg loss in last 1 epochs: - Train: 1.3016 - Val std: 1.0562 - Val orig: 1.0562\n",
      "Epoch 238 - Avg loss in last 1 epochs: - Train: 1.2994 - Val std: 1.0558 - Val orig: 1.0558\n",
      "Epoch 239 - Avg loss in last 1 epochs: - Train: 1.2975 - Val std: 1.0554 - Val orig: 1.0554\n",
      "Epoch 240 - Avg loss in last 1 epochs: - Train: 1.2965 - Val std: 1.0550 - Val orig: 1.0550\n",
      "Epoch 241 - Avg loss in last 1 epochs: - Train: 1.2960 - Val std: 1.0546 - Val orig: 1.0546\n",
      "Epoch 242 - Avg loss in last 1 epochs: - Train: 1.2944 - Val std: 1.0542 - Val orig: 1.0542\n",
      "Epoch 243 - Avg loss in last 1 epochs: - Train: 1.2926 - Val std: 1.0538 - Val orig: 1.0538\n",
      "Epoch 244 - Avg loss in last 1 epochs: - Train: 1.2915 - Val std: 1.0534 - Val orig: 1.0534\n",
      "Epoch 245 - Avg loss in last 1 epochs: - Train: 1.2896 - Val std: 1.0530 - Val orig: 1.0530\n",
      "Epoch 246 - Avg loss in last 1 epochs: - Train: 1.2879 - Val std: 1.0527 - Val orig: 1.0527\n",
      "Epoch 247 - Avg loss in last 1 epochs: - Train: 1.2867 - Val std: 1.0523 - Val orig: 1.0523\n",
      "Epoch 248 - Avg loss in last 1 epochs: - Train: 1.2856 - Val std: 1.0518 - Val orig: 1.0518\n",
      "Epoch 249 - Avg loss in last 1 epochs: - Train: 1.2838 - Val std: 1.0514 - Val orig: 1.0514\n",
      "Epoch 250 - Avg loss in last 1 epochs: - Train: 1.2826 - Val std: 1.0510 - Val orig: 1.0510\n",
      "Epoch 251 - Avg loss in last 1 epochs: - Train: 1.2814 - Val std: 1.0506 - Val orig: 1.0506\n",
      "Epoch 252 - Avg loss in last 1 epochs: - Train: 1.2799 - Val std: 1.0503 - Val orig: 1.0503\n",
      "Epoch 253 - Avg loss in last 1 epochs: - Train: 1.2782 - Val std: 1.0499 - Val orig: 1.0499\n",
      "Epoch 254 - Avg loss in last 1 epochs: - Train: 1.2768 - Val std: 1.0496 - Val orig: 1.0496\n",
      "Epoch 255 - Avg loss in last 1 epochs: - Train: 1.2749 - Val std: 1.0492 - Val orig: 1.0492\n",
      "Epoch 256 - Avg loss in last 1 epochs: - Train: 1.2741 - Val std: 1.0488 - Val orig: 1.0488\n",
      "Epoch 257 - Avg loss in last 1 epochs: - Train: 1.2722 - Val std: 1.0483 - Val orig: 1.0483\n",
      "Epoch 258 - Avg loss in last 1 epochs: - Train: 1.2706 - Val std: 1.0479 - Val orig: 1.0479\n",
      "Epoch 259 - Avg loss in last 1 epochs: - Train: 1.2698 - Val std: 1.0475 - Val orig: 1.0475\n",
      "Epoch 260 - Avg loss in last 1 epochs: - Train: 1.2679 - Val std: 1.0472 - Val orig: 1.0472\n",
      "Epoch 261 - Avg loss in last 1 epochs: - Train: 1.2674 - Val std: 1.0468 - Val orig: 1.0468\n",
      "Epoch 262 - Avg loss in last 1 epochs: - Train: 1.2654 - Val std: 1.0465 - Val orig: 1.0465\n",
      "Epoch 263 - Avg loss in last 1 epochs: - Train: 1.2635 - Val std: 1.0462 - Val orig: 1.0462\n",
      "Epoch 264 - Avg loss in last 1 epochs: - Train: 1.2630 - Val std: 1.0459 - Val orig: 1.0459\n",
      "Epoch 265 - Avg loss in last 1 epochs: - Train: 1.2621 - Val std: 1.0455 - Val orig: 1.0455\n",
      "Epoch 266 - Avg loss in last 1 epochs: - Train: 1.2590 - Val std: 1.0451 - Val orig: 1.0451\n",
      "Epoch 267 - Avg loss in last 1 epochs: - Train: 1.2580 - Val std: 1.0447 - Val orig: 1.0447\n",
      "Epoch 268 - Avg loss in last 1 epochs: - Train: 1.2573 - Val std: 1.0444 - Val orig: 1.0444\n",
      "Epoch 269 - Avg loss in last 1 epochs: - Train: 1.2559 - Val std: 1.0441 - Val orig: 1.0441\n",
      "Epoch 270 - Avg loss in last 1 epochs: - Train: 1.2545 - Val std: 1.0439 - Val orig: 1.0439\n",
      "Epoch 271 - Avg loss in last 1 epochs: - Train: 1.2533 - Val std: 1.0436 - Val orig: 1.0436\n",
      "Epoch 272 - Avg loss in last 1 epochs: - Train: 1.2524 - Val std: 1.0432 - Val orig: 1.0432\n",
      "Epoch 273 - Avg loss in last 1 epochs: - Train: 1.2503 - Val std: 1.0428 - Val orig: 1.0428\n",
      "Epoch 274 - Avg loss in last 1 epochs: - Train: 1.2506 - Val std: 1.0424 - Val orig: 1.0424\n",
      "Epoch 275 - Avg loss in last 1 epochs: - Train: 1.2477 - Val std: 1.0420 - Val orig: 1.0420\n",
      "Epoch 276 - Avg loss in last 1 epochs: - Train: 1.2466 - Val std: 1.0416 - Val orig: 1.0416\n",
      "Epoch 277 - Avg loss in last 1 epochs: - Train: 1.2446 - Val std: 1.0413 - Val orig: 1.0413\n",
      "Epoch 278 - Avg loss in last 1 epochs: - Train: 1.2438 - Val std: 1.0411 - Val orig: 1.0411\n",
      "Epoch 279 - Avg loss in last 1 epochs: - Train: 1.2431 - Val std: 1.0408 - Val orig: 1.0408\n",
      "Epoch 280 - Avg loss in last 1 epochs: - Train: 1.2413 - Val std: 1.0406 - Val orig: 1.0406\n",
      "Epoch 281 - Avg loss in last 1 epochs: - Train: 1.2413 - Val std: 1.0403 - Val orig: 1.0403\n",
      "Epoch 282 - Avg loss in last 1 epochs: - Train: 1.2398 - Val std: 1.0401 - Val orig: 1.0401\n",
      "Epoch 283 - Avg loss in last 1 epochs: - Train: 1.2375 - Val std: 1.0398 - Val orig: 1.0398\n",
      "Epoch 284 - Avg loss in last 1 epochs: - Train: 1.2363 - Val std: 1.0394 - Val orig: 1.0394\n",
      "Epoch 285 - Avg loss in last 1 epochs: - Train: 1.2359 - Val std: 1.0391 - Val orig: 1.0391\n",
      "Epoch 286 - Avg loss in last 1 epochs: - Train: 1.2350 - Val std: 1.0389 - Val orig: 1.0389\n",
      "Epoch 287 - Avg loss in last 1 epochs: - Train: 1.2333 - Val std: 1.0386 - Val orig: 1.0386\n",
      "Epoch 288 - Avg loss in last 1 epochs: - Train: 1.2319 - Val std: 1.0382 - Val orig: 1.0382\n",
      "Epoch 289 - Avg loss in last 1 epochs: - Train: 1.2309 - Val std: 1.0378 - Val orig: 1.0378\n",
      "Epoch 290 - Avg loss in last 1 epochs: - Train: 1.2297 - Val std: 1.0374 - Val orig: 1.0374\n",
      "Epoch 291 - Avg loss in last 1 epochs: - Train: 1.2289 - Val std: 1.0371 - Val orig: 1.0371\n",
      "Epoch 292 - Avg loss in last 1 epochs: - Train: 1.2277 - Val std: 1.0368 - Val orig: 1.0368\n",
      "Epoch 293 - Avg loss in last 1 epochs: - Train: 1.2256 - Val std: 1.0365 - Val orig: 1.0365\n",
      "Epoch 294 - Avg loss in last 1 epochs: - Train: 1.2249 - Val std: 1.0362 - Val orig: 1.0362\n",
      "Epoch 295 - Avg loss in last 1 epochs: - Train: 1.2231 - Val std: 1.0359 - Val orig: 1.0359\n",
      "Epoch 296 - Avg loss in last 1 epochs: - Train: 1.2229 - Val std: 1.0357 - Val orig: 1.0357\n",
      "Epoch 297 - Avg loss in last 1 epochs: - Train: 1.2217 - Val std: 1.0354 - Val orig: 1.0354\n",
      "Epoch 298 - Avg loss in last 1 epochs: - Train: 1.2198 - Val std: 1.0351 - Val orig: 1.0351\n",
      "Epoch 299 - Avg loss in last 1 epochs: - Train: 1.2193 - Val std: 1.0349 - Val orig: 1.0349\n",
      "Epoch 300 - Avg loss in last 1 epochs: - Train: 1.2175 - Val std: 1.0346 - Val orig: 1.0346\n",
      "Epoch 301 - Avg loss in last 1 epochs: - Train: 1.2164 - Val std: 1.0344 - Val orig: 1.0344\n",
      "Epoch 302 - Avg loss in last 1 epochs: - Train: 1.2157 - Val std: 1.0342 - Val orig: 1.0342\n",
      "Epoch 303 - Avg loss in last 1 epochs: - Train: 1.2137 - Val std: 1.0339 - Val orig: 1.0339\n",
      "Epoch 304 - Avg loss in last 1 epochs: - Train: 1.2122 - Val std: 1.0336 - Val orig: 1.0336\n",
      "Epoch 305 - Avg loss in last 1 epochs: - Train: 1.2126 - Val std: 1.0333 - Val orig: 1.0333\n",
      "Epoch 306 - Avg loss in last 1 epochs: - Train: 1.2113 - Val std: 1.0331 - Val orig: 1.0331\n",
      "Epoch 307 - Avg loss in last 1 epochs: - Train: 1.2099 - Val std: 1.0328 - Val orig: 1.0328\n",
      "Epoch 308 - Avg loss in last 1 epochs: - Train: 1.2084 - Val std: 1.0326 - Val orig: 1.0326\n",
      "Epoch 309 - Avg loss in last 1 epochs: - Train: 1.2070 - Val std: 1.0324 - Val orig: 1.0324\n",
      "Epoch 310 - Avg loss in last 1 epochs: - Train: 1.2065 - Val std: 1.0321 - Val orig: 1.0321\n",
      "Epoch 311 - Avg loss in last 1 epochs: - Train: 1.2062 - Val std: 1.0319 - Val orig: 1.0319\n",
      "Epoch 312 - Avg loss in last 1 epochs: - Train: 1.2044 - Val std: 1.0316 - Val orig: 1.0316\n",
      "Epoch 313 - Avg loss in last 1 epochs: - Train: 1.2037 - Val std: 1.0313 - Val orig: 1.0313\n",
      "Epoch 314 - Avg loss in last 1 epochs: - Train: 1.2016 - Val std: 1.0310 - Val orig: 1.0310\n",
      "Epoch 315 - Avg loss in last 1 epochs: - Train: 1.2008 - Val std: 1.0307 - Val orig: 1.0307\n",
      "Epoch 316 - Avg loss in last 1 epochs: - Train: 1.2002 - Val std: 1.0305 - Val orig: 1.0305\n",
      "Epoch 317 - Avg loss in last 1 epochs: - Train: 1.1987 - Val std: 1.0302 - Val orig: 1.0302\n",
      "Epoch 318 - Avg loss in last 1 epochs: - Train: 1.1973 - Val std: 1.0299 - Val orig: 1.0299\n",
      "Epoch 319 - Avg loss in last 1 epochs: - Train: 1.1963 - Val std: 1.0296 - Val orig: 1.0296\n",
      "Epoch 320 - Avg loss in last 1 epochs: - Train: 1.1960 - Val std: 1.0294 - Val orig: 1.0294\n",
      "Epoch 321 - Avg loss in last 1 epochs: - Train: 1.1948 - Val std: 1.0291 - Val orig: 1.0291\n",
      "Epoch 322 - Avg loss in last 1 epochs: - Train: 1.1948 - Val std: 1.0290 - Val orig: 1.0290\n",
      "Epoch 323 - Avg loss in last 1 epochs: - Train: 1.1924 - Val std: 1.0289 - Val orig: 1.0289\n",
      "Epoch 324 - Avg loss in last 1 epochs: - Train: 1.1924 - Val std: 1.0288 - Val orig: 1.0288\n",
      "Epoch 325 - Avg loss in last 1 epochs: - Train: 1.1906 - Val std: 1.0287 - Val orig: 1.0287\n",
      "Epoch 326 - Avg loss in last 1 epochs: - Train: 1.1890 - Val std: 1.0285 - Val orig: 1.0285\n",
      "Epoch 327 - Avg loss in last 1 epochs: - Train: 1.1889 - Val std: 1.0283 - Val orig: 1.0283\n",
      "Epoch 328 - Avg loss in last 1 epochs: - Train: 1.1876 - Val std: 1.0280 - Val orig: 1.0280\n",
      "Epoch 329 - Avg loss in last 1 epochs: - Train: 1.1863 - Val std: 1.0276 - Val orig: 1.0276\n",
      "Epoch 330 - Avg loss in last 1 epochs: - Train: 1.1860 - Val std: 1.0273 - Val orig: 1.0273\n",
      "Epoch 331 - Avg loss in last 1 epochs: - Train: 1.1850 - Val std: 1.0270 - Val orig: 1.0270\n",
      "Epoch 332 - Avg loss in last 1 epochs: - Train: 1.1834 - Val std: 1.0267 - Val orig: 1.0267\n",
      "Epoch 333 - Avg loss in last 1 epochs: - Train: 1.1826 - Val std: 1.0265 - Val orig: 1.0265\n",
      "Epoch 334 - Avg loss in last 1 epochs: - Train: 1.1811 - Val std: 1.0262 - Val orig: 1.0262\n",
      "Epoch 335 - Avg loss in last 1 epochs: - Train: 1.1808 - Val std: 1.0260 - Val orig: 1.0260\n",
      "Epoch 336 - Avg loss in last 1 epochs: - Train: 1.1795 - Val std: 1.0258 - Val orig: 1.0258\n",
      "Epoch 337 - Avg loss in last 1 epochs: - Train: 1.1797 - Val std: 1.0257 - Val orig: 1.0257\n",
      "Epoch 338 - Avg loss in last 1 epochs: - Train: 1.1777 - Val std: 1.0255 - Val orig: 1.0255\n",
      "Epoch 339 - Avg loss in last 1 epochs: - Train: 1.1761 - Val std: 1.0254 - Val orig: 1.0254\n",
      "Epoch 340 - Avg loss in last 1 epochs: - Train: 1.1756 - Val std: 1.0252 - Val orig: 1.0252\n",
      "Epoch 341 - Avg loss in last 1 epochs: - Train: 1.1748 - Val std: 1.0250 - Val orig: 1.0250\n",
      "Epoch 342 - Avg loss in last 1 epochs: - Train: 1.1744 - Val std: 1.0248 - Val orig: 1.0248\n",
      "Epoch 343 - Avg loss in last 1 epochs: - Train: 1.1732 - Val std: 1.0246 - Val orig: 1.0246\n",
      "Epoch 344 - Avg loss in last 1 epochs: - Train: 1.1732 - Val std: 1.0243 - Val orig: 1.0243\n",
      "Epoch 345 - Avg loss in last 1 epochs: - Train: 1.1720 - Val std: 1.0242 - Val orig: 1.0242\n",
      "Epoch 346 - Avg loss in last 1 epochs: - Train: 1.1711 - Val std: 1.0240 - Val orig: 1.0240\n",
      "Epoch 347 - Avg loss in last 1 epochs: - Train: 1.1699 - Val std: 1.0238 - Val orig: 1.0238\n",
      "Epoch 348 - Avg loss in last 1 epochs: - Train: 1.1690 - Val std: 1.0236 - Val orig: 1.0236\n",
      "Epoch 349 - Avg loss in last 1 epochs: - Train: 1.1682 - Val std: 1.0234 - Val orig: 1.0234\n",
      "Epoch 350 - Avg loss in last 1 epochs: - Train: 1.1663 - Val std: 1.0231 - Val orig: 1.0231\n",
      "Epoch 351 - Avg loss in last 1 epochs: - Train: 1.1670 - Val std: 1.0229 - Val orig: 1.0229\n",
      "Epoch 352 - Avg loss in last 1 epochs: - Train: 1.1653 - Val std: 1.0227 - Val orig: 1.0227\n",
      "Epoch 353 - Avg loss in last 1 epochs: - Train: 1.1637 - Val std: 1.0224 - Val orig: 1.0224\n",
      "Epoch 354 - Avg loss in last 1 epochs: - Train: 1.1622 - Val std: 1.0223 - Val orig: 1.0223\n",
      "Epoch 355 - Avg loss in last 1 epochs: - Train: 1.1633 - Val std: 1.0221 - Val orig: 1.0221\n",
      "Epoch 356 - Avg loss in last 1 epochs: - Train: 1.1613 - Val std: 1.0220 - Val orig: 1.0220\n",
      "Epoch 357 - Avg loss in last 1 epochs: - Train: 1.1606 - Val std: 1.0218 - Val orig: 1.0218\n",
      "Epoch 358 - Avg loss in last 1 epochs: - Train: 1.1602 - Val std: 1.0217 - Val orig: 1.0217\n",
      "Epoch 359 - Avg loss in last 1 epochs: - Train: 1.1586 - Val std: 1.0215 - Val orig: 1.0215\n",
      "Epoch 360 - Avg loss in last 1 epochs: - Train: 1.1584 - Val std: 1.0213 - Val orig: 1.0213\n",
      "Epoch 361 - Avg loss in last 1 epochs: - Train: 1.1564 - Val std: 1.0212 - Val orig: 1.0212\n",
      "Epoch 362 - Avg loss in last 1 epochs: - Train: 1.1560 - Val std: 1.0210 - Val orig: 1.0210\n",
      "Epoch 363 - Avg loss in last 1 epochs: - Train: 1.1556 - Val std: 1.0208 - Val orig: 1.0208\n",
      "Epoch 364 - Avg loss in last 1 epochs: - Train: 1.1546 - Val std: 1.0205 - Val orig: 1.0205\n",
      "Epoch 365 - Avg loss in last 1 epochs: - Train: 1.1541 - Val std: 1.0204 - Val orig: 1.0204\n",
      "Epoch 366 - Avg loss in last 1 epochs: - Train: 1.1527 - Val std: 1.0202 - Val orig: 1.0202\n",
      "Epoch 367 - Avg loss in last 1 epochs: - Train: 1.1520 - Val std: 1.0199 - Val orig: 1.0199\n",
      "Epoch 368 - Avg loss in last 1 epochs: - Train: 1.1516 - Val std: 1.0197 - Val orig: 1.0197\n",
      "Epoch 369 - Avg loss in last 1 epochs: - Train: 1.1503 - Val std: 1.0195 - Val orig: 1.0195\n",
      "Epoch 370 - Avg loss in last 1 epochs: - Train: 1.1500 - Val std: 1.0194 - Val orig: 1.0194\n",
      "Epoch 371 - Avg loss in last 1 epochs: - Train: 1.1490 - Val std: 1.0192 - Val orig: 1.0192\n",
      "Epoch 372 - Avg loss in last 1 epochs: - Train: 1.1496 - Val std: 1.0191 - Val orig: 1.0191\n",
      "Epoch 373 - Avg loss in last 1 epochs: - Train: 1.1471 - Val std: 1.0190 - Val orig: 1.0190\n",
      "Epoch 374 - Avg loss in last 1 epochs: - Train: 1.1465 - Val std: 1.0189 - Val orig: 1.0189\n",
      "Epoch 375 - Avg loss in last 1 epochs: - Train: 1.1458 - Val std: 1.0187 - Val orig: 1.0187\n",
      "Epoch 376 - Avg loss in last 1 epochs: - Train: 1.1453 - Val std: 1.0186 - Val orig: 1.0186\n",
      "Epoch 377 - Avg loss in last 1 epochs: - Train: 1.1450 - Val std: 1.0185 - Val orig: 1.0185\n",
      "Epoch 378 - Avg loss in last 1 epochs: - Train: 1.1447 - Val std: 1.0183 - Val orig: 1.0183\n",
      "Epoch 379 - Avg loss in last 1 epochs: - Train: 1.1426 - Val std: 1.0182 - Val orig: 1.0182\n",
      "Epoch 380 - Avg loss in last 1 epochs: - Train: 1.1428 - Val std: 1.0180 - Val orig: 1.0180\n",
      "Epoch 381 - Avg loss in last 1 epochs: - Train: 1.1409 - Val std: 1.0177 - Val orig: 1.0177\n",
      "Epoch 382 - Avg loss in last 1 epochs: - Train: 1.1409 - Val std: 1.0175 - Val orig: 1.0175\n",
      "Epoch 383 - Avg loss in last 1 epochs: - Train: 1.1400 - Val std: 1.0173 - Val orig: 1.0173\n",
      "Epoch 384 - Avg loss in last 1 epochs: - Train: 1.1391 - Val std: 1.0172 - Val orig: 1.0172\n",
      "Epoch 385 - Avg loss in last 1 epochs: - Train: 1.1380 - Val std: 1.0170 - Val orig: 1.0170\n",
      "Epoch 386 - Avg loss in last 1 epochs: - Train: 1.1373 - Val std: 1.0169 - Val orig: 1.0169\n",
      "Epoch 387 - Avg loss in last 1 epochs: - Train: 1.1375 - Val std: 1.0167 - Val orig: 1.0167\n",
      "Epoch 388 - Avg loss in last 1 epochs: - Train: 1.1372 - Val std: 1.0166 - Val orig: 1.0166\n",
      "Epoch 389 - Avg loss in last 1 epochs: - Train: 1.1360 - Val std: 1.0164 - Val orig: 1.0164\n",
      "Epoch 390 - Avg loss in last 1 epochs: - Train: 1.1345 - Val std: 1.0162 - Val orig: 1.0162\n",
      "Epoch 391 - Avg loss in last 1 epochs: - Train: 1.1343 - Val std: 1.0160 - Val orig: 1.0160\n",
      "Epoch 392 - Avg loss in last 1 epochs: - Train: 1.1325 - Val std: 1.0158 - Val orig: 1.0158\n",
      "Epoch 393 - Avg loss in last 1 epochs: - Train: 1.1330 - Val std: 1.0157 - Val orig: 1.0157\n",
      "Epoch 394 - Avg loss in last 1 epochs: - Train: 1.1316 - Val std: 1.0156 - Val orig: 1.0156\n",
      "Epoch 395 - Avg loss in last 1 epochs: - Train: 1.1319 - Val std: 1.0156 - Val orig: 1.0156\n",
      "Epoch 396 - Avg loss in last 1 epochs: - Train: 1.1311 - Val std: 1.0155 - Val orig: 1.0155\n",
      "Epoch 397 - Avg loss in last 1 epochs: - Train: 1.1298 - Val std: 1.0154 - Val orig: 1.0154\n",
      "Epoch 398 - Avg loss in last 1 epochs: - Train: 1.1294 - Val std: 1.0152 - Val orig: 1.0152\n",
      "Epoch 399 - Avg loss in last 1 epochs: - Train: 1.1284 - Val std: 1.0151 - Val orig: 1.0151\n",
      "Epoch 400 - Avg loss in last 1 epochs: - Train: 1.1277 - Val std: 1.0150 - Val orig: 1.0150\n",
      "Epoch 401 - Avg loss in last 1 epochs: - Train: 1.1268 - Val std: 1.0148 - Val orig: 1.0148\n",
      "Epoch 402 - Avg loss in last 1 epochs: - Train: 1.1268 - Val std: 1.0147 - Val orig: 1.0147\n",
      "Epoch 403 - Avg loss in last 1 epochs: - Train: 1.1254 - Val std: 1.0146 - Val orig: 1.0146\n",
      "Epoch 404 - Avg loss in last 1 epochs: - Train: 1.1257 - Val std: 1.0145 - Val orig: 1.0145\n",
      "Epoch 405 - Avg loss in last 1 epochs: - Train: 1.1244 - Val std: 1.0143 - Val orig: 1.0143\n",
      "Epoch 406 - Avg loss in last 1 epochs: - Train: 1.1231 - Val std: 1.0141 - Val orig: 1.0141\n",
      "Epoch 407 - Avg loss in last 1 epochs: - Train: 1.1238 - Val std: 1.0139 - Val orig: 1.0139\n",
      "Epoch 408 - Avg loss in last 1 epochs: - Train: 1.1223 - Val std: 1.0137 - Val orig: 1.0137\n",
      "Epoch 409 - Avg loss in last 1 epochs: - Train: 1.1214 - Val std: 1.0135 - Val orig: 1.0135\n",
      "Epoch 410 - Avg loss in last 1 epochs: - Train: 1.1216 - Val std: 1.0134 - Val orig: 1.0134\n",
      "Epoch 411 - Avg loss in last 1 epochs: - Train: 1.1197 - Val std: 1.0134 - Val orig: 1.0134\n",
      "Epoch 412 - Avg loss in last 1 epochs: - Train: 1.1197 - Val std: 1.0133 - Val orig: 1.0133\n",
      "Epoch 413 - Avg loss in last 1 epochs: - Train: 1.1188 - Val std: 1.0132 - Val orig: 1.0132\n",
      "Epoch 414 - Avg loss in last 1 epochs: - Train: 1.1187 - Val std: 1.0130 - Val orig: 1.0130\n",
      "Epoch 415 - Avg loss in last 1 epochs: - Train: 1.1173 - Val std: 1.0129 - Val orig: 1.0129\n",
      "Epoch 416 - Avg loss in last 1 epochs: - Train: 1.1172 - Val std: 1.0127 - Val orig: 1.0127\n",
      "Epoch 417 - Avg loss in last 1 epochs: - Train: 1.1170 - Val std: 1.0126 - Val orig: 1.0126\n",
      "Epoch 418 - Avg loss in last 1 epochs: - Train: 1.1167 - Val std: 1.0125 - Val orig: 1.0125\n",
      "Epoch 419 - Avg loss in last 1 epochs: - Train: 1.1156 - Val std: 1.0124 - Val orig: 1.0124\n",
      "Epoch 420 - Avg loss in last 1 epochs: - Train: 1.1142 - Val std: 1.0124 - Val orig: 1.0124\n",
      "Epoch 421 - Avg loss in last 1 epochs: - Train: 1.1147 - Val std: 1.0123 - Val orig: 1.0123\n",
      "Epoch 422 - Avg loss in last 1 epochs: - Train: 1.1134 - Val std: 1.0122 - Val orig: 1.0122\n",
      "Epoch 423 - Avg loss in last 1 epochs: - Train: 1.1132 - Val std: 1.0121 - Val orig: 1.0121\n",
      "Epoch 424 - Avg loss in last 1 epochs: - Train: 1.1125 - Val std: 1.0120 - Val orig: 1.0120\n",
      "Epoch 425 - Avg loss in last 1 epochs: - Train: 1.1121 - Val std: 1.0119 - Val orig: 1.0119\n",
      "Epoch 426 - Avg loss in last 1 epochs: - Train: 1.1114 - Val std: 1.0118 - Val orig: 1.0118\n",
      "Epoch 427 - Avg loss in last 1 epochs: - Train: 1.1102 - Val std: 1.0117 - Val orig: 1.0117\n",
      "Epoch 428 - Avg loss in last 1 epochs: - Train: 1.1101 - Val std: 1.0116 - Val orig: 1.0116\n",
      "Epoch 429 - Avg loss in last 1 epochs: - Train: 1.1086 - Val std: 1.0114 - Val orig: 1.0114\n",
      "Epoch 430 - Avg loss in last 1 epochs: - Train: 1.1087 - Val std: 1.0112 - Val orig: 1.0112\n",
      "Epoch 431 - Avg loss in last 1 epochs: - Train: 1.1085 - Val std: 1.0110 - Val orig: 1.0110\n",
      "Epoch 432 - Avg loss in last 1 epochs: - Train: 1.1075 - Val std: 1.0109 - Val orig: 1.0109\n",
      "Epoch 433 - Avg loss in last 1 epochs: - Train: 1.1069 - Val std: 1.0108 - Val orig: 1.0108\n",
      "Epoch 434 - Avg loss in last 1 epochs: - Train: 1.1065 - Val std: 1.0108 - Val orig: 1.0108\n",
      "Epoch 435 - Avg loss in last 1 epochs: - Train: 1.1063 - Val std: 1.0107 - Val orig: 1.0107\n",
      "Epoch 436 - Avg loss in last 1 epochs: - Train: 1.1059 - Val std: 1.0106 - Val orig: 1.0106\n",
      "Epoch 437 - Avg loss in last 1 epochs: - Train: 1.1059 - Val std: 1.0105 - Val orig: 1.0105\n",
      "Epoch 438 - Avg loss in last 1 epochs: - Train: 1.1046 - Val std: 1.0104 - Val orig: 1.0104\n",
      "Epoch 439 - Avg loss in last 1 epochs: - Train: 1.1033 - Val std: 1.0102 - Val orig: 1.0102\n",
      "Epoch 440 - Avg loss in last 1 epochs: - Train: 1.1035 - Val std: 1.0101 - Val orig: 1.0101\n",
      "Epoch 441 - Avg loss in last 1 epochs: - Train: 1.1032 - Val std: 1.0101 - Val orig: 1.0101\n",
      "Epoch 442 - Avg loss in last 1 epochs: - Train: 1.1021 - Val std: 1.0100 - Val orig: 1.0100\n",
      "Epoch 443 - Avg loss in last 1 epochs: - Train: 1.1013 - Val std: 1.0100 - Val orig: 1.0100\n",
      "Epoch 444 - Avg loss in last 1 epochs: - Train: 1.1010 - Val std: 1.0098 - Val orig: 1.0098\n",
      "Epoch 445 - Avg loss in last 1 epochs: - Train: 1.1007 - Val std: 1.0097 - Val orig: 1.0097\n",
      "Epoch 446 - Avg loss in last 1 epochs: - Train: 1.1001 - Val std: 1.0095 - Val orig: 1.0095\n",
      "Epoch 447 - Avg loss in last 1 epochs: - Train: 1.0998 - Val std: 1.0094 - Val orig: 1.0094\n",
      "Epoch 448 - Avg loss in last 1 epochs: - Train: 1.0991 - Val std: 1.0093 - Val orig: 1.0093\n",
      "Epoch 449 - Avg loss in last 1 epochs: - Train: 1.0986 - Val std: 1.0092 - Val orig: 1.0092\n",
      "Epoch 450 - Avg loss in last 1 epochs: - Train: 1.0983 - Val std: 1.0091 - Val orig: 1.0091\n",
      "Epoch 451 - Avg loss in last 1 epochs: - Train: 1.0979 - Val std: 1.0090 - Val orig: 1.0090\n",
      "Epoch 452 - Avg loss in last 1 epochs: - Train: 1.0975 - Val std: 1.0090 - Val orig: 1.0090\n",
      "Epoch 453 - Avg loss in last 1 epochs: - Train: 1.0972 - Val std: 1.0089 - Val orig: 1.0089\n",
      "Epoch 454 - Avg loss in last 1 epochs: - Train: 1.0955 - Val std: 1.0088 - Val orig: 1.0088\n",
      "Epoch 455 - Avg loss in last 1 epochs: - Train: 1.0943 - Val std: 1.0087 - Val orig: 1.0087\n",
      "Epoch 456 - Avg loss in last 1 epochs: - Train: 1.0947 - Val std: 1.0086 - Val orig: 1.0086\n",
      "Epoch 457 - Avg loss in last 1 epochs: - Train: 1.0950 - Val std: 1.0085 - Val orig: 1.0085\n",
      "Epoch 458 - Avg loss in last 1 epochs: - Train: 1.0937 - Val std: 1.0084 - Val orig: 1.0084\n",
      "Epoch 459 - Avg loss in last 1 epochs: - Train: 1.0936 - Val std: 1.0084 - Val orig: 1.0084\n",
      "Epoch 460 - Avg loss in last 1 epochs: - Train: 1.0926 - Val std: 1.0083 - Val orig: 1.0083\n",
      "Epoch 461 - Avg loss in last 1 epochs: - Train: 1.0930 - Val std: 1.0082 - Val orig: 1.0082\n",
      "Epoch 462 - Avg loss in last 1 epochs: - Train: 1.0920 - Val std: 1.0081 - Val orig: 1.0081\n",
      "Epoch 463 - Avg loss in last 1 epochs: - Train: 1.0909 - Val std: 1.0080 - Val orig: 1.0080\n",
      "Epoch 464 - Avg loss in last 1 epochs: - Train: 1.0914 - Val std: 1.0079 - Val orig: 1.0079\n",
      "Epoch 465 - Avg loss in last 1 epochs: - Train: 1.0910 - Val std: 1.0078 - Val orig: 1.0078\n",
      "Epoch 466 - Avg loss in last 1 epochs: - Train: 1.0903 - Val std: 1.0077 - Val orig: 1.0077\n",
      "Epoch 467 - Avg loss in last 1 epochs: - Train: 1.0893 - Val std: 1.0077 - Val orig: 1.0077\n",
      "Epoch 468 - Avg loss in last 1 epochs: - Train: 1.0894 - Val std: 1.0076 - Val orig: 1.0076\n",
      "Epoch 469 - Avg loss in last 1 epochs: - Train: 1.0895 - Val std: 1.0075 - Val orig: 1.0075\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 5\u001b[0m\n\u001b[1;32m      3\u001b[0m loss_fn \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mMSELoss()\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mK=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mK\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, L=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mL\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, C=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mC\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m----> 5\u001b[0m train_rmse, val_rmse_std, val_rmse_orig \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mloss_fn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_users\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_items\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_ratings\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_users\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_items\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_ratings\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_ratings\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmeans\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mEPOCHS\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mSTOP_THRESHOLD\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbosity\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      6\u001b[0m report_training_results(train_rmse, val_rmse_std, val_rmse_orig)\n",
      "File \u001b[0;32m~/Desktop/CIL-project/movie_rating_prediction/src/train.py:118\u001b[0m, in \u001b[0;36mtrain_model\u001b[0;34m(model, optimizer, loss_fn, train_users, train_items, standardized_train_ratings, val_users, val_items, orig_val_ratings, standardized_val_ratings, means, stds, n_epochs, stop_threshold, save_best_model, verbosity)\u001b[0m\n\u001b[1;32m    116\u001b[0m val_losses_orig \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m    117\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(n_epochs):\n\u001b[0;32m--> 118\u001b[0m     train_loss \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_one_epoch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mloss_fn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_users\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_items\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstandardized_train_ratings\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    119\u001b[0m     val_loss_standardized \u001b[38;5;241m=\u001b[39m evaluate_one_epoch(model, loss_fn, val_users, val_items, standardized_val_ratings)\n\u001b[1;32m    120\u001b[0m     val_loss_original \u001b[38;5;241m=\u001b[39m evaluate_one_epoch_original(model, loss_fn, val_users, val_items, orig_val_ratings, means, stds)\n",
      "File \u001b[0;32m~/Desktop/CIL-project/movie_rating_prediction/src/train.py:37\u001b[0m, in \u001b[0;36mtrain_one_epoch\u001b[0;34m(model, optimizer, loss_fn, users, items, ratings)\u001b[0m\n\u001b[1;32m     35\u001b[0m J\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[1;32m     36\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[0;32m---> 37\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msqrt\u001b[49m\u001b[43m(\u001b[49m\u001b[43mJ\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model = LightGCN(norm_adj_r, ACT_FN, K, L, INIT_EMBS_STD, DROPOUT, C).to(DEVICE)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=LR, weight_decay=WEIGHT_DECAY)\n",
    "loss_fn = nn.MSELoss()\n",
    "print(f\"K={K}, L={L}, C={C}\")\n",
    "train_rmse, val_rmse_std, val_rmse_orig = train_model(model, optimizer, loss_fn, train_users, train_items, train_ratings, val_users, val_items, val_ratings, val_ratings, means, stds, EPOCHS, STOP_THRESHOLD, False, verbosity=1)\n",
    "report_training_results(train_rmse, val_rmse_std, val_rmse_orig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
