{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LightGCNPlus\n",
    "\n",
    "### Scope\n",
    "In this notebook we train different variants of our LightGCNPlus model on different hyperparameters. We then re-train the best performing model and evaluate it on Kaggle's public test set.\n",
    "\n",
    "### About the model\n",
    "LightGCNPlus is a graph-based collaborative filtering model that extends the [original LightGCN model](https://arxiv.org/pdf/2002.02126.pdf). The original LightGCN is only able to rank items. We thus refine the model to be able to predict ratings. Our LightGCNPlus model is composed of the following components:\n",
    "- message passing: $e_u^{(l)} = \\sum_{v \\in N(u)} \\frac{sr_{u,v}}{\\sqrt{|N(u)||N(v)|}} e_v^{(l-1)}$, where $sr_{u,v}$ is the standardized rating of user $u$ for item $v$.\n",
    "- aggregation mechanism: $h_u = \\text{concat}(e_u^{(0)}, e_u^{(1)}, ..., e_u^{(L)})$, where $L$ is the number of layers in the message passing mechanism.\n",
    "- output layer: $\\hat{R}_{(i,j)} = \\text{MLP}(\\text{concat}(h_i, h_j))$, where $\\text{MLP}$ is a multi-layer perceptron that projects the embedding couples of observed ratings to the output space.\n",
    "\n",
    "How our model differs from the original LightGCN:\n",
    "- In the message passing layer we use the standardized ratings instead of the raw interactions (binary in the original case).\n",
    "- We use an MLP in the output layer instead of a dot product. This inspired by the [neural collaborative filtering paper](https://arxiv.org/pdf/1708.05031.pdf), in order to predict ratings instead of just ranking unobserved interactions.\n",
    "\n",
    "### Hyperparameter tuning\n",
    "The hyperparameters comprise both choices about the models' architecture and the training procedure.\n",
    "We perform a grid search over the following hyperparameters:\n",
    "- K: Embedding size.\n",
    "- L: Number of layers in the message passing mechanism.\n",
    "- PROJECTIONS: Architecture of the MLP that projects the embedding couples of observed ratings to the output space.\n",
    "\n",
    "Other hyperparameters such as the learning rate, the batch size, the number of epochs, the optimizer, etc. have been selected based on the results on previous experiments. Thus, to reduce the computational cost of the grid search, the following hyperparameters are kept constant:\n",
    "- LR: learning rate.\n",
    "- INIT_EMBS_STD: standard deviation of the normal distribution used to initialize the embeddings.\n",
    "- WEIGHT_DECAY: weight decay coefficient for the Adam optimizer.\n",
    "- DROPOUT: dropout rate used in the MLP's hidden layers.\n",
    "- ACT_FN: activation function used in the MLP's hidden layers.\n",
    "\n",
    "We also define hyperparameters for the training procedure:\n",
    "- EPOCHS: number of backpropagation steps.\n",
    "- STOP_THRESHOLD: minimum improvement in the validation loss to continue training.\n",
    "\n",
    "### Training\n",
    "- loss functions:\n",
    "    - train: $\\text{MSE} = \\frac{1}{|\\Omega|} \\sum_{(i,j) \\in \\Omega} (M_{(i,j)} - \\hat{M}_{(i,j)})^2$\n",
    "    - eval: $\\text{RMSE} = \\sqrt{\\text{MSE}}$\n",
    "- optimizer: Adam\n",
    "- Batching: no mini-batching of the training data, doesn't improve performance and slows convergence down. \n",
    "- Regularization: dropout and L2 regularization (weight decay in the Adam optimizer)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "from train import train_model\n",
    "from models import LightGCNPlus\n",
    "from config import DEVICE\n",
    "from train import train_model\n",
    "from postprocess import report_training_results\n",
    "from itertools import product"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load and Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from load import load_train_data\n",
    "train_df = load_train_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from preprocess import preprocess\n",
    "\n",
    "A_tilde, \\\n",
    "standardized_train_ratings, \\\n",
    "train_users, train_items, \\\n",
    "means, stds, \\\n",
    "val_users, \\\n",
    "val_items, \\\n",
    "orig_val_ratings, \\\n",
    "standardized_val_ratings \\\n",
    "= preprocess(train_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Train and Report\n",
    "\n",
    "In the ideal case one would search over multiple splits of the train and val sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model and optimizer hyperparameters\n",
    "INIT_EMBS_STD=0.075\n",
    "LR=0.1\n",
    "WEIGHT_DECAY=0.00005\n",
    "DROPOUT=0.5\n",
    "ACT_FN = nn.GELU()\n",
    "\n",
    "# Train loop hyperparameters\n",
    "EPOCHS = 4000\n",
    "STOP_THRESHOLD=1e-09\n",
    "\n",
    "# To be searched (example values)\n",
    "# K=28\n",
    "# L=4\n",
    "# PROJECTIONS = (4,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Searching over 60 combinations.\n"
     ]
    }
   ],
   "source": [
    "# Grid\n",
    "ks = [28, 30, 32]  # different Ks work with different projections\n",
    "layers = [4, 5]  # tested 3 already\n",
    "projections = [(6,1), (6,), (5, 1), (5,), (4, 1), (4,), (3, 1), (3,), (2, 1), (2,)]\n",
    "\n",
    "num_combinations = len(ks) * len(layers) * len(projections)\n",
    "print(f\"Searching over {num_combinations} combinations.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "K=28, L=4, C=(6, 1)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 13\u001b[0m\n\u001b[1;32m     11\u001b[0m loss_fn \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mMSELoss()\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mK=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mK\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, L=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mL\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, C=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mC\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 13\u001b[0m train_rmse, val_rmse_std, val_rmse_orig \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mloss_fn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_users\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_items\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstandardized_train_ratings\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_users\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_items\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43morig_val_ratings\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstandardized_val_ratings\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmeans\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mEPOCHS\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mSTOP_THRESHOLD\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbosity\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m400\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     14\u001b[0m report_training_results(train_rmse, val_rmse_std, val_rmse_orig)\n\u001b[1;32m     16\u001b[0m results[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmin_val_losses\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;28mmin\u001b[39m(val_rmse_orig))\n",
      "File \u001b[0;32m~/Desktop/CIL-project/movie_rating_prediction/src/train.py:116\u001b[0m, in \u001b[0;36mtrain_model\u001b[0;34m(model, optimizer, loss_fn, train_users, train_items, standardized_train_ratings, val_users, val_items, orig_val_ratings, standardized_val_ratings, means, stds, n_epochs, stop_threshold, save_best_model, verbosity)\u001b[0m\n\u001b[1;32m    114\u001b[0m val_losses_orig \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m    115\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(n_epochs):\n\u001b[0;32m--> 116\u001b[0m     train_loss \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_one_epoch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mloss_fn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_users\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_items\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstandardized_train_ratings\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    117\u001b[0m     val_loss_standardized \u001b[38;5;241m=\u001b[39m evaluate_one_epoch(model, loss_fn, val_users, val_items, standardized_val_ratings)\n\u001b[1;32m    118\u001b[0m     val_loss_original \u001b[38;5;241m=\u001b[39m evaluate_one_epoch_original(model, loss_fn, val_users, val_items, orig_val_ratings, means, stds)\n",
      "File \u001b[0;32m~/Desktop/CIL-project/movie_rating_prediction/src/train.py:37\u001b[0m, in \u001b[0;36mtrain_one_epoch\u001b[0;34m(model, optimizer, loss_fn, users, items, ratings)\u001b[0m\n\u001b[1;32m     35\u001b[0m J\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[1;32m     36\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[0;32m---> 37\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msqrt\u001b[49m\u001b[43m(\u001b[49m\u001b[43mJ\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Tuning LightGCNPlus\n",
    "results = {\n",
    "    \"min_val_losses\": [],\n",
    "    \"params\": []\n",
    "}\n",
    "for K in ks:\n",
    "    for L in layers:\n",
    "        for C in projections:\n",
    "            model = LightGCNPlus(A_tilde, ACT_FN, K, L, INIT_EMBS_STD, DROPOUT, C).to(DEVICE)\n",
    "            optimizer = torch.optim.Adam(model.parameters(), lr=LR, weight_decay=WEIGHT_DECAY)\n",
    "            loss_fn = nn.MSELoss()\n",
    "            print(f\"K={K}, L={L}, C={C}\")\n",
    "            train_rmse, val_rmse_std, val_rmse_orig = train_model(model, optimizer, loss_fn, train_users, train_items, standardized_train_ratings, val_users, val_items, orig_val_ratings, standardized_val_ratings, means, stds, EPOCHS, STOP_THRESHOLD, True, verbosity=400)\n",
    "            report_training_results(train_rmse, val_rmse_std, val_rmse_orig)\n",
    "            \n",
    "            results[\"min_val_losses\"].append(min(val_rmse_orig))\n",
    "            results[\"params\"].append((K, L, C))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best hyperparameters:\n",
      "(28, 4, (2,))\n",
      "Best val loss:\n",
      "0.9894648790359497\n",
      "Best hyperparameters:\n",
      "(32, 4, (2,))\n",
      "Best val loss:\n",
      "0.9895111918449402\n",
      "Best hyperparameters:\n",
      "(30, 4, (1,))\n",
      "Best val loss:\n",
      "0.9902642369270325\n"
     ]
    }
   ],
   "source": [
    "# Report top k best hyperparameter combos\n",
    "TOP_K = 5\n",
    "best_ids = np.argsort(results[\"min_val_losses\"])[:TOP_K]\n",
    "\n",
    "for i in best_ids:\n",
    "    print(f\"Best hyperparameters: {results[\"params\"][i]}\")\n",
    "    print(f\"Best val loss: {results[\"min_val_losses\"][i]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Retrain model on best hyperparam combo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup for final training\n",
    "EPOCHS = 4000\n",
    "K, L, C = (28, 4, (5,))\n",
    "\n",
    "model = LightGCNPlus(A_tilde, ACT_FN, K, L, INIT_EMBS_STD, DROPOUT, C).to(DEVICE)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=LR, weight_decay=WEIGHT_DECAY)\n",
    "loss_fn = nn.MSELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 - Avg loss in last 1 epochs: - Train: 1.1189 - Val std: 1.0876 - Val orig: 1.1144\n",
      "Epoch 2 - Avg loss in last 1 epochs: - Train: 1.0971 - Val std: 7.1819 - Val orig: 7.3322\n",
      "Epoch 3 - Avg loss in last 1 epochs: - Train: 7.2486 - Val std: 1.2170 - Val orig: 1.2548\n",
      "Epoch 4 - Avg loss in last 1 epochs: - Train: 1.2367 - Val std: 1.4900 - Val orig: 1.5322\n",
      "Epoch 5 - Avg loss in last 1 epochs: - Train: 1.5015 - Val std: 1.0824 - Val orig: 1.1117\n",
      "Epoch 6 - Avg loss in last 1 epochs: - Train: 1.1099 - Val std: 2.7034 - Val orig: 2.7386\n",
      "Epoch 7 - Avg loss in last 1 epochs: - Train: 2.8037 - Val std: 3.2317 - Val orig: 3.2804\n",
      "Epoch 8 - Avg loss in last 1 epochs: - Train: 3.3896 - Val std: 1.0366 - Val orig: 1.0684\n",
      "Epoch 9 - Avg loss in last 1 epochs: - Train: 1.2655 - Val std: 2.3781 - Val orig: 2.4338\n",
      "Epoch 10 - Avg loss in last 1 epochs: - Train: 2.4624 - Val std: 1.3319 - Val orig: 1.3709\n",
      "Epoch 11 - Avg loss in last 1 epochs: - Train: 1.3919 - Val std: 1.1725 - Val orig: 1.2033\n",
      "Epoch 12 - Avg loss in last 1 epochs: - Train: 1.2064 - Val std: 1.4768 - Val orig: 1.5185\n",
      "Epoch 13 - Avg loss in last 1 epochs: - Train: 1.4980 - Val std: 1.1980 - Val orig: 1.2364\n",
      "Epoch 14 - Avg loss in last 1 epochs: - Train: 1.2193 - Val std: 1.0408 - Val orig: 1.0712\n",
      "Epoch 15 - Avg loss in last 1 epochs: - Train: 1.0709 - Val std: 1.1009 - Val orig: 1.1340\n",
      "Epoch 16 - Avg loss in last 1 epochs: - Train: 1.1340 - Val std: 1.1171 - Val orig: 1.1542\n",
      "Epoch 17 - Avg loss in last 1 epochs: - Train: 1.1518 - Val std: 1.0765 - Val orig: 1.1127\n",
      "Epoch 18 - Avg loss in last 1 epochs: - Train: 1.1107 - Val std: 1.0411 - Val orig: 1.0726\n",
      "Epoch 19 - Avg loss in last 1 epochs: - Train: 1.0728 - Val std: 1.0316 - Val orig: 1.0594\n",
      "Epoch 20 - Avg loss in last 1 epochs: - Train: 1.0580 - Val std: 1.0156 - Val orig: 1.0437\n",
      "Epoch 21 - Avg loss in last 1 epochs: - Train: 1.0381 - Val std: 1.0593 - Val orig: 1.0855\n",
      "Epoch 22 - Avg loss in last 1 epochs: - Train: 1.0967 - Val std: 1.0253 - Val orig: 1.0540\n",
      "Epoch 23 - Avg loss in last 1 epochs: - Train: 1.0545 - Val std: 1.0608 - Val orig: 1.0904\n",
      "Epoch 24 - Avg loss in last 1 epochs: - Train: 1.0819 - Val std: 1.0195 - Val orig: 1.0495\n",
      "Epoch 25 - Avg loss in last 1 epochs: - Train: 1.0426 - Val std: 1.0036 - Val orig: 1.0309\n",
      "Epoch 26 - Avg loss in last 1 epochs: - Train: 1.0517 - Val std: 0.9817 - Val orig: 1.0100\n",
      "Epoch 27 - Avg loss in last 1 epochs: - Train: 1.0177 - Val std: 1.0086 - Val orig: 1.0376\n",
      "Epoch 28 - Avg loss in last 1 epochs: - Train: 1.0279 - Val std: 0.9943 - Val orig: 1.0224\n",
      "Epoch 29 - Avg loss in last 1 epochs: - Train: 1.0124 - Val std: 0.9755 - Val orig: 1.0038\n",
      "Epoch 30 - Avg loss in last 1 epochs: - Train: 1.0019 - Val std: 0.9825 - Val orig: 1.0115\n",
      "Epoch 31 - Avg loss in last 1 epochs: - Train: 1.0090 - Val std: 0.9764 - Val orig: 1.0040\n",
      "Epoch 32 - Avg loss in last 1 epochs: - Train: 0.9935 - Val std: 0.9875 - Val orig: 1.0144\n",
      "Epoch 33 - Avg loss in last 1 epochs: - Train: 0.9997 - Val std: 0.9874 - Val orig: 1.0143\n",
      "Epoch 34 - Avg loss in last 1 epochs: - Train: 0.9986 - Val std: 0.9772 - Val orig: 1.0045\n",
      "Epoch 35 - Avg loss in last 1 epochs: - Train: 0.9896 - Val std: 0.9750 - Val orig: 1.0032\n",
      "Epoch 36 - Avg loss in last 1 epochs: - Train: 0.9894 - Val std: 0.9759 - Val orig: 1.0045\n",
      "Epoch 37 - Avg loss in last 1 epochs: - Train: 0.9903 - Val std: 0.9722 - Val orig: 1.0005\n",
      "Epoch 38 - Avg loss in last 1 epochs: - Train: 0.9840 - Val std: 0.9727 - Val orig: 1.0009\n",
      "Epoch 39 - Avg loss in last 1 epochs: - Train: 0.9822 - Val std: 0.9753 - Val orig: 1.0036\n",
      "Epoch 40 - Avg loss in last 1 epochs: - Train: 0.9840 - Val std: 0.9745 - Val orig: 1.0030\n",
      "Epoch 41 - Avg loss in last 1 epochs: - Train: 0.9826 - Val std: 0.9717 - Val orig: 1.0002\n",
      "Epoch 42 - Avg loss in last 1 epochs: - Train: 0.9791 - Val std: 0.9701 - Val orig: 0.9988\n",
      "Epoch 43 - Avg loss in last 1 epochs: - Train: 0.9778 - Val std: 0.9703 - Val orig: 0.9990\n",
      "Epoch 44 - Avg loss in last 1 epochs: - Train: 0.9776 - Val std: 0.9702 - Val orig: 0.9990\n",
      "Epoch 45 - Avg loss in last 1 epochs: - Train: 0.9772 - Val std: 0.9694 - Val orig: 0.9982\n",
      "Epoch 46 - Avg loss in last 1 epochs: - Train: 0.9757 - Val std: 0.9690 - Val orig: 0.9977\n",
      "Epoch 47 - Avg loss in last 1 epochs: - Train: 0.9746 - Val std: 0.9696 - Val orig: 0.9983\n",
      "Epoch 48 - Avg loss in last 1 epochs: - Train: 0.9746 - Val std: 0.9701 - Val orig: 0.9987\n",
      "Epoch 49 - Avg loss in last 1 epochs: - Train: 0.9747 - Val std: 0.9694 - Val orig: 0.9981\n",
      "Epoch 50 - Avg loss in last 1 epochs: - Train: 0.9736 - Val std: 0.9684 - Val orig: 0.9971\n",
      "Epoch 51 - Avg loss in last 1 epochs: - Train: 0.9723 - Val std: 0.9678 - Val orig: 0.9967\n",
      "Epoch 52 - Avg loss in last 1 epochs: - Train: 0.9716 - Val std: 0.9678 - Val orig: 0.9967\n",
      "Epoch 53 - Avg loss in last 1 epochs: - Train: 0.9714 - Val std: 0.9677 - Val orig: 0.9966\n",
      "Epoch 54 - Avg loss in last 1 epochs: - Train: 0.9711 - Val std: 0.9672 - Val orig: 0.9961\n",
      "Epoch 55 - Avg loss in last 1 epochs: - Train: 0.9701 - Val std: 0.9667 - Val orig: 0.9955\n",
      "Epoch 56 - Avg loss in last 1 epochs: - Train: 0.9697 - Val std: 0.9665 - Val orig: 0.9952\n",
      "Epoch 57 - Avg loss in last 1 epochs: - Train: 0.9693 - Val std: 0.9666 - Val orig: 0.9952\n",
      "Epoch 58 - Avg loss in last 1 epochs: - Train: 0.9688 - Val std: 0.9664 - Val orig: 0.9951\n",
      "Epoch 59 - Avg loss in last 1 epochs: - Train: 0.9687 - Val std: 0.9660 - Val orig: 0.9946\n",
      "Epoch 60 - Avg loss in last 1 epochs: - Train: 0.9681 - Val std: 0.9656 - Val orig: 0.9942\n",
      "Epoch 61 - Avg loss in last 1 epochs: - Train: 0.9677 - Val std: 0.9655 - Val orig: 0.9941\n",
      "Epoch 62 - Avg loss in last 1 epochs: - Train: 0.9670 - Val std: 0.9655 - Val orig: 0.9942\n",
      "Epoch 63 - Avg loss in last 1 epochs: - Train: 0.9674 - Val std: 0.9654 - Val orig: 0.9941\n",
      "Epoch 64 - Avg loss in last 1 epochs: - Train: 0.9670 - Val std: 0.9653 - Val orig: 0.9939\n",
      "Epoch 65 - Avg loss in last 1 epochs: - Train: 0.9666 - Val std: 0.9653 - Val orig: 0.9939\n",
      "Epoch 66 - Avg loss in last 1 epochs: - Train: 0.9663 - Val std: 0.9654 - Val orig: 0.9940\n",
      "Epoch 67 - Avg loss in last 1 epochs: - Train: 0.9658 - Val std: 0.9654 - Val orig: 0.9940\n",
      "Epoch 68 - Avg loss in last 1 epochs: - Train: 0.9658 - Val std: 0.9653 - Val orig: 0.9938\n",
      "Epoch 69 - Avg loss in last 1 epochs: - Train: 0.9657 - Val std: 0.9651 - Val orig: 0.9937\n",
      "Epoch 70 - Avg loss in last 1 epochs: - Train: 0.9655 - Val std: 0.9651 - Val orig: 0.9937\n",
      "Epoch 71 - Avg loss in last 1 epochs: - Train: 0.9651 - Val std: 0.9651 - Val orig: 0.9937\n",
      "Epoch 72 - Avg loss in last 1 epochs: - Train: 0.9650 - Val std: 0.9651 - Val orig: 0.9937\n",
      "Epoch 73 - Avg loss in last 1 epochs: - Train: 0.9651 - Val std: 0.9651 - Val orig: 0.9937\n",
      "Epoch 74 - Avg loss in last 1 epochs: - Train: 0.9649 - Val std: 0.9651 - Val orig: 0.9937\n",
      "Epoch 75 - Avg loss in last 1 epochs: - Train: 0.9646 - Val std: 0.9651 - Val orig: 0.9937\n",
      "Epoch 76 - Avg loss in last 1 epochs: - Train: 0.9646 - Val std: 0.9651 - Val orig: 0.9937\n",
      "Epoch 77 - Avg loss in last 1 epochs: - Train: 0.9643 - Val std: 0.9651 - Val orig: 0.9937\n",
      "Epoch 78 - Avg loss in last 1 epochs: - Train: 0.9642 - Val std: 0.9650 - Val orig: 0.9936\n",
      "Epoch 79 - Avg loss in last 1 epochs: - Train: 0.9640 - Val std: 0.9649 - Val orig: 0.9936\n",
      "Epoch 80 - Avg loss in last 1 epochs: - Train: 0.9639 - Val std: 0.9648 - Val orig: 0.9935\n",
      "Epoch 81 - Avg loss in last 1 epochs: - Train: 0.9638 - Val std: 0.9647 - Val orig: 0.9934\n",
      "Epoch 82 - Avg loss in last 1 epochs: - Train: 0.9635 - Val std: 0.9646 - Val orig: 0.9932\n",
      "Epoch 83 - Avg loss in last 1 epochs: - Train: 0.9635 - Val std: 0.9645 - Val orig: 0.9931\n",
      "Epoch 84 - Avg loss in last 1 epochs: - Train: 0.9632 - Val std: 0.9643 - Val orig: 0.9930\n",
      "Epoch 85 - Avg loss in last 1 epochs: - Train: 0.9633 - Val std: 0.9642 - Val orig: 0.9928\n",
      "Epoch 86 - Avg loss in last 1 epochs: - Train: 0.9630 - Val std: 0.9640 - Val orig: 0.9926\n",
      "Epoch 87 - Avg loss in last 1 epochs: - Train: 0.9629 - Val std: 0.9638 - Val orig: 0.9924\n",
      "Epoch 88 - Avg loss in last 1 epochs: - Train: 0.9628 - Val std: 0.9637 - Val orig: 0.9923\n",
      "Epoch 89 - Avg loss in last 1 epochs: - Train: 0.9626 - Val std: 0.9636 - Val orig: 0.9921\n",
      "Epoch 90 - Avg loss in last 1 epochs: - Train: 0.9624 - Val std: 0.9634 - Val orig: 0.9920\n",
      "Epoch 91 - Avg loss in last 1 epochs: - Train: 0.9623 - Val std: 0.9633 - Val orig: 0.9918\n",
      "Epoch 92 - Avg loss in last 1 epochs: - Train: 0.9621 - Val std: 0.9633 - Val orig: 0.9918\n",
      "Epoch 93 - Avg loss in last 1 epochs: - Train: 0.9620 - Val std: 0.9632 - Val orig: 0.9918\n",
      "Epoch 94 - Avg loss in last 1 epochs: - Train: 0.9619 - Val std: 0.9632 - Val orig: 0.9918\n",
      "Epoch 95 - Avg loss in last 1 epochs: - Train: 0.9617 - Val std: 0.9632 - Val orig: 0.9917\n",
      "Epoch 96 - Avg loss in last 1 epochs: - Train: 0.9617 - Val std: 0.9632 - Val orig: 0.9918\n",
      "Epoch 97 - Avg loss in last 1 epochs: - Train: 0.9612 - Val std: 0.9632 - Val orig: 0.9918\n",
      "Epoch 98 - Avg loss in last 1 epochs: - Train: 0.9612 - Val std: 0.9632 - Val orig: 0.9918\n",
      "Epoch 99 - Avg loss in last 1 epochs: - Train: 0.9612 - Val std: 0.9632 - Val orig: 0.9919\n",
      "Epoch 100 - Avg loss in last 1 epochs: - Train: 0.9611 - Val std: 0.9632 - Val orig: 0.9919\n",
      "Epoch 101 - Avg loss in last 1 epochs: - Train: 0.9608 - Val std: 0.9632 - Val orig: 0.9919\n",
      "Epoch 102 - Avg loss in last 1 epochs: - Train: 0.9608 - Val std: 0.9632 - Val orig: 0.9919\n",
      "Epoch 103 - Avg loss in last 1 epochs: - Train: 0.9608 - Val std: 0.9632 - Val orig: 0.9919\n",
      "Epoch 104 - Avg loss in last 1 epochs: - Train: 0.9606 - Val std: 0.9632 - Val orig: 0.9920\n",
      "Epoch 105 - Avg loss in last 1 epochs: - Train: 0.9606 - Val std: 0.9633 - Val orig: 0.9920\n",
      "Epoch 106 - Avg loss in last 1 epochs: - Train: 0.9603 - Val std: 0.9633 - Val orig: 0.9921\n",
      "Epoch 107 - Avg loss in last 1 epochs: - Train: 0.9605 - Val std: 0.9634 - Val orig: 0.9922\n",
      "Epoch 108 - Avg loss in last 1 epochs: - Train: 0.9602 - Val std: 0.9635 - Val orig: 0.9922\n",
      "Epoch 109 - Avg loss in last 1 epochs: - Train: 0.9601 - Val std: 0.9635 - Val orig: 0.9922\n",
      "Epoch 110 - Avg loss in last 1 epochs: - Train: 0.9602 - Val std: 0.9635 - Val orig: 0.9923\n",
      "Epoch 111 - Avg loss in last 1 epochs: - Train: 0.9600 - Val std: 0.9635 - Val orig: 0.9923\n",
      "Epoch 112 - Avg loss in last 1 epochs: - Train: 0.9598 - Val std: 0.9635 - Val orig: 0.9923\n",
      "Epoch 113 - Avg loss in last 1 epochs: - Train: 0.9597 - Val std: 0.9635 - Val orig: 0.9923\n",
      "Epoch 114 - Avg loss in last 1 epochs: - Train: 0.9598 - Val std: 0.9635 - Val orig: 0.9923\n",
      "Epoch 115 - Avg loss in last 1 epochs: - Train: 0.9598 - Val std: 0.9635 - Val orig: 0.9923\n",
      "Epoch 116 - Avg loss in last 1 epochs: - Train: 0.9593 - Val std: 0.9635 - Val orig: 0.9923\n",
      "Epoch 117 - Avg loss in last 1 epochs: - Train: 0.9594 - Val std: 0.9635 - Val orig: 0.9923\n",
      "Epoch 118 - Avg loss in last 1 epochs: - Train: 0.9595 - Val std: 0.9635 - Val orig: 0.9923\n",
      "Epoch 119 - Avg loss in last 1 epochs: - Train: 0.9591 - Val std: 0.9635 - Val orig: 0.9922\n",
      "Epoch 120 - Avg loss in last 1 epochs: - Train: 0.9593 - Val std: 0.9635 - Val orig: 0.9923\n",
      "Epoch 121 - Avg loss in last 1 epochs: - Train: 0.9593 - Val std: 0.9635 - Val orig: 0.9923\n",
      "Epoch 122 - Avg loss in last 1 epochs: - Train: 0.9593 - Val std: 0.9636 - Val orig: 0.9924\n",
      "Epoch 123 - Avg loss in last 1 epochs: - Train: 0.9590 - Val std: 0.9636 - Val orig: 0.9925\n",
      "Epoch 124 - Avg loss in last 1 epochs: - Train: 0.9589 - Val std: 0.9637 - Val orig: 0.9925\n",
      "Epoch 125 - Avg loss in last 1 epochs: - Train: 0.9588 - Val std: 0.9636 - Val orig: 0.9925\n",
      "Epoch 126 - Avg loss in last 1 epochs: - Train: 0.9588 - Val std: 0.9636 - Val orig: 0.9924\n",
      "Epoch 127 - Avg loss in last 1 epochs: - Train: 0.9585 - Val std: 0.9635 - Val orig: 0.9924\n",
      "Epoch 128 - Avg loss in last 1 epochs: - Train: 0.9585 - Val std: 0.9634 - Val orig: 0.9923\n",
      "Epoch 129 - Avg loss in last 1 epochs: - Train: 0.9589 - Val std: 0.9633 - Val orig: 0.9923\n",
      "Epoch 130 - Avg loss in last 1 epochs: - Train: 0.9587 - Val std: 0.9633 - Val orig: 0.9922\n",
      "Epoch 131 - Avg loss in last 1 epochs: - Train: 0.9586 - Val std: 0.9632 - Val orig: 0.9921\n",
      "Epoch 132 - Avg loss in last 1 epochs: - Train: 0.9584 - Val std: 0.9632 - Val orig: 0.9921\n",
      "Epoch 133 - Avg loss in last 1 epochs: - Train: 0.9583 - Val std: 0.9632 - Val orig: 0.9921\n",
      "Epoch 134 - Avg loss in last 1 epochs: - Train: 0.9582 - Val std: 0.9632 - Val orig: 0.9921\n",
      "Epoch 135 - Avg loss in last 1 epochs: - Train: 0.9581 - Val std: 0.9632 - Val orig: 0.9922\n",
      "Epoch 136 - Avg loss in last 1 epochs: - Train: 0.9583 - Val std: 0.9633 - Val orig: 0.9923\n",
      "Epoch 137 - Avg loss in last 1 epochs: - Train: 0.9582 - Val std: 0.9633 - Val orig: 0.9923\n",
      "Epoch 138 - Avg loss in last 1 epochs: - Train: 0.9580 - Val std: 0.9633 - Val orig: 0.9923\n",
      "Epoch 139 - Avg loss in last 1 epochs: - Train: 0.9582 - Val std: 0.9633 - Val orig: 0.9923\n",
      "Epoch 140 - Avg loss in last 1 epochs: - Train: 0.9580 - Val std: 0.9633 - Val orig: 0.9923\n",
      "Epoch 141 - Avg loss in last 1 epochs: - Train: 0.9580 - Val std: 0.9633 - Val orig: 0.9923\n",
      "Epoch 142 - Avg loss in last 1 epochs: - Train: 0.9580 - Val std: 0.9633 - Val orig: 0.9923\n",
      "Epoch 143 - Avg loss in last 1 epochs: - Train: 0.9579 - Val std: 0.9633 - Val orig: 0.9923\n",
      "Epoch 144 - Avg loss in last 1 epochs: - Train: 0.9577 - Val std: 0.9633 - Val orig: 0.9923\n",
      "Epoch 145 - Avg loss in last 1 epochs: - Train: 0.9579 - Val std: 0.9633 - Val orig: 0.9923\n",
      "Epoch 146 - Avg loss in last 1 epochs: - Train: 0.9576 - Val std: 0.9633 - Val orig: 0.9923\n",
      "Epoch 147 - Avg loss in last 1 epochs: - Train: 0.9577 - Val std: 0.9633 - Val orig: 0.9923\n",
      "Epoch 148 - Avg loss in last 1 epochs: - Train: 0.9576 - Val std: 0.9633 - Val orig: 0.9923\n",
      "Epoch 149 - Avg loss in last 1 epochs: - Train: 0.9574 - Val std: 0.9632 - Val orig: 0.9923\n",
      "Epoch 150 - Avg loss in last 1 epochs: - Train: 0.9575 - Val std: 0.9632 - Val orig: 0.9923\n",
      "Epoch 151 - Avg loss in last 1 epochs: - Train: 0.9576 - Val std: 0.9631 - Val orig: 0.9922\n",
      "Epoch 152 - Avg loss in last 1 epochs: - Train: 0.9574 - Val std: 0.9631 - Val orig: 0.9922\n",
      "Epoch 153 - Avg loss in last 1 epochs: - Train: 0.9571 - Val std: 0.9631 - Val orig: 0.9921\n",
      "Epoch 154 - Avg loss in last 1 epochs: - Train: 0.9571 - Val std: 0.9631 - Val orig: 0.9921\n",
      "Epoch 155 - Avg loss in last 1 epochs: - Train: 0.9572 - Val std: 0.9631 - Val orig: 0.9921\n",
      "Epoch 156 - Avg loss in last 1 epochs: - Train: 0.9571 - Val std: 0.9631 - Val orig: 0.9921\n",
      "Epoch 157 - Avg loss in last 1 epochs: - Train: 0.9572 - Val std: 0.9631 - Val orig: 0.9922\n",
      "Epoch 158 - Avg loss in last 1 epochs: - Train: 0.9572 - Val std: 0.9631 - Val orig: 0.9922\n",
      "Epoch 159 - Avg loss in last 1 epochs: - Train: 0.9570 - Val std: 0.9631 - Val orig: 0.9922\n",
      "Epoch 160 - Avg loss in last 1 epochs: - Train: 0.9570 - Val std: 0.9632 - Val orig: 0.9923\n",
      "Epoch 161 - Avg loss in last 1 epochs: - Train: 0.9569 - Val std: 0.9633 - Val orig: 0.9924\n",
      "Epoch 162 - Avg loss in last 1 epochs: - Train: 0.9568 - Val std: 0.9633 - Val orig: 0.9925\n",
      "Epoch 163 - Avg loss in last 1 epochs: - Train: 0.9568 - Val std: 0.9634 - Val orig: 0.9926\n",
      "Epoch 164 - Avg loss in last 1 epochs: - Train: 0.9568 - Val std: 0.9634 - Val orig: 0.9926\n",
      "Epoch 165 - Avg loss in last 1 epochs: - Train: 0.9568 - Val std: 0.9633 - Val orig: 0.9925\n",
      "Epoch 166 - Avg loss in last 1 epochs: - Train: 0.9568 - Val std: 0.9633 - Val orig: 0.9925\n",
      "Epoch 167 - Avg loss in last 1 epochs: - Train: 0.9568 - Val std: 0.9632 - Val orig: 0.9924\n",
      "Epoch 168 - Avg loss in last 1 epochs: - Train: 0.9565 - Val std: 0.9632 - Val orig: 0.9924\n",
      "Epoch 169 - Avg loss in last 1 epochs: - Train: 0.9565 - Val std: 0.9631 - Val orig: 0.9923\n",
      "Epoch 170 - Avg loss in last 1 epochs: - Train: 0.9566 - Val std: 0.9631 - Val orig: 0.9923\n",
      "Epoch 171 - Avg loss in last 1 epochs: - Train: 0.9565 - Val std: 0.9631 - Val orig: 0.9923\n",
      "Epoch 172 - Avg loss in last 1 epochs: - Train: 0.9566 - Val std: 0.9631 - Val orig: 0.9923\n"
     ]
    }
   ],
   "source": [
    "# Train and report\n",
    "train_rmse, val_rmse_std, val_rmse_orig = train_model(model, optimizer, loss_fn, train_users, train_items, standardized_train_ratings, val_users, val_items, orig_val_ratings, standardized_val_ratings, means, stds, EPOCHS, STOP_THRESHOLD, True, verbosity=1)\n",
    "report_training_results(train_rmse, val_rmse_std, val_rmse_orig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make ensamble\n",
    "\n",
    "# Find example combination to get best predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Post-Process and generate submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'means' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 4\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mmodels\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m LightGCNPlus\n\u001b[1;32m      3\u001b[0m ID \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mstr\u001b[39m(K) \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mstr\u001b[39m(L) \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mstr\u001b[39m(C)\n\u001b[0;32m----> 4\u001b[0m postprocess(LightGCNPlus, ID, \u001b[43mmeans\u001b[49m, stds)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'means' is not defined"
     ]
    }
   ],
   "source": [
    "from postprocess import postprocess\n",
    "from models import LightGCNPlus\n",
    "ID = str(K) + \"_\" + str(L) + \"_\" + str(C)\n",
    "postprocess(LightGCNPlus, ID, means, stds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
